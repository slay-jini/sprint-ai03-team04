# src/training/trainer.py
"""ëª¨ë¸ í•™ìŠµ ê´€ë¦¬"""

import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from pathlib import Path
from tqdm import tqdm
import time


class Trainer:
    """ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, model, config, device='cuda'):
        self.model = model
        self.config = config
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # ë¡œê¹…
        self.writer = SummaryWriter(config['output']['log_dir'])
        self.best_map = 0.0
        
    def _create_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ìƒì„±"""
        params = [p for p in self.model.parameters() if p.requires_grad]
        
        opt_type = self.config['optimizer'].get('type', 'sgd')
        lr = self.config['training']['learning_rate']
        
        if opt_type == 'sgd':
            return optim.SGD(
                params,
                lr=lr,
                momentum=self.config['optimizer'].get('momentum', 0.9),
                weight_decay=self.config['optimizer'].get('weight_decay', 0.0005)
            )
        elif opt_type == 'adam':
            return optim.Adam(params, lr=lr)
        elif opt_type == 'adamw':  # ì¶”ê°€!
            return optim.AdamW(
                params,
                lr=lr,
                weight_decay=self.config['optimizer'].get('weight_decay', 0.0005),
                betas=(0.9, 0.999)
            )
        else:
            raise ValueError(f"Unknown optimizer: {opt_type}")
    
    def _create_scheduler(self):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±"""
        scheduler_type = self.config['scheduler'].get('type', 'step')
        
        if scheduler_type == 'step':
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config['scheduler'].get('step_size', 30),
                gamma=self.config['scheduler'].get('gamma', 0.1)
            )
        elif scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['training']['epochs']
            )
        else:
            return None
    
    def train_epoch(self, data_loader, epoch):
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        
        epoch_loss = 0.0
        num_batches = len(data_loader)
        
        pbar = tqdm(data_loader, desc=f'Epoch {epoch}')
        for batch_idx, (images, targets) in enumerate(pbar):
            # GPUë¡œ ì´ë™
            images = [img.to(self.device) for img in images]
            targets = [{k: v.to(self.device) if hasattr(v, 'to') else v 
                        for k, v in t.items()} for t in targets]
            
            # ìˆœì „íŒŒ
            loss_dict = self.model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            losses.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            if self.config['training'].get('gradient_clip'):
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['training']['gradient_clip']
                )
            
            self.optimizer.step()
            
            # ë¡œê¹…
            loss_value = losses.item()
            epoch_loss += loss_value
            
            if batch_idx % 10 == 0:
                pbar.set_postfix({'loss': f'{loss_value:.4f}'})
                
                # Tensorboard ë¡œê¹…
                step = epoch * num_batches + batch_idx
                self.writer.add_scalar('Loss/train', loss_value, step)
                
                for k, v in loss_dict.items():
                    self.writer.add_scalar(f'Loss/{k}', v.item(), step)
        
        return epoch_loss / num_batches
    
    def validate(self, data_loader, evaluator, epoch):
        """ê²€ì¦ ìˆ˜í–‰"""
        print(f"\nê²€ì¦ ì¤‘...")
        
        # mAP ê³„ì‚°
        results = evaluator.evaluate(self.model, data_loader)
        
        # ë¡œê¹…
        self.writer.add_scalar('mAP/val', results['mAP'], epoch)
        self.writer.add_scalar('mAP@.5/val', results['mAP_50'], epoch)
        self.writer.add_scalar('mAP@.75/val', results['mAP_75'], epoch)
        
        # í´ë˜ìŠ¤ë³„ AP ë¡œê¹…
        for class_id, ap in results['ap_per_class'].items():
            self.writer.add_scalar(f'AP/class_{class_id}', ap, epoch)
        
        return results
    
    def train(self, train_loader, val_loader, evaluator):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        epochs = self.config['training']['epochs']
        save_interval = self.config['checkpoint']['save_interval']
        
        print(f"í•™ìŠµ ì‹œì‘: {epochs} ì—í­")
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.config['training']['batch_size']}")
        print("=" * 50)
        
        for epoch in range(1, epochs + 1):
            # í•™ìŠµ
            start_time = time.time()
            train_loss = self.train_epoch(train_loader, epoch)
            epoch_time = time.time() - start_time
            
            # í•™ìŠµë¥  ì¡°ì •
            if self.scheduler:
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                self.writer.add_scalar('LR', current_lr, epoch)
            
            # ê²€ì¦ ë° ìµœê³  ëª¨ë¸ ì €ì¥
            val_results = None
            best_updated = False

            if val_loader and epoch % self.config['validation']['interval'] == 0:
                val_results = self.validate(val_loader, evaluator, epoch)
                
                print(f"\nEpoch {epoch}/{epochs}")
                print(f"Train Loss: {train_loss:.4f}")
                print(f"Val mAP: {val_results['mAP']:.4f}")
                print(f"Val mAP@.5: {val_results['mAP_50']:.4f}")
                print(f"Val mAP@.75: {val_results['mAP_75']:.4f}")
                print(f"Time: {epoch_time:.1f}s")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_results['mAP'] >= self.best_map:
                    self.best_map = val_results['mAP']
                    self.save_checkpoint(epoch, train_loss, val_results, best=True)
                    best_updated = True
                    print(f"ìµœê³  ì„±ëŠ¥ ê°±ì‹ ! mAP: {self.best_map:.4f}")
            else:
                print(f"\nEpoch {epoch}/{epochs} - Loss: {train_loss:.4f} - Time: {epoch_time:.1f}s")
            
            # íš¨ìœ¨ì ì¸ ì •ê¸° ì €ì¥
            should_save = False
            save_reason = ""
            
            if epoch == 1:
                # ì²« ë²ˆì§¸ ì—í­: ë¬´ì¡°ê±´ ì €ì¥
                should_save = True
                save_reason = "ì²« ë²ˆì§¸ ì—í­"
            elif epoch % save_interval == 0:
                # ì •ê¸° ì €ì¥ (ëª¨ë“  ì—í­)
                should_save = True
                save_reason = f"ì •ê¸° ì €ì¥ (interval: {save_interval})"
            
            if should_save:
                self.save_checkpoint(epoch, train_loss, val_results)
                print(f"ğŸ“ ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥: epoch_{epoch}.pth ({save_reason})")
            
            print("-" * 50)
            
        self.writer.close()
        print(f"\ní•™ìŠµ ì™„ë£Œ! ìµœê³  mAP: {self.best_map:.4f}")
        print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {self.config['output']['checkpoint_dir']}")
    
    def save_checkpoint(self, epoch, loss, metrics=None, best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'loss': loss,
            'metrics': metrics,
            'config': self.config
        }
        
        # ì €ì¥ ê²½ë¡œ
        if best:
            path = Path(self.config['output']['checkpoint_dir']) / 'best_model.pth'
        else:
            path = Path(self.config['output']['checkpoint_dir']) / f'epoch_{epoch}.pth'
        
        path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(checkpoint, path)
        print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")
