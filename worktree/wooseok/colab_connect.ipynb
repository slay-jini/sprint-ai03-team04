{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc697ea",
   "metadata": {},
   "source": [
    "# Google Colab에서 Git Clone을 통한 프로젝트 사용 가이드\n",
    "\n",
    "이 노트북은 Google Colab에서 GitHub 리포지토리를 클론하여 머신러닝 모델을 학습하는 방법을 설명합니다. 로컬 개발 환경 없이도 Colab의 GPU 리소스를 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a3640",
   "metadata": {},
   "source": [
    "## 1. Colab 환경 설정\n",
    "\n",
    "Google Colab에서 프로젝트를 실행하기 위한 기본 설정:\n",
    "\n",
    "1. **런타임 타입 설정**\n",
    "   - 런타임 > 런타임 유형 변경 > GPU 선택\n",
    "   - 고사양 RAM 선택 (필요시)\n",
    "\n",
    "2. **필요한 라이브러리 설치**\n",
    "   - PyTorch, torchvision 등 ML 라이브러리\n",
    "   - 이미지 처리 라이브러리\n",
    "\n",
    "3. **Google Drive 연결** (선택사항)\n",
    "   - 데이터셋이 Drive에 저장된 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e32248",
   "metadata": {},
   "source": [
    "## 2. GitHub 리포지토리 브랜치 클론\n",
    "\n",
    "**개발 중인 코드를 테스트하기 위해 특정 브랜치에서 클론**:\n",
    "\n",
    "1. **브랜치 클론 옵션**:\n",
    "   - 특정 브랜치 직접 클론: `git clone -b branch_name`\n",
    "   - 모든 브랜치 클론 후 체크아웃\n",
    "   - 현재 개발 브랜치: `feature/wooseok-baseline-v0`\n",
    "\n",
    "2. **브랜치 확인 및 전환**:\n",
    "   - 현재 브랜치 확인\n",
    "   - 다른 브랜치로 전환\n",
    "   - 최신 변경사항 업데이트\n",
    "\n",
    "3. **의존성 설치**:\n",
    "   - 필요한 Python 패키지 설치\n",
    "   - 프로젝트 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25547588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 기본 라이브러리 설치\n",
    "!pip install torch torchvision pillow\n",
    "\n",
    "# 2. 개발 중인 브랜치에서 직접 클론 (권장)\n",
    "BRANCH_NAME = \"feature/wooseok-baseline-v0\"  # 현재 개발 브랜치\n",
    "print(f\"🌿 클론할 브랜치: {BRANCH_NAME}\")\n",
    "\n",
    "!git clone -b {BRANCH_NAME} https://github.com/slay-jini/sprint-ai03-team04.git\n",
    "\n",
    "# 3. 프로젝트 디렉토리로 이동\n",
    "import os\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# 4. 브랜치 확인\n",
    "print(\"\\n🔍 현재 브랜치 확인:\")\n",
    "!git branch -a\n",
    "!git status\n",
    "\n",
    "# 5. 현재 디렉토리 및 파일 확인\n",
    "print(f\"\\n📁 현재 작업 디렉토리: {os.getcwd()}\")\n",
    "print(\"\\n📋 프로젝트 파일 목록:\")\n",
    "!ls -la\n",
    "\n",
    "# 6. GPU 사용 가능 여부 확인\n",
    "import torch\n",
    "print(f\"\\n🎮 CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 모델: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️  GPU 사용 불가 - CPU 모드로 실행됩니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대안 방법: 다른 브랜치로 전환하거나 최신 변경사항 업데이트\n",
    "\n",
    "# 옵션 1: 이미 클론된 상태에서 다른 브랜치로 전환\n",
    "def switch_branch(branch_name):\n",
    "    \"\"\"다른 브랜치로 전환\"\"\"\n",
    "    print(f\"🔄 브랜치 전환: {branch_name}\")\n",
    "    !git checkout {branch_name}\n",
    "    !git pull origin {branch_name}\n",
    "    print(f\"✅ {branch_name} 브랜치로 전환 완료\")\n",
    "\n",
    "# 옵션 2: 모든 브랜치 확인\n",
    "def list_all_branches():\n",
    "    \"\"\"모든 브랜치 목록 확인\"\"\"\n",
    "    print(\"📋 사용 가능한 브랜치 목록:\")\n",
    "    !git branch -r\n",
    "\n",
    "# 옵션 3: 현재 브랜치 최신 상태로 업데이트\n",
    "def update_current_branch():\n",
    "    \"\"\"현재 브랜치 최신 상태로 업데이트\"\"\"\n",
    "    current_branch = !git branch --show-current\n",
    "    branch_name = current_branch[0].strip() if current_branch else \"main\"\n",
    "    print(f\"🔄 현재 브랜치 업데이트: {branch_name}\")\n",
    "    !git pull origin {branch_name}\n",
    "    print(\"✅ 업데이트 완료\")\n",
    "\n",
    "# 사용 예시 (필요시 주석 해제)\n",
    "# list_all_branches()\n",
    "# switch_branch(\"main\")  # main 브랜치로 전환\n",
    "# update_current_branch()  # 현재 브랜치 업데이트\n",
    "\n",
    "print(\"🛠️  브랜치 관리 함수 로드 완료\")\n",
    "print(\"사용 가능한 함수:\")\n",
    "print(\"- list_all_branches(): 모든 브랜치 목록 확인\")\n",
    "print(\"- switch_branch('브랜치명'): 다른 브랜치로 전환\")\n",
    "print(\"- update_current_branch(): 현재 브랜치 업데이트\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5e578",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 설정 및 준비\n",
    "\n",
    "**⚠️ 중요**: 현재 브랜치에서는 `dataset/` 폴더가 `.gitignore`에 포함되어 있습니다.\n",
    "\n",
    "1. **데이터셋 업로드**:\n",
    "   - Google Drive 마운트 또는 직접 업로드\n",
    "   - 데이터셋 경로 설정 (Git에서 제외됨)\n",
    "   - 파일 구조 확인\n",
    "\n",
    "2. **설정 파일 수정**:\n",
    "   - config.py에서 경로 설정\n",
    "   - 배치 크기 및 학습 파라미터 조정\n",
    "   - 브랜치별 설정 차이 확인\n",
    "\n",
    "3. **브랜치별 차이점**:\n",
    "   - 개발 브랜치의 최신 기능 사용\n",
    "   - 실험적 코드 포함 가능성\n",
    "   - main 브랜치 대비 추가 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. .gitignore 확인 (dataset 폴더 제외 여부)\n",
    "print(\"📋 .gitignore 파일 확인:\")\n",
    "!cat /content/sprint-ai03-team04/.gitignore | grep -E \"(dataset|data)\" || echo \"dataset 관련 항목 없음\"\n",
    "\n",
    "# 2. Google Drive 마운트 (데이터셋이 Drive에 있는 경우)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 3. 데이터셋 경로 설정\n",
    "import os\n",
    "\n",
    "# ⚠️ 중요: dataset 폴더가 .gitignore에 포함되어 있으므로 별도로 데이터셋을 준비해야 합니다\n",
    "print(\"\\n🚨 데이터셋 준비 방법:\")\n",
    "print(\"1. Google Drive에서 데이터셋을 복사하거나\")\n",
    "print(\"2. 직접 업로드하여 사용하세요\")\n",
    "\n",
    "# 옵션 1: Google Drive에서 데이터셋 복사\n",
    "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/your_dataset\"  # 실제 경로로 변경\n",
    "PROJECT_DATASET_PATH = \"/content/sprint-ai03-team04/worktree/wooseok/dataset\"\n",
    "\n",
    "# 옵션 2: 직접 업로드한 경우의 데이터셋 경로\n",
    "DATASET_PATH = \"/content/dataset\"  # 실제 경로로 변경\n",
    "\n",
    "print(f\"\\n📁 데이터셋 경로 옵션:\")\n",
    "print(f\"1. Google Drive: {DRIVE_DATASET_PATH}\")\n",
    "print(f\"2. 프로젝트 내: {PROJECT_DATASET_PATH}\")\n",
    "print(f\"3. 직접 업로드: {DATASET_PATH}\")\n",
    "\n",
    "# 4. 사용할 데이터셋 경로 선택 (아래 중 하나를 선택)\n",
    "SELECTED_DATASET = DRIVE_DATASET_PATH  # 실제 상황에 맞게 변경\n",
    "\n",
    "# 5. 데이터셋 구조 확인\n",
    "if os.path.exists(SELECTED_DATASET):\n",
    "    print(f\"\\n✅ 데이터셋 경로 발견: {SELECTED_DATASET}\")\n",
    "    print(\"📋 데이터셋 구조:\")\n",
    "    !ls -la {SELECTED_DATASET}\n",
    "    \n",
    "    # train.json 파일 확인\n",
    "    if os.path.exists(f\"{SELECTED_DATASET}/train.json\"):\n",
    "        print(f\"✅ train.json 파일 발견\")\n",
    "    else:\n",
    "        print(\"❌ train.json 파일을 찾을 수 없습니다.\")\n",
    "        \n",
    "    # 이미지 디렉토리 확인\n",
    "    if os.path.exists(f\"{SELECTED_DATASET}/images/train\"):\n",
    "        img_count = len(os.listdir(f\"{SELECTED_DATASET}/images/train\"))\n",
    "        print(f\"✅ 이미지 디렉토리 발견 ({img_count}개 파일)\")\n",
    "    else:\n",
    "        print(\"❌ 이미지 디렉토리를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"\\n❌ 데이터셋 경로가 존재하지 않습니다: {SELECTED_DATASET}\")\n",
    "    print(\"📋 데이터셋 준비 가이드:\")\n",
    "    print(\"1. Google Drive에 데이터셋 업로드\")\n",
    "    print(\"2. 경로를 SELECTED_DATASET 변수에 설정\")\n",
    "    print(\"3. 또는 Colab에 직접 업로드\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bae6eb",
   "metadata": {},
   "source": [
    "## 4. 모델 학습 실행\n",
    "\n",
    "프로젝트 설정이 완료되면 학습을 시작할 수 있습니다:\n",
    "\n",
    "1. **데이터셋 생성**:\n",
    "   - 데이터셋 객체 생성\n",
    "   - 데이터 로더 설정\n",
    "   - 전처리 파이프라인 확인\n",
    "\n",
    "2. **학습 실행**:\n",
    "   - 모델 학습 시작\n",
    "   - 학습 진행 상황 모니터링\n",
    "   - 결과 저장\n",
    "\n",
    "3. **주의사항**:\n",
    "   - Colab 세션 시간 제한 (12시간)\n",
    "   - 중간 결과 저장 (체크포인트)\n",
    "   - 메모리 사용량 모니터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa317b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 설정 파일 확인 및 수정\n",
    "import sys\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# config.py 내용 확인\n",
    "try:\n",
    "    from config import cfg\n",
    "    print(\"✅ 설정 파일 로드 성공\")\n",
    "    print(f\"디바이스: {cfg.DEVICE}\")\n",
    "    print(f\"배치 크기: {cfg.BATCH_SIZE}\")\n",
    "    print(f\"에포크: {cfg.EPOCHS}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 설정 파일 로드 실패: {e}\")\n",
    "\n",
    "# 2. 메모리 사용량 확인\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "print(f\"\\n=== 시스템 리소스 확인 ===\")\n",
    "print(f\"RAM 사용량: {psutil.virtual_memory().percent}%\")\n",
    "print(f\"사용 가능한 RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"GPU 사용량: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "\n",
    "print(\"\\n=== 환경 설정 완료 ===\")\n",
    "print(\"이제 데이터셋을 생성하고 학습을 시작할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c05d0",
   "metadata": {},
   "source": [
    "## 개발 브랜치에서 데이터셋 생성 및 학습 실행\n",
    "\n",
    "**🌿 현재 브랜치**: `feature/wooseok-baseline-v0`\n",
    "\n",
    "아래 코드를 사용하여 개발 중인 브랜치에서 데이터셋을 생성하고 학습을 실행할 수 있습니다. \n",
    "main 브랜치에 merge하기 전에 여기서 먼저 테스트해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab에서 클론한 프로젝트로 데이터셋 생성 및 학습 실행\n",
    "\n",
    "# 1. 작업 디렉토리 확인 및 설정\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 프로젝트 디렉토리로 이동\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "# 2. 데이터셋 경로 설정 (실제 경로로 변경하세요)\n",
    "DATASET_JSON = \"/content/dataset/train.json\"  # 실제 경로로 변경\n",
    "DATASET_IMAGES = \"/content/dataset/images/train\"  # 실제 경로로 변경\n",
    "\n",
    "# 또는 Google Drive에서 가져오는 경우:\n",
    "# DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"\n",
    "# DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"\n",
    "\n",
    "# 3. 데이터셋 생성\n",
    "try:\n",
    "    from dataset import create_colab_dataset\n",
    "    \n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"✅ 데이터셋 생성 성공: {len(dataset)} 개의 샘플\")\n",
    "        \n",
    "        # 4. 학습 실행\n",
    "        from train import train_with_dataset\n",
    "        print(\"🚀 학습 시작...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 데이터셋 생성 실패\")\n",
    "        print(\"데이터셋 경로를 확인하세요:\")\n",
    "        print(f\"JSON 파일: {DATASET_JSON}\")\n",
    "        print(f\"이미지 디렉토리: {DATASET_IMAGES}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {e}\")\n",
    "    print(\"프로젝트 파일이 올바르게 클론되었는지 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대안 방법: 단계별로 학습 실행하기\n",
    "\n",
    "# 1. 프로젝트 디렉토리 확인\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "print(\"📁 프로젝트 파일 확인:\")\n",
    "required_files = ['train.py', 'dataset.py', 'config.py', 'trainer.py']\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✅ {file}\")\n",
    "    else:\n",
    "        print(f\"❌ {file} - 파일이 없습니다\")\n",
    "\n",
    "# 2. 데이터셋 파일 직접 지정\n",
    "# Google Drive 또는 업로드된 데이터셋 경로를 정확히 입력하세요\n",
    "json_path = \"/content/drive/MyDrive/your_dataset/train.json\"  # 실제 경로로 변경\n",
    "img_dir = \"/content/drive/MyDrive/your_dataset/images/train\"  # 실제 경로로 변경\n",
    "\n",
    "print(f\"\\n📊 데이터셋 경로:\")\n",
    "print(f\"JSON: {json_path}\")\n",
    "print(f\"Images: {img_dir}\")\n",
    "\n",
    "# 3. 경로 존재 확인\n",
    "json_exists = os.path.exists(json_path)\n",
    "img_exists = os.path.exists(img_dir)\n",
    "\n",
    "print(f\"\\n📋 경로 확인:\")\n",
    "print(f\"JSON 파일 존재: {json_exists}\")\n",
    "print(f\"이미지 디렉토리 존재: {img_exists}\")\n",
    "\n",
    "if json_exists and img_exists:\n",
    "    print(f\"이미지 개수: {len(os.listdir(img_dir))}\")\n",
    "    \n",
    "    # 4. 데이터셋 생성 및 학습\n",
    "    try:\n",
    "        from dataset import PillDetectionDataset\n",
    "        from train import train_with_dataset\n",
    "        \n",
    "        dataset = PillDetectionDataset(\n",
    "            json_file=json_path,\n",
    "            img_dir=img_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 데이터셋 로드 완료: {len(dataset)} 샘플\")\n",
    "        \n",
    "        # 학습 실행\n",
    "        print(\"🚀 학습 시작...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 학습 실행 중 오류: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 데이터셋 경로를 확인하고 다시 시도하세요.\")\n",
    "    print(\"Google Drive를 마운트했는지, 경로가 올바른지 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d3a9f",
   "metadata": {},
   "source": [
    "## 5. 브랜치 개발 및 문제 해결\n",
    "\n",
    "### 🌿 브랜치 관련 팁\n",
    "\n",
    "1. **현재 브랜치 확인**:\n",
    "   - `!git branch --show-current`\n",
    "   - `!git status`\n",
    "\n",
    "2. **브랜치 전환**:\n",
    "   - `!git checkout main` (main 브랜치로)\n",
    "   - `!git checkout feature/wooseok-baseline-v0` (개발 브랜치로)\n",
    "\n",
    "3. **최신 변경사항 가져오기**:\n",
    "   - `!git pull origin feature/wooseok-baseline-v0`\n",
    "\n",
    "### 📁 데이터셋 관리\n",
    "\n",
    "1. **Git에서 제외된 파일들**:\n",
    "   - `dataset/` 폴더 (`.gitignore`에 포함)\n",
    "   - `__pycache__/` 폴더\n",
    "   - 로그 파일, 모델 체크포인트\n",
    "\n",
    "2. **데이터셋 준비 방법**:\n",
    "   - Google Drive 업로드 후 마운트\n",
    "   - 직접 Colab에 업로드\n",
    "   - 외부 링크에서 다운로드\n",
    "\n",
    "### 🚨 개발 브랜치 주의사항\n",
    "\n",
    "- 개발 중인 코드로 예상치 못한 오류 가능\n",
    "- 실험적 기능 포함 가능성\n",
    "- main 브랜치 대비 불안정할 수 있음\n",
    "- 테스트 후 이상 없으면 main으로 merge\n",
    "\n",
    "### 🔄 merge 전 체크리스트\n",
    "\n",
    "- [ ] 코드 정상 실행 확인\n",
    "- [ ] 기본 기능 테스트 완료\n",
    "- [ ] 오류 메시지 없음\n",
    "- [ ] 성능 저하 없음\n",
    "- [ ] 문서 업데이트 필요시 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d279a5",
   "metadata": {},
   "source": [
    "## 🚨 CUDA 메모리 오류 해결 방법\n",
    "\n",
    "**\"CUDA error: an illegal memory access was encountered\"** 오류가 발생하는 경우:\n",
    "\n",
    "### 📋 일반적인 원인들:\n",
    "1. **GPU 메모리 부족** - 배치 크기가 너무 큼\n",
    "2. **텐서 크기 불일치** - 모델 입력과 실제 데이터 크기 차이\n",
    "3. **CUDA 버전 호환성** - PyTorch와 CUDA 버전 불일치\n",
    "4. **메모리 누수** - 이전 실행에서 남은 GPU 메모리\n",
    "\n",
    "### 🔧 해결 방법들:\n",
    "1. **GPU 메모리 정리 후 재시작**\n",
    "2. **배치 크기 줄이기**\n",
    "3. **디버깅 모드 활성화**\n",
    "4. **데이터 로더 worker 수 조정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7119c7a",
   "metadata": {},
   "source": [
    "## 🔄 PyTorch 순환 Import 오류 해결\n",
    "\n",
    "**\"partially initialized module 'torch._dynamo' has no attribute 'config'\"** 오류 해결:\n",
    "\n",
    "### 📋 원인 분석:\n",
    "1. **순환 Import** - 프로젝트 파일들 간의 circular import\n",
    "2. **PyTorch 버전 문제** - 버전 호환성 이슈\n",
    "3. **모듈 초기화 오류** - 모듈이 완전히 로드되기 전 접근\n",
    "4. **캐시된 모듈** - 이전 실행의 잘못된 캐시\n",
    "\n",
    "### 🔧 해결 방법:\n",
    "1. **파이썬 모듈 캐시 정리**\n",
    "2. **Import 순서 조정**\n",
    "3. **PyTorch 재설치**\n",
    "4. **런타임 재시작**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba87857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 순환 Import 오류 해결을 위한 종합 솔루션\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import gc\n",
    "\n",
    "def clean_python_modules():\n",
    "    \"\"\"Python 모듈 캐시 정리\"\"\"\n",
    "    print(\"🧹 Python 모듈 캐시 정리 중...\")\n",
    "    \n",
    "    # 1. 프로젝트 관련 모듈 제거\n",
    "    modules_to_remove = []\n",
    "    for module_name in list(sys.modules.keys()):\n",
    "        if any(keyword in module_name for keyword in ['config', 'dataset', 'train', 'trainer', 'torch._dynamo']):\n",
    "            modules_to_remove.append(module_name)\n",
    "    \n",
    "    for module_name in modules_to_remove:\n",
    "        if module_name in sys.modules:\n",
    "            del sys.modules[module_name]\n",
    "            print(f\"  ✅ {module_name} 모듈 제거\")\n",
    "    \n",
    "    # 2. 가비지 컬렉션\n",
    "    gc.collect()\n",
    "    print(\"✅ 모듈 캐시 정리 완료\")\n",
    "\n",
    "def fix_pytorch_import():\n",
    "    \"\"\"PyTorch Import 문제 해결\"\"\"\n",
    "    print(\"🔧 PyTorch Import 문제 해결 중...\")\n",
    "    \n",
    "    # 1. 환경 변수 설정\n",
    "    os.environ['PYTHONDONTWRITEBYTECODE'] = '1'  # .pyc 파일 생성 방지\n",
    "    \n",
    "    # 2. PyTorch 모듈 강제 재로드\n",
    "    try:\n",
    "        import torch\n",
    "        importlib.reload(torch)\n",
    "        print(\"✅ PyTorch 모듈 재로드 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PyTorch 재로드 실패: {e}\")\n",
    "    \n",
    "    # 3. torch._dynamo 직접 확인\n",
    "    try:\n",
    "        import torch._dynamo\n",
    "        if hasattr(torch._dynamo, 'config'):\n",
    "            print(\"✅ torch._dynamo.config 정상 접근 가능\")\n",
    "        else:\n",
    "            print(\"❌ torch._dynamo.config 속성 없음\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ torch._dynamo 접근 실패: {e}\")\n",
    "\n",
    "def check_circular_imports():\n",
    "    \"\"\"순환 import 확인\"\"\"\n",
    "    print(\"🔍 순환 import 확인 중...\")\n",
    "    \n",
    "    # 프로젝트 디렉토리 설정\n",
    "    project_dir = '/content/sprint-ai03-team04/worktree/wooseok'\n",
    "    \n",
    "    if os.path.exists(project_dir):\n",
    "        os.chdir(project_dir)\n",
    "        \n",
    "        # 프로젝트 파일들 확인\n",
    "        project_files = ['config.py', 'dataset.py', 'train.py', 'trainer.py']\n",
    "        \n",
    "        for file in project_files:\n",
    "            if os.path.exists(file):\n",
    "                print(f\"📄 {file} 파일 존재\")\n",
    "                \n",
    "                # 각 파일의 import 구문 확인\n",
    "                try:\n",
    "                    with open(file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        imports = [line.strip() for line in content.split('\\n') \n",
    "                                 if line.strip().startswith('import ') or line.strip().startswith('from ')]\n",
    "                        \n",
    "                        print(f\"  Import 구문 ({len(imports)}개):\")\n",
    "                        for imp in imports[:5]:  # 처음 5개만 표시\n",
    "                            print(f\"    {imp}\")\n",
    "                        if len(imports) > 5:\n",
    "                            print(f\"    ... 및 {len(imports) - 5}개 더\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ 파일 읽기 실패: {e}\")\n",
    "            else:\n",
    "                print(f\"❌ {file} 파일 없음\")\n",
    "    else:\n",
    "        print(f\"❌ 프로젝트 디렉토리 없음: {project_dir}\")\n",
    "\n",
    "def reinstall_pytorch():\n",
    "    \"\"\"PyTorch 재설치\"\"\"\n",
    "    print(\"🔄 PyTorch 재설치 중...\")\n",
    "    \n",
    "    # 현재 PyTorch 버전 확인\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"현재 PyTorch 버전: {torch.__version__}\")\n",
    "        print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "    except:\n",
    "        print(\"PyTorch 버전 확인 불가\")\n",
    "    \n",
    "    # 재설치 명령어 제공\n",
    "    print(\"\\n🛠️  PyTorch 재설치 명령어:\")\n",
    "    print(\"다음 셀에서 실행하세요:\")\n",
    "    print(\"!pip uninstall torch torchvision torchaudio -y\")\n",
    "    print(\"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "\n",
    "# 실행\n",
    "print(\"🚨 PyTorch 순환 Import 오류 해결 시작\")\n",
    "clean_python_modules()\n",
    "fix_pytorch_import()\n",
    "check_circular_imports()\n",
    "reinstall_pytorch()\n",
    "\n",
    "print(\"\\n🎯 권장 해결 순서:\")\n",
    "print(\"1. 런타임 재시작 (가장 효과적)\")\n",
    "print(\"2. PyTorch 재설치\")\n",
    "print(\"3. 프로젝트 파일 import 구조 확인\")\n",
    "print(\"4. 안전한 import 순서로 재실행\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32341928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 재설치 (순환 import 오류 해결)\n",
    "\n",
    "print(\"🔄 PyTorch 완전 재설치 시작...\")\n",
    "\n",
    "# 1. 기존 PyTorch 완전 제거\n",
    "print(\"1️⃣ 기존 PyTorch 제거 중...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# 2. 시스템 캐시 정리\n",
    "print(\"\\n2️⃣ 시스템 캐시 정리 중...\")\n",
    "!pip cache purge\n",
    "\n",
    "# 3. PyTorch 재설치 (CUDA 11.8 버전)\n",
    "print(\"\\n3️⃣ PyTorch 재설치 중...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# 4. 설치 확인\n",
    "print(\"\\n4️⃣ 설치 확인 중...\")\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"✅ PyTorch 버전: {torch.__version__}\")\n",
    "    print(f\"✅ TorchVision 버전: {torchvision.__version__}\")\n",
    "    print(f\"✅ CUDA 버전: {torch.version.cuda}\")\n",
    "    print(f\"✅ CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # torch._dynamo 확인\n",
    "    try:\n",
    "        import torch._dynamo\n",
    "        if hasattr(torch._dynamo, 'config'):\n",
    "            print(\"✅ torch._dynamo.config 정상 접근 가능\")\n",
    "        else:\n",
    "            print(\"⚠️  torch._dynamo.config 속성 없음\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  torch._dynamo 접근 문제: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ PyTorch 설치 확인 실패: {e}\")\n",
    "\n",
    "print(\"\\n🎯 다음 단계:\")\n",
    "print(\"1. 런타임 재시작 (권장)\")\n",
    "print(\"2. 프로젝트 다시 클론\")\n",
    "print(\"3. 학습 코드 재실행\")\n",
    "print(\"4. 여전히 문제 시 순환 import 확인 필요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 안전한 Import 순서로 학습 실행 (순환 import 방지)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"🛡️  안전한 Import 순서로 학습 실행\")\n",
    "\n",
    "# 1. 환경 초기화\n",
    "print(\"1️⃣ 환경 초기화 중...\")\n",
    "os.environ['PYTHONDONTWRITEBYTECODE'] = '1'\n",
    "gc.collect()\n",
    "\n",
    "# 2. 프로젝트 디렉토리 설정\n",
    "project_dir = '/content/sprint-ai03-team04/worktree/wooseok'\n",
    "os.chdir(project_dir)\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "print(f\"📁 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "# 3. 안전한 순서로 모듈 Import\n",
    "print(\"\\n2️⃣ 안전한 순서로 모듈 Import 중...\")\n",
    "\n",
    "try:\n",
    "    # 기본 라이브러리 먼저\n",
    "    print(\"  🔹 기본 라이브러리 import...\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # 프로젝트 모듈들 순서대로\n",
    "    print(\"  🔹 config 모듈 import...\")\n",
    "    from config import cfg\n",
    "    \n",
    "    print(\"  🔹 dataset 모듈 import...\")\n",
    "    from dataset import PillDetectionDataset, create_colab_dataset\n",
    "    \n",
    "    print(\"  🔹 model 모듈 import...\")\n",
    "    from models.yolo_model import create_model\n",
    "    \n",
    "    print(\"  🔹 trainer 모듈 import...\")\n",
    "    from trainer import Trainer\n",
    "    \n",
    "    print(\"  🔹 train 모듈 import...\")\n",
    "    from train import train_with_dataset\n",
    "    \n",
    "    print(\"✅ 모든 모듈 import 성공\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import 오류: {e}\")\n",
    "    print(\"🔄 권장 조치:\")\n",
    "    print(\"1. 런타임 재시작\")\n",
    "    print(\"2. PyTorch 재설치\")\n",
    "    print(\"3. 프로젝트 파일 확인\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 예상치 못한 오류: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. 안전한 설정\n",
    "print(\"\\n3️⃣ 안전한 설정 적용 중...\")\n",
    "try:\n",
    "    # 안전한 설정으로 변경\n",
    "    cfg.BATCH_SIZE = 2\n",
    "    cfg.NUM_WORKERS = 0\n",
    "    cfg.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"✅ 배치 크기: {cfg.BATCH_SIZE}\")\n",
    "    print(f\"✅ Worker 수: {cfg.NUM_WORKERS}\")\n",
    "    print(f\"✅ 디바이스: {cfg.DEVICE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 설정 적용 실패: {e}\")\n",
    "\n",
    "# 5. 데이터셋 경로 설정\n",
    "print(\"\\n4️⃣ 데이터셋 경로 설정...\")\n",
    "DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"  # 실제 경로로 변경\n",
    "DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"  # 실제 경로로 변경\n",
    "\n",
    "print(f\"📊 JSON 파일: {DATASET_JSON}\")\n",
    "print(f\"🖼️  이미지 디렉토리: {DATASET_IMAGES}\")\n",
    "\n",
    "# 6. 학습 실행 (안전 모드)\n",
    "print(\"\\n5️⃣ 학습 실행 준비 완료\")\n",
    "print(\"🎯 다음 단계:\")\n",
    "print(\"1. 위의 경로를 실제 데이터셋 경로로 변경\")\n",
    "print(\"2. 아래 코드 주석 해제하여 학습 실행\")\n",
    "\n",
    "# 아래 주석을 해제하면 학습 실행\n",
    "\"\"\"\n",
    "try:\n",
    "    # 데이터셋 생성\n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"✅ 데이터셋 생성 성공: {len(dataset)} 샘플\")\n",
    "        \n",
    "        # 학습 실행\n",
    "        print(\"🚀 안전 모드로 학습 시작...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 데이터셋 생성 실패\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 학습 실행 중 오류: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 메모리 오류 해결을 위한 디버깅 및 수정 코드\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def fix_cuda_memory_error():\n",
    "    \"\"\"CUDA 메모리 오류 해결을 위한 종합 솔루션\"\"\"\n",
    "    print(\"🔧 CUDA 메모리 오류 해결 중...\")\n",
    "    \n",
    "    # 1. GPU 메모리 정리\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"✅ GPU 메모리 캐시 정리 완료\")\n",
    "    \n",
    "    # 2. 디버깅 모드 활성화\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    print(\"✅ CUDA 디버깅 모드 활성화\")\n",
    "    \n",
    "    # 3. GPU 메모리 상태 확인\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        cached_memory = torch.cuda.memory_reserved(device)\n",
    "        \n",
    "        print(f\"\\n📊 GPU 메모리 상태:\")\n",
    "        print(f\"총 메모리: {total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"할당된 메모리: {allocated_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"캐시된 메모리: {cached_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"사용 가능한 메모리: {(total_memory - allocated_memory) / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # 4. 권장 설정 출력\n",
    "    print(f\"\\n⚙️  권장 설정:\")\n",
    "    print(f\"배치 크기: 4 이하 (현재 A100-40GB 기준)\")\n",
    "    print(f\"Worker 수: 0-2 (Colab 환경)\")\n",
    "    print(f\"Mixed precision: 사용 권장\")\n",
    "\n",
    "def create_safe_config():\n",
    "    \"\"\"안전한 학습 설정 생성\"\"\"\n",
    "    print(\"🛡️  안전한 학습 설정 생성...\")\n",
    "    \n",
    "    # config.py 수정을 위한 안전한 값들\n",
    "    safe_config = {\n",
    "        'BATCH_SIZE': 2,          # 배치 크기 대폭 감소\n",
    "        'NUM_WORKERS': 0,         # Worker 수 0으로 설정\n",
    "        'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'MIXED_PRECISION': True,  # Mixed precision 활성화\n",
    "        'GRADIENT_CLIP': 1.0,     # 그래디언트 클리핑\n",
    "    }\n",
    "    \n",
    "    print(\"📋 안전한 설정값:\")\n",
    "    for key, value in safe_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return safe_config\n",
    "\n",
    "def test_simple_tensor_operations():\n",
    "    \"\"\"간단한 텐서 연산으로 CUDA 상태 테스트\"\"\"\n",
    "    print(\"🧪 CUDA 상태 테스트 중...\")\n",
    "    \n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            # 간단한 텐서 연산 테스트\n",
    "            device = torch.device('cuda')\n",
    "            x = torch.randn(10, 10).to(device)\n",
    "            y = torch.randn(10, 10).to(device)\n",
    "            z = torch.mm(x, y)\n",
    "            print(\"✅ 기본 CUDA 텐서 연산 정상\")\n",
    "            \n",
    "            # 메모리 해제\n",
    "            del x, y, z\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ CUDA 사용 불가\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CUDA 테스트 실패: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 실행\n",
    "print(\"🚨 CUDA 메모리 오류 해결 시작\")\n",
    "fix_cuda_memory_error()\n",
    "safe_config = create_safe_config()\n",
    "cuda_test_passed = test_simple_tensor_operations()\n",
    "\n",
    "if cuda_test_passed:\n",
    "    print(\"\\n✅ CUDA 상태 정상 - 안전한 설정으로 학습 재시도 가능\")\n",
    "else:\n",
    "    print(\"\\n❌ CUDA 상태 불안정 - 런타임 재시작 권장\")\n",
    "    \n",
    "print(\"\\n🔄 다음 단계:\")\n",
    "print(\"1. 런타임 재시작 (필요시)\")\n",
    "print(\"2. 배치 크기를 2로 설정\")\n",
    "print(\"3. NUM_WORKERS를 0으로 설정\")\n",
    "print(\"4. 학습 재시도\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84783eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 오류 해결 후 안전한 학습 실행\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 디버깅 모드\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# 2. GPU 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"🧹 GPU 메모리 정리 완료\")\n",
    "\n",
    "# 3. 안전한 설정으로 config 임시 수정\n",
    "try:\n",
    "    from config import cfg\n",
    "    \n",
    "    # 원본 설정 백업\n",
    "    original_batch_size = cfg.BATCH_SIZE\n",
    "    original_num_workers = cfg.NUM_WORKERS\n",
    "    \n",
    "    # 안전한 설정으로 임시 변경\n",
    "    cfg.BATCH_SIZE = 2          # 배치 크기 대폭 감소\n",
    "    cfg.NUM_WORKERS = 0         # Worker 수 0으로 설정\n",
    "    cfg.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(\"⚙️  안전한 설정으로 변경:\")\n",
    "    print(f\"배치 크기: {original_batch_size} → {cfg.BATCH_SIZE}\")\n",
    "    print(f\"Worker 수: {original_num_workers} → {cfg.NUM_WORKERS}\")\n",
    "    print(f\"디바이스: {cfg.DEVICE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 설정 로드 실패: {e}\")\n",
    "\n",
    "# 4. 데이터셋 경로 설정 (실제 경로로 변경)\n",
    "DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"  # 실제 경로로 변경\n",
    "DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"  # 실제 경로로 변경\n",
    "\n",
    "# 5. 안전한 학습 실행\n",
    "try:\n",
    "    print(\"🚀 안전한 학습 시작...\")\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    from dataset import create_colab_dataset\n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"✅ 데이터셋 생성 성공: {len(dataset)} 샘플\")\n",
    "        \n",
    "        # 안전한 학습 실행\n",
    "        from train import train_with_dataset\n",
    "        \n",
    "        # 추가 안전 장치\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        print(\"🛡️  안전 모드로 학습 시작...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 데이터셋 생성 실패\")\n",
    "        print(\"데이터셋 경로를 확인하세요:\")\n",
    "        print(f\"JSON: {DATASET_JSON}\")\n",
    "        print(f\"Images: {DATASET_IMAGES}\")\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e):\n",
    "        print(f\"❌ CUDA 오류 재발생: {e}\")\n",
    "        print(\"\\n🔄 권장 조치:\")\n",
    "        print(\"1. 런타임 > 런타임 재시작\")\n",
    "        print(\"2. 배치 크기를 1로 더 줄이기\")\n",
    "        print(\"3. CPU 모드로 전환 (cfg.DEVICE = 'cpu')\")\n",
    "    else:\n",
    "        print(f\"❌ 기타 오류: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 예상치 못한 오류: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # 메모리 정리\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"🧹 학습 후 메모리 정리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd625c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 긴급 대안: CPU 전용 학습 (CUDA 오류 지속 시)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"🔄 CPU 전용 모드로 전환...\")\n",
    "\n",
    "# 1. CUDA 완전 비활성화\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "torch.cuda.is_available = lambda: False\n",
    "\n",
    "# 2. 프로젝트 설정\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# 3. CPU 전용 설정\n",
    "try:\n",
    "    from config import cfg\n",
    "    \n",
    "    cfg.DEVICE = 'cpu'\n",
    "    cfg.BATCH_SIZE = 4          # CPU에서는 배치 크기 조금 늘려도 됨\n",
    "    cfg.NUM_WORKERS = 2         # CPU에서는 worker 사용 가능\n",
    "    cfg.EPOCHS = 2              # 테스트를 위해 에포크 수 줄이기\n",
    "    \n",
    "    print(\"💻 CPU 전용 설정:\")\n",
    "    print(f\"디바이스: {cfg.DEVICE}\")\n",
    "    print(f\"배치 크기: {cfg.BATCH_SIZE}\")\n",
    "    print(f\"Worker 수: {cfg.NUM_WORKERS}\")\n",
    "    print(f\"에포크: {cfg.EPOCHS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 설정 로드 실패: {e}\")\n",
    "\n",
    "# 4. CPU 전용 학습 실행\n",
    "print(\"\\n⚠️  주의: CPU 모드는 매우 느립니다 (테스트 목적)\")\n",
    "print(\"실제 학습은 CUDA 오류 해결 후 GPU에서 실행하세요\")\n",
    "\n",
    "# 사용자 확인\n",
    "print(\"\\n🤔 CPU 모드로 계속 진행하시겠습니까?\")\n",
    "print(\"계속하려면 아래 주석을 해제하고 실행하세요:\")\n",
    "\n",
    "# 아래 주석을 해제하면 CPU 모드로 실행\n",
    "\"\"\"\n",
    "try:\n",
    "    # 데이터셋 경로 설정\n",
    "    DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"\n",
    "    DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"\n",
    "    \n",
    "    from dataset import create_colab_dataset\n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"✅ 데이터셋 생성 성공: {len(dataset)} 샘플\")\n",
    "        \n",
    "        from train import train_with_dataset\n",
    "        print(\"💻 CPU 모드로 학습 시작... (매우 느림)\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 데이터셋 생성 실패\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ CPU 학습 중 오류: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n🎯 권장 해결책:\")\n",
    "print(\"1. 런타임 > 런타임 재시작\")\n",
    "print(\"2. GPU 메모리 정리 후 재시도\")\n",
    "print(\"3. 배치 크기를 1로 설정\")\n",
    "print(\"4. 이전 셀의 '안전한 학습 실행' 코드 사용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ada6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유용한 유틸리티 함수들\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def check_system_resources():\n",
    "    \"\"\"시스템 리소스 사용량 확인\"\"\"\n",
    "    print(\"=== 시스템 리소스 현황 ===\")\n",
    "    \n",
    "    # CPU 및 메모리\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    \n",
    "    print(f\"🖥️  CPU 사용량: {cpu_percent}%\")\n",
    "    print(f\"💾 메모리 사용량: {memory.percent}%\")\n",
    "    print(f\"📊 사용 가능한 메모리: {memory.available / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # GPU 정보\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        gpu_allocated = torch.cuda.memory_allocated(0)\n",
    "        gpu_cached = torch.cuda.memory_reserved(0)\n",
    "        \n",
    "        print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"📈 GPU 메모리 총량: {gpu_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"🔢 GPU 할당된 메모리: {gpu_allocated / (1024**3):.2f} GB\")\n",
    "        print(f\"💽 GPU 캐시된 메모리: {gpu_cached / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"❌ GPU를 사용할 수 없습니다\")\n",
    "\n",
    "def save_model_to_drive(model_path, drive_path):\n",
    "    \"\"\"모델을 Google Drive에 저장\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(drive_path):\n",
    "            os.makedirs(drive_path)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_path = os.path.join(drive_path, f\"model_{timestamp}.pth\")\n",
    "        \n",
    "        shutil.copy2(model_path, save_path)\n",
    "        print(f\"✅ 모델 저장 완료: {save_path}\")\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 저장 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_gpu_cache():\n",
    "    \"\"\"GPU 캐시 정리\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"🧹 GPU 캐시 정리 완료\")\n",
    "    else:\n",
    "        print(\"❌ GPU를 사용할 수 없습니다\")\n",
    "\n",
    "def quick_dataset_check(json_path, img_dir):\n",
    "    \"\"\"데이터셋 빠른 확인\"\"\"\n",
    "    print(\"=== 데이터셋 빠른 확인 ===\")\n",
    "    \n",
    "    # JSON 파일 확인\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"✅ JSON 파일 존재: {json_path}\")\n",
    "        \n",
    "        import json\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"📊 이미지 개수: {len(data['images'])}\")\n",
    "            print(f\"🏷️  어노테이션 개수: {len(data['annotations'])}\")\n",
    "            print(f\"🎯 카테고리 개수: {len(data['categories'])}\")\n",
    "    else:\n",
    "        print(f\"❌ JSON 파일 없음: {json_path}\")\n",
    "    \n",
    "    # 이미지 디렉토리 확인\n",
    "    if os.path.exists(img_dir):\n",
    "        img_count = len([f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        print(f\"✅ 이미지 디렉토리 존재: {img_dir}\")\n",
    "        print(f\"🖼️  이미지 파일 개수: {img_count}\")\n",
    "    else:\n",
    "        print(f\"❌ 이미지 디렉토리 없음: {img_dir}\")\n",
    "\n",
    "# 사용 예시\n",
    "print(\"🔧 유틸리티 함수 로드 완료\")\n",
    "print(\"사용 가능한 함수들:\")\n",
    "print(\"- check_system_resources(): 시스템 리소스 확인\")\n",
    "print(\"- save_model_to_drive(model_path, drive_path): 모델 Drive 저장\")\n",
    "print(\"- clean_gpu_cache(): GPU 캐시 정리\")\n",
    "print(\"- quick_dataset_check(json_path, img_dir): 데이터셋 빠른 확인\")\n",
    "\n",
    "# 현재 리소스 상태 확인\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2525c4",
   "metadata": {},
   "source": [
    "## 6. 학습 결과 분석 및 평가\n",
    "\n",
    "학습이 완료된 후 다음 도구들을 사용하여 모델의 성능을 분석하고 평가할 수 있습니다:\n",
    "\n",
    "### 📊 학습 결과 확인\n",
    "1. **학습 곡선 분석**:\n",
    "   - 손실 함수 변화 추이\n",
    "   - 검증 성능 변화\n",
    "   - Overfitting 여부 확인\n",
    "\n",
    "2. **체크포인트 및 로그 확인**:\n",
    "   - 저장된 모델 체크포인트\n",
    "   - 학습 로그 분석\n",
    "   - 최고 성능 모델 선택\n",
    "\n",
    "3. **성능 메트릭 계산**:\n",
    "   - mAP (mean Average Precision)\n",
    "   - 클래스별 성능 분석\n",
    "   - Confusion Matrix\n",
    "\n",
    "### 🎯 모델 평가 도구\n",
    "- **정량적 평가**: mAP, Precision, Recall\n",
    "- **정성적 평가**: 예측 결과 시각화\n",
    "- **오류 분석**: 잘못 예측된 케이스 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 및 체크포인트 확인\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def check_training_results():\n",
    "    \"\"\"학습 결과 확인\"\"\"\n",
    "    print(\"=== 학습 결과 확인 ===\")\n",
    "    \n",
    "    # 현재 작업 디렉토리 확인\n",
    "    current_dir = '/content/sprint-ai03-team04/worktree/wooseok'\n",
    "    os.chdir(current_dir)\n",
    "    print(f\"📁 작업 디렉토리: {os.getcwd()}\")\n",
    "    \n",
    "    # outputs 디렉토리 확인\n",
    "    outputs_dir = 'outputs'\n",
    "    if os.path.exists(outputs_dir):\n",
    "        print(f\"\\n📂 출력 디렉토리 ({outputs_dir}) 내용:\")\n",
    "        files = os.listdir(outputs_dir)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(outputs_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path)\n",
    "                modified = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                print(f\"  📄 {file} ({size} bytes, {modified.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "    else:\n",
    "        print(f\"❌ 출력 디렉토리를 찾을 수 없습니다: {outputs_dir}\")\n",
    "    \n",
    "    # 학습 로그 확인\n",
    "    log_file = os.path.join(outputs_dir, 'training.log')\n",
    "    if os.path.exists(log_file):\n",
    "        print(f\"\\n📜 학습 로그 마지막 10줄:\")\n",
    "        with open(log_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[-10:]:\n",
    "                print(f\"  {line.strip()}\")\n",
    "    else:\n",
    "        print(\"❌ 학습 로그 파일을 찾을 수 없습니다\")\n",
    "    \n",
    "    # 체크포인트 파일 확인\n",
    "    checkpoint_files = [f for f in os.listdir(outputs_dir) if f.endswith('.pth')] if os.path.exists(outputs_dir) else []\n",
    "    if checkpoint_files:\n",
    "        print(f\"\\n💾 체크포인트 파일들:\")\n",
    "        for checkpoint in checkpoint_files:\n",
    "            file_path = os.path.join(outputs_dir, checkpoint)\n",
    "            size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            print(f\"  📦 {checkpoint} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"❌ 체크포인트 파일을 찾을 수 없습니다\")\n",
    "    \n",
    "    return outputs_dir, checkpoint_files\n",
    "\n",
    "# 실행\n",
    "outputs_dir, checkpoint_files = check_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def plot_training_curves():\n",
    "    \"\"\"학습 곡선 그래프 표시\"\"\"\n",
    "    print(\"📈 학습 곡선 시각화\")\n",
    "    \n",
    "    # 학습 곡선 이미지 파일 확인\n",
    "    curve_image_path = os.path.join(outputs_dir, 'learning_curves.png')\n",
    "    \n",
    "    if os.path.exists(curve_image_path):\n",
    "        print(f\"✅ 학습 곡선 이미지 발견: {curve_image_path}\")\n",
    "        \n",
    "        # 이미지 표시\n",
    "        img = Image.open(curve_image_path)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title('Training and Validation Curves', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 학습 곡선 이미지를 찾을 수 없습니다\")\n",
    "        print(\"학습이 완료되지 않았거나 그래프 생성에 실패했을 수 있습니다\")\n",
    "\n",
    "def load_checkpoint_history(checkpoint_path):\n",
    "    \"\"\"체크포인트에서 학습 이력 로드\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        if 'history' in checkpoint:\n",
    "            history = checkpoint['history']\n",
    "            print(f\"✅ 학습 이력 로드 성공: {checkpoint_path}\")\n",
    "            return history\n",
    "        else:\n",
    "            print(f\"❌ 체크포인트에 학습 이력이 없습니다: {checkpoint_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 체크포인트 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_history_from_checkpoint():\n",
    "    \"\"\"체크포인트에서 학습 이력을 읽어서 그래프 그리기\"\"\"\n",
    "    if not checkpoint_files:\n",
    "        print(\"❌ 체크포인트 파일이 없습니다\")\n",
    "        return\n",
    "    \n",
    "    # 가장 최근 체크포인트 또는 best_model.pth 우선 선택\n",
    "    best_checkpoint = None\n",
    "    for checkpoint in checkpoint_files:\n",
    "        if 'best_model.pth' in checkpoint:\n",
    "            best_checkpoint = checkpoint\n",
    "            break\n",
    "    \n",
    "    if not best_checkpoint:\n",
    "        best_checkpoint = checkpoint_files[-1]  # 마지막 체크포인트\n",
    "    \n",
    "    checkpoint_path = os.path.join(outputs_dir, best_checkpoint)\n",
    "    print(f\"📊 체크포인트에서 학습 이력 추출: {best_checkpoint}\")\n",
    "    \n",
    "    history = load_checkpoint_history(checkpoint_path)\n",
    "    \n",
    "    if history:\n",
    "        # 학습 곡선 그리기\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # 손실 그래프\n",
    "        if 'train_loss' in history and 'val_loss' in history:\n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "            ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training and Validation Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # mAP 그래프\n",
    "        if 'val_map' in history:\n",
    "            epochs = range(1, len(history['val_map']) + 1)\n",
    "            ax2.plot(epochs, history['val_map'], 'g-', label='Validation mAP', linewidth=2)\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('mAP')\n",
    "            ax2.set_title('Validation mAP')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 수치 정보 출력\n",
    "        print(f\"\\n📊 학습 결과 요약:\")\n",
    "        if 'train_loss' in history:\n",
    "            print(f\"  🔹 최종 Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        if 'val_loss' in history:\n",
    "            print(f\"  🔹 최종 Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        if 'val_map' in history:\n",
    "            print(f\"  🔹 최종 Validation mAP: {history['val_map'][-1]:.4f}\")\n",
    "            if len(history['val_map']) > 1:\n",
    "                best_map = max(history['val_map'])\n",
    "                best_epoch = history['val_map'].index(best_map) + 1\n",
    "                print(f\"  🏆 최고 mAP: {best_map:.4f} (Epoch {best_epoch})\")\n",
    "\n",
    "# 실행\n",
    "print(\"=\" * 50)\n",
    "plot_training_curves()\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "plot_history_from_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mAP 측정 및 모델 평가\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 프로젝트 모듈 import\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "def evaluate_model_map(model, val_loader, device):\n",
    "    \"\"\"검증 데이터셋에서 mAP 측정\"\"\"\n",
    "    print(\"🎯 모델 mAP 평가 시작...\")\n",
    "    \n",
    "    model.eval()\n",
    "    pred_boxes, pred_labels, pred_scores = [], [], []\n",
    "    gt_boxes, gt_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc='mAP 계산 중'):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # 예측 수행\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if outputs is not None:\n",
    "                # 예측 결과 수집\n",
    "                for out, target in zip(outputs, targets):\n",
    "                    if isinstance(out, dict) and 'boxes' in out:\n",
    "                        pred_boxes.append(out['boxes'].cpu())\n",
    "                        pred_labels.append(out['labels'].cpu())\n",
    "                        pred_scores.append(out['scores'].cpu())\n",
    "                        gt_boxes.append(target['boxes'].cpu())\n",
    "                        gt_labels.append(target['labels'].cpu())\n",
    "    \n",
    "    # mAP 계산\n",
    "    if pred_boxes and gt_boxes:\n",
    "        from utils.metrics import calculate_mAP\n",
    "        map_score = calculate_mAP(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels)\n",
    "        print(f\"✅ mAP 계산 완료: {map_score:.4f}\")\n",
    "        return map_score\n",
    "    else:\n",
    "        print(\"❌ 유효한 예측 결과가 없습니다\")\n",
    "        return 0.0\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"최고 성능 모델 로드\"\"\"\n",
    "    try:\n",
    "        from config import cfg\n",
    "        from models.yolo_model import create_model\n",
    "        \n",
    "        # 모델 생성\n",
    "        model = create_model(cfg)\n",
    "        \n",
    "        # 최고 성능 체크포인트 로드\n",
    "        best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "        \n",
    "        if os.path.exists(best_model_path):\n",
    "            print(f\"📦 최고 성능 모델 로드: {best_model_path}\")\n",
    "            checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # 체크포인트 정보 출력\n",
    "            if 'epoch' in checkpoint:\n",
    "                print(f\"  🔹 에포크: {checkpoint['epoch']}\")\n",
    "            if 'val_loss' in checkpoint:\n",
    "                print(f\"  🔹 검증 손실: {checkpoint['val_loss']:.4f}\")\n",
    "            \n",
    "            return model\n",
    "        else:\n",
    "            print(\"❌ 최고 성능 모델 파일을 찾을 수 없습니다\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_validation_loader():\n",
    "    \"\"\"검증 데이터 로더 생성\"\"\"\n",
    "    try:\n",
    "        from config import cfg\n",
    "        from dataset import PillDetectionDataset\n",
    "        from torch.utils.data import DataLoader, random_split\n",
    "        \n",
    "        def collate_fn(batch):\n",
    "            return tuple(zip(*batch))\n",
    "        \n",
    "        # 데이터셋 경로 설정 (실제 경로로 변경 필요)\n",
    "        # Google Drive나 업로드된 데이터셋 경로를 여기에 설정하세요\n",
    "        DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"  # 실제 경로로 변경\n",
    "        DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"  # 실제 경로로 변경\n",
    "        \n",
    "        print(f\"📊 데이터셋 경로:\")\n",
    "        print(f\"  JSON: {DATASET_JSON}\")\n",
    "        print(f\"  Images: {DATASET_IMAGES}\")\n",
    "        \n",
    "        if not os.path.exists(DATASET_JSON) or not os.path.exists(DATASET_IMAGES):\n",
    "            print(\"❌ 데이터셋 경로를 확인하세요\")\n",
    "            return None\n",
    "        \n",
    "        # 데이터셋 로드\n",
    "        dataset = PillDetectionDataset(\n",
    "            json_file=DATASET_JSON,\n",
    "            img_dir=DATASET_IMAGES\n",
    "        )\n",
    "        \n",
    "        # 학습/검증 분할 (학습 시와 동일한 방식)\n",
    "        val_size = int(len(dataset) * cfg.VAL_RATIO)\n",
    "        train_size = len(dataset) - val_size\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)  # 동일한 시드 사용\n",
    "        )\n",
    "        \n",
    "        # 검증 데이터 로더 생성\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Colab에서는 0으로 설정\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 검증 데이터 로더 생성 완료: {len(val_dataset)} 샘플\")\n",
    "        return val_loader\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 검증 데이터 로더 생성 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_model_evaluation():\n",
    "    \"\"\"전체 모델 평가 실행\"\"\"\n",
    "    print(\"🔬 모델 평가 시작\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 디바이스 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"💻 사용 디바이스: {device}\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = load_best_model()\n",
    "    if model is None:\n",
    "        print(\"❌ 모델 평가 중단: 모델 로드 실패\")\n",
    "        return\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 검증 데이터 로더 생성\n",
    "    val_loader = create_validation_loader()\n",
    "    if val_loader is None:\n",
    "        print(\"❌ 모델 평가 중단: 데이터 로더 생성 실패\")\n",
    "        return\n",
    "    \n",
    "    # mAP 평가 실행\n",
    "    map_score = evaluate_model_map(model, val_loader, device)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"🏆 최종 평가 결과\")\n",
    "    print(f\"📊 mAP (mean Average Precision): {map_score:.4f}\")\n",
    "    \n",
    "    # 성능 해석\n",
    "    if map_score > 0.5:\n",
    "        print(\"🎉 훌륭한 성능입니다!\")\n",
    "    elif map_score > 0.3:\n",
    "        print(\"👍 양호한 성능입니다.\")\n",
    "    elif map_score > 0.1:\n",
    "        print(\"📈 개선이 필요합니다.\")\n",
    "    else:\n",
    "        print(\"🔧 모델 구조나 학습 방법을 재검토해야 합니다.\")\n",
    "    \n",
    "    return map_score\n",
    "\n",
    "# 사용법 안내\n",
    "print(\"🎯 mAP 평가 도구 로드 완료\")\n",
    "print(\"\\n📋 평가 실행 방법:\")\n",
    "print(\"1. 데이터셋 경로를 실제 경로로 수정\")\n",
    "print(\"2. run_model_evaluation() 함수 실행\")\n",
    "print(\"\\n⚠️  주의사항:\")\n",
    "print(\"- 학습이 완료된 후 실행하세요\")\n",
    "print(\"- 데이터셋 경로가 올바른지 확인하세요\")\n",
    "print(\"- 최고 성능 모델(best_model.pth)이 있는지 확인하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1347a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 실제 mAP 평가 실행\n",
    "\n",
    "# ⚠️ 중요: 실행 전에 데이터셋 경로를 실제 경로로 수정하세요!\n",
    "\n",
    "# 1. 데이터셋 경로 설정 (실제 경로로 변경 필요)\n",
    "print(\"📊 데이터셋 경로 설정\")\n",
    "print(\"⚠️  아래 경로를 실제 데이터셋 경로로 변경하세요:\")\n",
    "\n",
    "# Google Drive에 데이터셋이 있는 경우\n",
    "DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"\n",
    "DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"\n",
    "\n",
    "# 또는 직접 업로드한 경우\n",
    "# DATASET_JSON = \"/content/dataset/train.json\"\n",
    "# DATASET_IMAGES = \"/content/dataset/images/train\"\n",
    "\n",
    "print(f\"JSON 파일: {DATASET_JSON}\")\n",
    "print(f\"이미지 디렉토리: {DATASET_IMAGES}\")\n",
    "\n",
    "# 2. 경로 확인\n",
    "json_exists = os.path.exists(DATASET_JSON)\n",
    "img_exists = os.path.exists(DATASET_IMAGES)\n",
    "\n",
    "print(f\"\\n✅ 경로 확인:\")\n",
    "print(f\"JSON 파일 존재: {json_exists}\")\n",
    "print(f\"이미지 디렉토리 존재: {img_exists}\")\n",
    "\n",
    "if json_exists and img_exists:\n",
    "    print(\"✅ 데이터셋 경로 확인 완료!\")\n",
    "    print(\"\\n🚀 mAP 평가를 시작합니다...\")\n",
    "    \n",
    "    # config.py에서 데이터셋 경로 임시 수정\n",
    "    try:\n",
    "        from config import cfg\n",
    "        # 경로 임시 변경\n",
    "        original_json = cfg.TRAIN_JSON\n",
    "        original_img = cfg.TRAIN_IMG_DIR\n",
    "        \n",
    "        cfg.TRAIN_JSON = DATASET_JSON\n",
    "        cfg.TRAIN_IMG_DIR = DATASET_IMAGES\n",
    "        \n",
    "        print(f\"📝 설정 임시 변경:\")\n",
    "        print(f\"  JSON: {original_json} → {cfg.TRAIN_JSON}\")\n",
    "        print(f\"  Images: {original_img} → {cfg.TRAIN_IMG_DIR}\")\n",
    "        \n",
    "        # mAP 평가 실행\n",
    "        final_map = run_model_evaluation()\n",
    "        \n",
    "        # 설정 복원\n",
    "        cfg.TRAIN_JSON = original_json\n",
    "        cfg.TRAIN_IMG_DIR = original_img\n",
    "        \n",
    "        print(f\"\\n🎊 평가 완료!\")\n",
    "        print(f\"🏆 최종 mAP: {final_map:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 평가 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 데이터셋 경로를 확인하고 다시 시도하세요.\")\n",
    "    print(\"\\n📋 해결 방법:\")\n",
    "    print(\"1. Google Drive를 마운트했는지 확인\")\n",
    "    print(\"2. 데이터셋 경로가 올바른지 확인\")\n",
    "    print(\"3. 위의 DATASET_JSON, DATASET_IMAGES 변수를 실제 경로로 수정\")\n",
    "    print(\"4. 이 셀을 다시 실행\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 성능 분석 및 개선 제안\n",
    "\n",
    "def analyze_training_performance():\n",
    "    \"\"\"학습 성능 종합 분석\"\"\"\n",
    "    print(\"🔍 학습 성능 종합 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 체크포인트에서 상세 정보 추출\n",
    "    best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        \n",
    "        print(\"📈 학습 결과 요약:\")\n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"  🔹 최고 성능 에포크: {checkpoint['epoch']}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"  🔹 최고 성능 시 검증 손실: {checkpoint['val_loss']:.6f}\")\n",
    "        \n",
    "        if 'history' in checkpoint:\n",
    "            history = checkpoint['history']\n",
    "            \n",
    "            # 손실 분석\n",
    "            if 'train_loss' in history and 'val_loss' in history:\n",
    "                train_losses = history['train_loss']\n",
    "                val_losses = history['val_loss']\n",
    "                \n",
    "                print(f\"\\n📉 손실 함수 분석:\")\n",
    "                print(f\"  🔹 최종 훈련 손실: {train_losses[-1]:.6f}\")\n",
    "                print(f\"  🔹 최종 검증 손실: {val_losses[-1]:.6f}\")\n",
    "                print(f\"  🔹 손실 비율 (val/train): {val_losses[-1]/train_losses[-1]:.2f}\")\n",
    "                \n",
    "                # Overfitting 분석\n",
    "                if len(train_losses) > 3:\n",
    "                    recent_train = np.mean(train_losses[-3:])\n",
    "                    recent_val = np.mean(val_losses[-3:])\n",
    "                    \n",
    "                    if recent_val > recent_train * 1.5:\n",
    "                        print(\"  ⚠️  과적합(Overfitting) 가능성 높음\")\n",
    "                    elif recent_val > recent_train * 1.2:\n",
    "                        print(\"  📋 약간의 과적합 경향\")\n",
    "                    else:\n",
    "                        print(\"  ✅ 적절한 일반화 성능\")\n",
    "            \n",
    "            # mAP 분석\n",
    "            if 'val_map' in history:\n",
    "                map_scores = history['val_map']\n",
    "                print(f\"\\n🎯 mAP 성능 분석:\")\n",
    "                print(f\"  🔹 최종 mAP: {map_scores[-1]:.4f}\")\n",
    "                \n",
    "                if len(map_scores) > 1:\n",
    "                    best_map = max(map_scores)\n",
    "                    best_map_epoch = map_scores.index(best_map)\n",
    "                    print(f\"  🏆 최고 mAP: {best_map:.4f} (에포크 {best_map_epoch})\")\n",
    "                    \n",
    "                    # 성능 개선 추이\n",
    "                    if len(map_scores) > 3:\n",
    "                        early_avg = np.mean(map_scores[:3])\n",
    "                        late_avg = np.mean(map_scores[-3:])\n",
    "                        improvement = late_avg - early_avg\n",
    "                        \n",
    "                        if improvement > 0.05:\n",
    "                            print(\"  📈 성능 크게 개선됨\")\n",
    "                        elif improvement > 0.01:\n",
    "                            print(\"  📊 성능 약간 개선됨\")\n",
    "                        else:\n",
    "                            print(\"  📉 성능 개선 제한적\")\n",
    "                            \n",
    "    else:\n",
    "        print(\"❌ 체크포인트 파일을 찾을 수 없습니다\")\n",
    "\n",
    "def suggest_improvements():\n",
    "    \"\"\"성능 개선 제안\"\"\"\n",
    "    print(\"\\n💡 성능 개선 제안\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    suggestions = [\n",
    "        {\n",
    "            \"category\": \"🏗️ 모델 구조\",\n",
    "            \"items\": [\n",
    "                \"더 깊은 백본 네트워크 사용 (ResNet50, EfficientNet 등)\",\n",
    "                \"Multi-scale 특성 추출을 위한 FPN (Feature Pyramid Network) 추가\",\n",
    "                \"Attention 메커니즘 도입\",\n",
    "                \"더 많은 앵커 박스 크기 및 비율 실험\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"📊 데이터 및 전처리\",\n",
    "            \"items\": [\n",
    "                \"데이터 증강 기법 적용 (회전, 크기 변경, 색상 조정 등)\",\n",
    "                \"더 많은 학습 데이터 수집\",\n",
    "                \"하드 네거티브 마이닝 (Hard Negative Mining)\",\n",
    "                \"클래스 불균형 문제 해결 (가중치 조정, 오버샘플링 등)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"🎛️ 학습 설정\",\n",
    "            \"items\": [\n",
    "                \"학습률 스케줄링 (CosineAnnealingLR, ReduceLROnPlateau 등)\",\n",
    "                \"더 긴 학습 시간 (더 많은 에포크)\",\n",
    "                \"배치 크기 조정 실험\",\n",
    "                \"옵티마이저 변경 (Adam, AdamW 등)\",\n",
    "                \"그래디언트 클리핑 값 조정\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"🔧 손실 함수\",\n",
    "            \"items\": [\n",
    "                \"Focal Loss 도입 (클래스 불균형 해결)\",\n",
    "                \"IoU Loss, GIoU Loss 등 박스 회귀 손실 개선\",\n",
    "                \"Multi-task 손실 가중치 조정\",\n",
    "                \"Label Smoothing 적용\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"📈 후처리\",\n",
    "            \"items\": [\n",
    "                \"NMS (Non-Maximum Suppression) 임계값 튜닝\",\n",
    "                \"신뢰도 임계값 최적화\",\n",
    "                \"Test Time Augmentation (TTA) 적용\",\n",
    "                \"앙상블 방법 도입\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for suggestion in suggestions:\n",
    "        print(f\"\\n{suggestion['category']}\")\n",
    "        for i, item in enumerate(suggestion['items'], 1):\n",
    "            print(f\"  {i}. {item}\")\n",
    "\n",
    "def performance_checklist():\n",
    "    \"\"\"성능 체크리스트\"\"\"\n",
    "    print(\"\\n✅ 성능 개선 체크리스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    checklist = [\n",
    "        \"[ ] 학습 데이터의 품질과 다양성 확인\",\n",
    "        \"[ ] 데이터 전처리 및 증강 기법 적용\",\n",
    "        \"[ ] 적절한 모델 복잡도 선택 (과적합 vs 과소적합)\",\n",
    "        \"[ ] 하이퍼파라미터 튜닝 (학습률, 배치 크기 등)\",\n",
    "        \"[ ] 손실 함수 개선 (Focal Loss, IoU Loss 등)\",\n",
    "        \"[ ] 정규화 기법 적용 (Dropout, BatchNorm 등)\",\n",
    "        \"[ ] 학습률 스케줄링 적용\",\n",
    "        \"[ ] Early Stopping 및 체크포인트 전략\",\n",
    "        \"[ ] 검증 데이터에서 성능 모니터링\",\n",
    "        \"[ ] 테스트 데이터에서 최종 평가\"\n",
    "    ]\n",
    "    \n",
    "    for item in checklist:\n",
    "        print(f\"  {item}\")\n",
    "    \n",
    "    print(f\"\\n📋 다음 단계 권장 사항:\")\n",
    "    print(f\"1. 위 체크리스트 항목들을 순서대로 검토\")\n",
    "    print(f\"2. 가장 영향이 클 것 같은 항목부터 우선 적용\")\n",
    "    print(f\"3. 각 변경사항 적용 후 성능 측정 및 비교\")\n",
    "    print(f\"4. 체계적인 실험 기록 및 관리\")\n",
    "\n",
    "# 실행\n",
    "analyze_training_performance()\n",
    "suggest_improvements()\n",
    "performance_checklist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2535864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 모델 저장 및 다운로드\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "def save_training_results():\n",
    "    \"\"\"학습 결과를 Google Drive에 저장\"\"\"\n",
    "    print(\"💾 학습 결과 저장\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Google Drive 저장 경로 설정\n",
    "    drive_save_path = \"/content/drive/MyDrive/sprint-ai03-team04-results\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_dir = f\"{drive_save_path}/training_session_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        # 저장 디렉토리 생성\n",
    "        os.makedirs(session_dir, exist_ok=True)\n",
    "        print(f\"📁 저장 디렉토리 생성: {session_dir}\")\n",
    "        \n",
    "        # outputs 디렉토리 전체 복사\n",
    "        if os.path.exists(outputs_dir):\n",
    "            target_outputs = os.path.join(session_dir, \"outputs\")\n",
    "            shutil.copytree(outputs_dir, target_outputs, dirs_exist_ok=True)\n",
    "            print(f\"✅ 학습 결과 복사 완료: {target_outputs}\")\n",
    "            \n",
    "            # 복사된 파일 목록 출력\n",
    "            copied_files = os.listdir(target_outputs)\n",
    "            print(f\"📋 저장된 파일들:\")\n",
    "            for file in copied_files:\n",
    "                file_path = os.path.join(target_outputs, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                    print(f\"  📄 {file} ({size:.2f} MB)\")\n",
    "        else:\n",
    "            print(\"❌ outputs 디렉토리를 찾을 수 없습니다\")\n",
    "            \n",
    "        return session_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 저장 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_best_model():\n",
    "    \"\"\"최고 성능 모델 다운로드\"\"\"\n",
    "    print(\"\\n📥 최고 성능 모델 다운로드\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            # 파일 크기 확인\n",
    "            file_size = os.path.getsize(best_model_path) / (1024 * 1024)  # MB\n",
    "            print(f\"📦 모델 파일: best_model.pth ({file_size:.2f} MB)\")\n",
    "            \n",
    "            # 다운로드 실행\n",
    "            print(\"🔽 다운로드 시작...\")\n",
    "            files.download(best_model_path)\n",
    "            print(\"✅ 다운로드 완료!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 다운로드 실패: {e}\")\n",
    "    else:\n",
    "        print(\"❌ best_model.pth 파일을 찾을 수 없습니다\")\n",
    "\n",
    "def create_training_summary():\n",
    "    \"\"\"학습 요약 보고서 생성\"\"\"\n",
    "    print(\"\\n📄 학습 요약 보고서 생성\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    summary_content = []\n",
    "    summary_content.append(\"# 학습 요약 보고서\")\n",
    "    summary_content.append(f\"생성 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    summary_content.append(\"\")\n",
    "    \n",
    "    # 체크포인트 정보\n",
    "    best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        \n",
    "        summary_content.append(\"## 학습 결과\")\n",
    "        if 'epoch' in checkpoint:\n",
    "            summary_content.append(f\"- 최고 성능 에포크: {checkpoint['epoch']}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            summary_content.append(f\"- 최고 성능 시 검증 손실: {checkpoint['val_loss']:.6f}\")\n",
    "        \n",
    "        if 'history' in checkpoint:\n",
    "            history = checkpoint['history']\n",
    "            if 'val_map' in history and history['val_map']:\n",
    "                best_map = max(history['val_map'])\n",
    "                summary_content.append(f\"- 최고 mAP: {best_map:.4f}\")\n",
    "    \n",
    "    summary_content.append(\"\")\n",
    "    summary_content.append(\"## 파일 목록\")\n",
    "    \n",
    "    if os.path.exists(outputs_dir):\n",
    "        for file in os.listdir(outputs_dir):\n",
    "            file_path = os.path.join(outputs_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                summary_content.append(f\"- {file} ({size:.2f} MB)\")\n",
    "    \n",
    "    # 요약 파일 저장\n",
    "    summary_path = os.path.join(outputs_dir, 'training_summary.md')\n",
    "    try:\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\\\n'.join(summary_content))\n",
    "        print(f\"✅ 요약 보고서 생성 완료: {summary_path}\")\n",
    "        \n",
    "        # 내용 출력\n",
    "        print(\"\\\\n📋 요약 내용:\")\n",
    "        for line in summary_content:\n",
    "            print(f\"  {line}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 요약 보고서 생성 실패: {e}\")\n",
    "\n",
    "def backup_and_download():\n",
    "    \"\"\"전체 백업 및 다운로드 실행\"\"\"\n",
    "    print(\"🚀 학습 결과 백업 및 다운로드 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 요약 보고서 생성\n",
    "    create_training_summary()\n",
    "    \n",
    "    # 2. Google Drive에 저장\n",
    "    saved_dir = save_training_results()\n",
    "    if saved_dir:\n",
    "        print(f\"\\\\n✅ Google Drive 저장 완료: {saved_dir}\")\n",
    "    \n",
    "    # 3. 최고 성능 모델 다운로드\n",
    "    download_best_model()\n",
    "    \n",
    "    print(\"\\\\n🎉 모든 작업 완료!\")\n",
    "    print(\"\\\\n📋 저장된 내용:\")\n",
    "    print(\"1. Google Drive에 전체 학습 결과 백업\")\n",
    "    print(\"2. 로컬에 best_model.pth 다운로드\")\n",
    "    print(\"3. training_summary.md 요약 보고서\")\n",
    "\n",
    "# 사용법 안내\n",
    "print(\"💾 모델 저장 및 다운로드 도구 로드 완료\")\n",
    "print(\"\\\\n📋 사용 방법:\")\n",
    "print(\"1. backup_and_download() - 전체 백업 및 다운로드\")\n",
    "print(\"2. download_best_model() - 최고 성능 모델만 다운로드\")\n",
    "print(\"3. save_training_results() - Google Drive에만 저장\")\n",
    "print(\"\\\\n⚠️  주의사항:\")\n",
    "print(\"- Google Drive가 마운트되어 있는지 확인하세요\")\n",
    "print(\"- 충분한 Drive 저장 공간이 있는지 확인하세요\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
