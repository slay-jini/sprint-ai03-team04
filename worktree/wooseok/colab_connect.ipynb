{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc697ea",
   "metadata": {},
   "source": [
    "# Google Colabì—ì„œ Git Cloneì„ í†µí•œ í”„ë¡œì íŠ¸ ì‚¬ìš© ê°€ì´ë“œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Google Colabì—ì„œ GitHub ë¦¬í¬ì§€í† ë¦¬ë¥¼ í´ë¡ í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë¡œì»¬ ê°œë°œ í™˜ê²½ ì—†ì´ë„ Colabì˜ GPU ë¦¬ì†ŒìŠ¤ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a3640",
   "metadata": {},
   "source": [
    "## 1. Colab í™˜ê²½ ì„¤ì •\n",
    "\n",
    "Google Colabì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê¸°ë³¸ ì„¤ì •:\n",
    "\n",
    "1. **ëŸ°íƒ€ì„ íƒ€ì… ì„¤ì •**\n",
    "   - ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > GPU ì„ íƒ\n",
    "   - ê³ ì‚¬ì–‘ RAM ì„ íƒ (í•„ìš”ì‹œ)\n",
    "\n",
    "2. **í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**\n",
    "   - PyTorch, torchvision ë“± ML ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "   - ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "3. **Google Drive ì—°ê²°** (ì„ íƒì‚¬í•­)\n",
    "   - ë°ì´í„°ì…‹ì´ Driveì— ì €ì¥ëœ ê²½ìš°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e32248",
   "metadata": {},
   "source": [
    "## 2. GitHub ë¦¬í¬ì§€í† ë¦¬ ë¸Œëœì¹˜ í´ë¡ \n",
    "\n",
    "**ê°œë°œ ì¤‘ì¸ ì½”ë“œë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ íŠ¹ì • ë¸Œëœì¹˜ì—ì„œ í´ë¡ **:\n",
    "\n",
    "1. **ë¸Œëœì¹˜ í´ë¡  ì˜µì…˜**:\n",
    "   - íŠ¹ì • ë¸Œëœì¹˜ ì§ì ‘ í´ë¡ : `git clone -b branch_name`\n",
    "   - ëª¨ë“  ë¸Œëœì¹˜ í´ë¡  í›„ ì²´í¬ì•„ì›ƒ\n",
    "   - í˜„ì¬ ê°œë°œ ë¸Œëœì¹˜: `feature/wooseok-baseline-v0`\n",
    "\n",
    "2. **ë¸Œëœì¹˜ í™•ì¸ ë° ì „í™˜**:\n",
    "   - í˜„ì¬ ë¸Œëœì¹˜ í™•ì¸\n",
    "   - ë‹¤ë¥¸ ë¸Œëœì¹˜ë¡œ ì „í™˜\n",
    "   - ìµœì‹  ë³€ê²½ì‚¬í•­ ì—…ë°ì´íŠ¸\n",
    "\n",
    "3. **ì˜ì¡´ì„± ì„¤ì¹˜**:\n",
    "   - í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "   - í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25547588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install torch torchvision pillow\n",
    "\n",
    "# 2. ê°œë°œ ì¤‘ì¸ ë¸Œëœì¹˜ì—ì„œ ì§ì ‘ í´ë¡  (ê¶Œì¥)\n",
    "BRANCH_NAME = \"feature/wooseok-baseline-v0\"  # í˜„ì¬ ê°œë°œ ë¸Œëœì¹˜\n",
    "print(f\"ğŸŒ¿ í´ë¡ í•  ë¸Œëœì¹˜: {BRANCH_NAME}\")\n",
    "\n",
    "!git clone -b {BRANCH_NAME} https://github.com/slay-jini/sprint-ai03-team04.git\n",
    "\n",
    "# 3. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™\n",
    "import os\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# 4. ë¸Œëœì¹˜ í™•ì¸\n",
    "print(\"\\nğŸ” í˜„ì¬ ë¸Œëœì¹˜ í™•ì¸:\")\n",
    "!git branch -a\n",
    "!git status\n",
    "\n",
    "# 5. í˜„ì¬ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ í™•ì¸\n",
    "print(f\"\\nğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "print(\"\\nğŸ“‹ í”„ë¡œì íŠ¸ íŒŒì¼ ëª©ë¡:\")\n",
    "!ls -la\n",
    "\n",
    "# 6. GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "import torch\n",
    "print(f\"\\nğŸ® CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU ëª¨ë¸: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€ì•ˆ ë°©ë²•: ë‹¤ë¥¸ ë¸Œëœì¹˜ë¡œ ì „í™˜í•˜ê±°ë‚˜ ìµœì‹  ë³€ê²½ì‚¬í•­ ì—…ë°ì´íŠ¸\n",
    "\n",
    "# ì˜µì…˜ 1: ì´ë¯¸ í´ë¡ ëœ ìƒíƒœì—ì„œ ë‹¤ë¥¸ ë¸Œëœì¹˜ë¡œ ì „í™˜\n",
    "def switch_branch(branch_name):\n",
    "    \"\"\"ë‹¤ë¥¸ ë¸Œëœì¹˜ë¡œ ì „í™˜\"\"\"\n",
    "    print(f\"ğŸ”„ ë¸Œëœì¹˜ ì „í™˜: {branch_name}\")\n",
    "    !git checkout {branch_name}\n",
    "    !git pull origin {branch_name}\n",
    "    print(f\"âœ… {branch_name} ë¸Œëœì¹˜ë¡œ ì „í™˜ ì™„ë£Œ\")\n",
    "\n",
    "# ì˜µì…˜ 2: ëª¨ë“  ë¸Œëœì¹˜ í™•ì¸\n",
    "def list_all_branches():\n",
    "    \"\"\"ëª¨ë“  ë¸Œëœì¹˜ ëª©ë¡ í™•ì¸\"\"\"\n",
    "    print(\"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë¸Œëœì¹˜ ëª©ë¡:\")\n",
    "    !git branch -r\n",
    "\n",
    "# ì˜µì…˜ 3: í˜„ì¬ ë¸Œëœì¹˜ ìµœì‹  ìƒíƒœë¡œ ì—…ë°ì´íŠ¸\n",
    "def update_current_branch():\n",
    "    \"\"\"í˜„ì¬ ë¸Œëœì¹˜ ìµœì‹  ìƒíƒœë¡œ ì—…ë°ì´íŠ¸\"\"\"\n",
    "    current_branch = !git branch --show-current\n",
    "    branch_name = current_branch[0].strip() if current_branch else \"main\"\n",
    "    print(f\"ğŸ”„ í˜„ì¬ ë¸Œëœì¹˜ ì—…ë°ì´íŠ¸: {branch_name}\")\n",
    "    !git pull origin {branch_name}\n",
    "    print(\"âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
    "# list_all_branches()\n",
    "# switch_branch(\"main\")  # main ë¸Œëœì¹˜ë¡œ ì „í™˜\n",
    "# update_current_branch()  # í˜„ì¬ ë¸Œëœì¹˜ ì—…ë°ì´íŠ¸\n",
    "\n",
    "print(\"ğŸ› ï¸  ë¸Œëœì¹˜ ê´€ë¦¬ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜:\")\n",
    "print(\"- list_all_branches(): ëª¨ë“  ë¸Œëœì¹˜ ëª©ë¡ í™•ì¸\")\n",
    "print(\"- switch_branch('ë¸Œëœì¹˜ëª…'): ë‹¤ë¥¸ ë¸Œëœì¹˜ë¡œ ì „í™˜\")\n",
    "print(\"- update_current_branch(): í˜„ì¬ ë¸Œëœì¹˜ ì—…ë°ì´íŠ¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5e578",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„°ì…‹ ì„¤ì • ë° ì¤€ë¹„\n",
    "\n",
    "**âš ï¸ ì¤‘ìš”**: í˜„ì¬ ë¸Œëœì¹˜ì—ì„œëŠ” `dataset/` í´ë”ê°€ `.gitignore`ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. **ë°ì´í„°ì…‹ ì—…ë¡œë“œ**:\n",
    "   - Google Drive ë§ˆìš´íŠ¸ ë˜ëŠ” ì§ì ‘ ì—…ë¡œë“œ\n",
    "   - ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (Gitì—ì„œ ì œì™¸ë¨)\n",
    "   - íŒŒì¼ êµ¬ì¡° í™•ì¸\n",
    "\n",
    "2. **ì„¤ì • íŒŒì¼ ìˆ˜ì •**:\n",
    "   - config.pyì—ì„œ ê²½ë¡œ ì„¤ì •\n",
    "   - ë°°ì¹˜ í¬ê¸° ë° í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "   - ë¸Œëœì¹˜ë³„ ì„¤ì • ì°¨ì´ í™•ì¸\n",
    "\n",
    "3. **ë¸Œëœì¹˜ë³„ ì°¨ì´ì **:\n",
    "   - ê°œë°œ ë¸Œëœì¹˜ì˜ ìµœì‹  ê¸°ëŠ¥ ì‚¬ìš©\n",
    "   - ì‹¤í—˜ì  ì½”ë“œ í¬í•¨ ê°€ëŠ¥ì„±\n",
    "   - main ë¸Œëœì¹˜ ëŒ€ë¹„ ì¶”ê°€ ê¸°ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. .gitignore í™•ì¸ (dataset í´ë” ì œì™¸ ì—¬ë¶€)\n",
    "print(\"ğŸ“‹ .gitignore íŒŒì¼ í™•ì¸:\")\n",
    "!cat /content/sprint-ai03-team04/.gitignore | grep -E \"(dataset|data)\" || echo \"dataset ê´€ë ¨ í•­ëª© ì—†ìŒ\"\n",
    "\n",
    "# 2. Google Drive ë§ˆìš´íŠ¸ (ë°ì´í„°ì…‹ì´ Driveì— ìˆëŠ” ê²½ìš°)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 3. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "import os\n",
    "\n",
    "# âš ï¸ ì¤‘ìš”: dataset í´ë”ê°€ .gitignoreì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "print(\"\\nğŸš¨ ë°ì´í„°ì…‹ ì¤€ë¹„ ë°©ë²•:\")\n",
    "print(\"1. Google Driveì—ì„œ ë°ì´í„°ì…‹ì„ ë³µì‚¬í•˜ê±°ë‚˜\")\n",
    "print(\"2. ì§ì ‘ ì—…ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”\")\n",
    "\n",
    "# ì˜µì…˜ 1: Google Driveì—ì„œ ë°ì´í„°ì…‹ ë³µì‚¬\n",
    "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/your_dataset\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "PROJECT_DATASET_PATH = \"/content/sprint-ai03-team04/worktree/wooseok/dataset\"\n",
    "\n",
    "# ì˜µì…˜ 2: ì§ì ‘ ì—…ë¡œë“œí•œ ê²½ìš°ì˜ ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "DATASET_PATH = \"/content/dataset\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "\n",
    "print(f\"\\nğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ ì˜µì…˜:\")\n",
    "print(f\"1. Google Drive: {DRIVE_DATASET_PATH}\")\n",
    "print(f\"2. í”„ë¡œì íŠ¸ ë‚´: {PROJECT_DATASET_PATH}\")\n",
    "print(f\"3. ì§ì ‘ ì—…ë¡œë“œ: {DATASET_PATH}\")\n",
    "\n",
    "# 4. ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²½ë¡œ ì„ íƒ (ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ)\n",
    "SELECTED_DATASET = DRIVE_DATASET_PATH  # ì‹¤ì œ ìƒí™©ì— ë§ê²Œ ë³€ê²½\n",
    "\n",
    "# 5. ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
    "if os.path.exists(SELECTED_DATASET):\n",
    "    print(f\"\\nâœ… ë°ì´í„°ì…‹ ê²½ë¡œ ë°œê²¬: {SELECTED_DATASET}\")\n",
    "    print(\"ğŸ“‹ ë°ì´í„°ì…‹ êµ¬ì¡°:\")\n",
    "    !ls -la {SELECTED_DATASET}\n",
    "    \n",
    "    # train.json íŒŒì¼ í™•ì¸\n",
    "    if os.path.exists(f\"{SELECTED_DATASET}/train.json\"):\n",
    "        print(f\"âœ… train.json íŒŒì¼ ë°œê²¬\")\n",
    "    else:\n",
    "        print(\"âŒ train.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "    if os.path.exists(f\"{SELECTED_DATASET}/images/train\"):\n",
    "        img_count = len(os.listdir(f\"{SELECTED_DATASET}/images/train\"))\n",
    "        print(f\"âœ… ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ë°œê²¬ ({img_count}ê°œ íŒŒì¼)\")\n",
    "    else:\n",
    "        print(\"âŒ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"\\nâŒ ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_DATASET}\")\n",
    "    print(\"ğŸ“‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ê°€ì´ë“œ:\")\n",
    "    print(\"1. Google Driveì— ë°ì´í„°ì…‹ ì—…ë¡œë“œ\")\n",
    "    print(\"2. ê²½ë¡œë¥¼ SELECTED_DATASET ë³€ìˆ˜ì— ì„¤ì •\")\n",
    "    print(\"3. ë˜ëŠ” Colabì— ì§ì ‘ ì—…ë¡œë“œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bae6eb",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "í”„ë¡œì íŠ¸ ì„¤ì •ì´ ì™„ë£Œë˜ë©´ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **ë°ì´í„°ì…‹ ìƒì„±**:\n",
    "   - ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±\n",
    "   - ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "   - ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í™•ì¸\n",
    "\n",
    "2. **í•™ìŠµ ì‹¤í–‰**:\n",
    "   - ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
    "   - í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§\n",
    "   - ê²°ê³¼ ì €ì¥\n",
    "\n",
    "3. **ì£¼ì˜ì‚¬í•­**:\n",
    "   - Colab ì„¸ì…˜ ì‹œê°„ ì œí•œ (12ì‹œê°„)\n",
    "   - ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì²´í¬í¬ì¸íŠ¸)\n",
    "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa317b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ì„¤ì • íŒŒì¼ í™•ì¸ ë° ìˆ˜ì •\n",
    "import sys\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# config.py ë‚´ìš© í™•ì¸\n",
    "try:\n",
    "    from config import cfg\n",
    "    print(\"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ\")\n",
    "    print(f\"ë””ë°”ì´ìŠ¤: {cfg.DEVICE}\")\n",
    "    print(f\"ë°°ì¹˜ í¬ê¸°: {cfg.BATCH_SIZE}\")\n",
    "    print(f\"ì—í¬í¬: {cfg.EPOCHS}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "print(f\"\\n=== ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ===\")\n",
    "print(f\"RAM ì‚¬ìš©ëŸ‰: {psutil.virtual_memory().percent}%\")\n",
    "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"GPU ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / (1024**3):.2f} GB\")\n",
    "\n",
    "print(\"\\n=== í™˜ê²½ ì„¤ì • ì™„ë£Œ ===\")\n",
    "print(\"ì´ì œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê³  í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c05d0",
   "metadata": {},
   "source": [
    "## ê°œë°œ ë¸Œëœì¹˜ì—ì„œ ë°ì´í„°ì…‹ ìƒì„± ë° í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "**ğŸŒ¿ í˜„ì¬ ë¸Œëœì¹˜**: `feature/wooseok-baseline-v0`\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë°œ ì¤‘ì¸ ë¸Œëœì¹˜ì—ì„œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê³  í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "main ë¸Œëœì¹˜ì— mergeí•˜ê¸° ì „ì— ì—¬ê¸°ì„œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colabì—ì„œ í´ë¡ í•œ í”„ë¡œì íŠ¸ë¡œ ë°ì´í„°ì…‹ ìƒì„± ë° í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "# 1. ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸ ë° ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "print(f\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”)\n",
    "DATASET_JSON = \"/content/dataset/train.json\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "DATASET_IMAGES = \"/content/dataset/images/train\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "\n",
    "# ë˜ëŠ” Google Driveì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²½ìš°:\n",
    "# DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"\n",
    "# DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"\n",
    "\n",
    "# 3. ë°ì´í„°ì…‹ ìƒì„±\n",
    "try:\n",
    "    from dataset import create_colab_dataset\n",
    "    \n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {len(dataset)} ê°œì˜ ìƒ˜í”Œ\")\n",
    "        \n",
    "        # 4. í•™ìŠµ ì‹¤í–‰\n",
    "        from train import train_with_dataset\n",
    "        print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n",
    "        print(\"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "        print(f\"JSON íŒŒì¼: {DATASET_JSON}\")\n",
    "        print(f\"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {DATASET_IMAGES}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"í”„ë¡œì íŠ¸ íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ í´ë¡ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€ì•ˆ ë°©ë²•: ë‹¨ê³„ë³„ë¡œ í•™ìŠµ ì‹¤í–‰í•˜ê¸°\n",
    "\n",
    "# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "print(\"ğŸ“ í”„ë¡œì íŠ¸ íŒŒì¼ í™•ì¸:\")\n",
    "required_files = ['train.py', 'dataset.py', 'config.py', 'trainer.py']\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"âœ… {file}\")\n",
    "    else:\n",
    "        print(f\"âŒ {file} - íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ íŒŒì¼ ì§ì ‘ ì§€ì •\n",
    "# Google Drive ë˜ëŠ” ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”\n",
    "json_path = \"/content/drive/MyDrive/your_dataset/train.json\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "img_dir = \"/content/drive/MyDrive/your_dataset/images/train\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ê²½ë¡œ:\")\n",
    "print(f\"JSON: {json_path}\")\n",
    "print(f\"Images: {img_dir}\")\n",
    "\n",
    "# 3. ê²½ë¡œ ì¡´ì¬ í™•ì¸\n",
    "json_exists = os.path.exists(json_path)\n",
    "img_exists = os.path.exists(img_dir)\n",
    "\n",
    "print(f\"\\nğŸ“‹ ê²½ë¡œ í™•ì¸:\")\n",
    "print(f\"JSON íŒŒì¼ ì¡´ì¬: {json_exists}\")\n",
    "print(f\"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì¡´ì¬: {img_exists}\")\n",
    "\n",
    "if json_exists and img_exists:\n",
    "    print(f\"ì´ë¯¸ì§€ ê°œìˆ˜: {len(os.listdir(img_dir))}\")\n",
    "    \n",
    "    # 4. ë°ì´í„°ì…‹ ìƒì„± ë° í•™ìŠµ\n",
    "    try:\n",
    "        from dataset import PillDetectionDataset\n",
    "        from train import train_with_dataset\n",
    "        \n",
    "        dataset = PillDetectionDataset(\n",
    "            json_file=json_path,\n",
    "            img_dir=img_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        # í•™ìŠµ ì‹¤í–‰\n",
    "        print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í•™ìŠµ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
    "    print(\"Google Driveë¥¼ ë§ˆìš´íŠ¸í–ˆëŠ”ì§€, ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d3a9f",
   "metadata": {},
   "source": [
    "## 5. ë¸Œëœì¹˜ ê°œë°œ ë° ë¬¸ì œ í•´ê²°\n",
    "\n",
    "### ğŸŒ¿ ë¸Œëœì¹˜ ê´€ë ¨ íŒ\n",
    "\n",
    "1. **í˜„ì¬ ë¸Œëœì¹˜ í™•ì¸**:\n",
    "   - `!git branch --show-current`\n",
    "   - `!git status`\n",
    "\n",
    "2. **ë¸Œëœì¹˜ ì „í™˜**:\n",
    "   - `!git checkout main` (main ë¸Œëœì¹˜ë¡œ)\n",
    "   - `!git checkout feature/wooseok-baseline-v0` (ê°œë°œ ë¸Œëœì¹˜ë¡œ)\n",
    "\n",
    "3. **ìµœì‹  ë³€ê²½ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°**:\n",
    "   - `!git pull origin feature/wooseok-baseline-v0`\n",
    "\n",
    "### ğŸ“ ë°ì´í„°ì…‹ ê´€ë¦¬\n",
    "\n",
    "1. **Gitì—ì„œ ì œì™¸ëœ íŒŒì¼ë“¤**:\n",
    "   - `dataset/` í´ë” (`.gitignore`ì— í¬í•¨)\n",
    "   - `__pycache__/` í´ë”\n",
    "   - ë¡œê·¸ íŒŒì¼, ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸\n",
    "\n",
    "2. **ë°ì´í„°ì…‹ ì¤€ë¹„ ë°©ë²•**:\n",
    "   - Google Drive ì—…ë¡œë“œ í›„ ë§ˆìš´íŠ¸\n",
    "   - ì§ì ‘ Colabì— ì—…ë¡œë“œ\n",
    "   - ì™¸ë¶€ ë§í¬ì—ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "### ğŸš¨ ê°œë°œ ë¸Œëœì¹˜ ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "- ê°œë°œ ì¤‘ì¸ ì½”ë“œë¡œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ê°€ëŠ¥\n",
    "- ì‹¤í—˜ì  ê¸°ëŠ¥ í¬í•¨ ê°€ëŠ¥ì„±\n",
    "- main ë¸Œëœì¹˜ ëŒ€ë¹„ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ\n",
    "- í…ŒìŠ¤íŠ¸ í›„ ì´ìƒ ì—†ìœ¼ë©´ mainìœ¼ë¡œ merge\n",
    "\n",
    "### ğŸ”„ merge ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "- [ ] ì½”ë“œ ì •ìƒ ì‹¤í–‰ í™•ì¸\n",
    "- [ ] ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n",
    "- [ ] ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ\n",
    "- [ ] ì„±ëŠ¥ ì €í•˜ ì—†ìŒ\n",
    "- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸ í•„ìš”ì‹œ ë°˜ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d279a5",
   "metadata": {},
   "source": [
    "## ğŸš¨ CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²° ë°©ë²•\n",
    "\n",
    "**\"CUDA error: an illegal memory access was encountered\"** ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°:\n",
    "\n",
    "### ğŸ“‹ ì¼ë°˜ì ì¸ ì›ì¸ë“¤:\n",
    "1. **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±** - ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ í¼\n",
    "2. **í…ì„œ í¬ê¸° ë¶ˆì¼ì¹˜** - ëª¨ë¸ ì…ë ¥ê³¼ ì‹¤ì œ ë°ì´í„° í¬ê¸° ì°¨ì´\n",
    "3. **CUDA ë²„ì „ í˜¸í™˜ì„±** - PyTorchì™€ CUDA ë²„ì „ ë¶ˆì¼ì¹˜\n",
    "4. **ë©”ëª¨ë¦¬ ëˆ„ìˆ˜** - ì´ì „ ì‹¤í–‰ì—ì„œ ë‚¨ì€ GPU ë©”ëª¨ë¦¬\n",
    "\n",
    "### ğŸ”§ í•´ê²° ë°©ë²•ë“¤:\n",
    "1. **GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œì‘**\n",
    "2. **ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°**\n",
    "3. **ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”**\n",
    "4. **ë°ì´í„° ë¡œë” worker ìˆ˜ ì¡°ì •**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7119c7a",
   "metadata": {},
   "source": [
    "## ğŸ”„ PyTorch ìˆœí™˜ Import ì˜¤ë¥˜ í•´ê²°\n",
    "\n",
    "**\"partially initialized module 'torch._dynamo' has no attribute 'config'\"** ì˜¤ë¥˜ í•´ê²°:\n",
    "\n",
    "### ğŸ“‹ ì›ì¸ ë¶„ì„:\n",
    "1. **ìˆœí™˜ Import** - í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ ê°„ì˜ circular import\n",
    "2. **PyTorch ë²„ì „ ë¬¸ì œ** - ë²„ì „ í˜¸í™˜ì„± ì´ìŠˆ\n",
    "3. **ëª¨ë“ˆ ì´ˆê¸°í™” ì˜¤ë¥˜** - ëª¨ë“ˆì´ ì™„ì „íˆ ë¡œë“œë˜ê¸° ì „ ì ‘ê·¼\n",
    "4. **ìºì‹œëœ ëª¨ë“ˆ** - ì´ì „ ì‹¤í–‰ì˜ ì˜ëª»ëœ ìºì‹œ\n",
    "\n",
    "### ğŸ”§ í•´ê²° ë°©ë²•:\n",
    "1. **íŒŒì´ì¬ ëª¨ë“ˆ ìºì‹œ ì •ë¦¬**\n",
    "2. **Import ìˆœì„œ ì¡°ì •**\n",
    "3. **PyTorch ì¬ì„¤ì¹˜**\n",
    "4. **ëŸ°íƒ€ì„ ì¬ì‹œì‘**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba87857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ìˆœí™˜ Import ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì¢…í•© ì†”ë£¨ì…˜\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import gc\n",
    "\n",
    "def clean_python_modules():\n",
    "    \"\"\"Python ëª¨ë“ˆ ìºì‹œ ì •ë¦¬\"\"\"\n",
    "    print(\"ğŸ§¹ Python ëª¨ë“ˆ ìºì‹œ ì •ë¦¬ ì¤‘...\")\n",
    "    \n",
    "    # 1. í”„ë¡œì íŠ¸ ê´€ë ¨ ëª¨ë“ˆ ì œê±°\n",
    "    modules_to_remove = []\n",
    "    for module_name in list(sys.modules.keys()):\n",
    "        if any(keyword in module_name for keyword in ['config', 'dataset', 'train', 'trainer', 'torch._dynamo']):\n",
    "            modules_to_remove.append(module_name)\n",
    "    \n",
    "    for module_name in modules_to_remove:\n",
    "        if module_name in sys.modules:\n",
    "            del sys.modules[module_name]\n",
    "            print(f\"  âœ… {module_name} ëª¨ë“ˆ ì œê±°\")\n",
    "    \n",
    "    # 2. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n",
    "    gc.collect()\n",
    "    print(\"âœ… ëª¨ë“ˆ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "def fix_pytorch_import():\n",
    "    \"\"\"PyTorch Import ë¬¸ì œ í•´ê²°\"\"\"\n",
    "    print(\"ğŸ”§ PyTorch Import ë¬¸ì œ í•´ê²° ì¤‘...\")\n",
    "    \n",
    "    # 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "    os.environ['PYTHONDONTWRITEBYTECODE'] = '1'  # .pyc íŒŒì¼ ìƒì„± ë°©ì§€\n",
    "    \n",
    "    # 2. PyTorch ëª¨ë“ˆ ê°•ì œ ì¬ë¡œë“œ\n",
    "    try:\n",
    "        import torch\n",
    "        importlib.reload(torch)\n",
    "        print(\"âœ… PyTorch ëª¨ë“ˆ ì¬ë¡œë“œ ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PyTorch ì¬ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # 3. torch._dynamo ì§ì ‘ í™•ì¸\n",
    "    try:\n",
    "        import torch._dynamo\n",
    "        if hasattr(torch._dynamo, 'config'):\n",
    "            print(\"âœ… torch._dynamo.config ì •ìƒ ì ‘ê·¼ ê°€ëŠ¥\")\n",
    "        else:\n",
    "            print(\"âŒ torch._dynamo.config ì†ì„± ì—†ìŒ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ torch._dynamo ì ‘ê·¼ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "def check_circular_imports():\n",
    "    \"\"\"ìˆœí™˜ import í™•ì¸\"\"\"\n",
    "    print(\"ğŸ” ìˆœí™˜ import í™•ì¸ ì¤‘...\")\n",
    "    \n",
    "    # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "    project_dir = '/content/sprint-ai03-team04/worktree/wooseok'\n",
    "    \n",
    "    if os.path.exists(project_dir):\n",
    "        os.chdir(project_dir)\n",
    "        \n",
    "        # í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ í™•ì¸\n",
    "        project_files = ['config.py', 'dataset.py', 'train.py', 'trainer.py']\n",
    "        \n",
    "        for file in project_files:\n",
    "            if os.path.exists(file):\n",
    "                print(f\"ğŸ“„ {file} íŒŒì¼ ì¡´ì¬\")\n",
    "                \n",
    "                # ê° íŒŒì¼ì˜ import êµ¬ë¬¸ í™•ì¸\n",
    "                try:\n",
    "                    with open(file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        imports = [line.strip() for line in content.split('\\n') \n",
    "                                 if line.strip().startswith('import ') or line.strip().startswith('from ')]\n",
    "                        \n",
    "                        print(f\"  Import êµ¬ë¬¸ ({len(imports)}ê°œ):\")\n",
    "                        for imp in imports[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ\n",
    "                            print(f\"    {imp}\")\n",
    "                        if len(imports) > 5:\n",
    "                            print(f\"    ... ë° {len(imports) - 5}ê°œ ë”\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "            else:\n",
    "                print(f\"âŒ {file} íŒŒì¼ ì—†ìŒ\")\n",
    "    else:\n",
    "        print(f\"âŒ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {project_dir}\")\n",
    "\n",
    "def reinstall_pytorch():\n",
    "    \"\"\"PyTorch ì¬ì„¤ì¹˜\"\"\"\n",
    "    print(\"ğŸ”„ PyTorch ì¬ì„¤ì¹˜ ì¤‘...\")\n",
    "    \n",
    "    # í˜„ì¬ PyTorch ë²„ì „ í™•ì¸\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"í˜„ì¬ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "        print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    except:\n",
    "        print(\"PyTorch ë²„ì „ í™•ì¸ ë¶ˆê°€\")\n",
    "    \n",
    "    # ì¬ì„¤ì¹˜ ëª…ë ¹ì–´ ì œê³µ\n",
    "    print(\"\\nğŸ› ï¸  PyTorch ì¬ì„¤ì¹˜ ëª…ë ¹ì–´:\")\n",
    "    print(\"ë‹¤ìŒ ì…€ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "    print(\"!pip uninstall torch torchvision torchaudio -y\")\n",
    "    print(\"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ğŸš¨ PyTorch ìˆœí™˜ Import ì˜¤ë¥˜ í•´ê²° ì‹œì‘\")\n",
    "clean_python_modules()\n",
    "fix_pytorch_import()\n",
    "check_circular_imports()\n",
    "reinstall_pytorch()\n",
    "\n",
    "print(\"\\nğŸ¯ ê¶Œì¥ í•´ê²° ìˆœì„œ:\")\n",
    "print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘ (ê°€ì¥ íš¨ê³¼ì )\")\n",
    "print(\"2. PyTorch ì¬ì„¤ì¹˜\")\n",
    "print(\"3. í”„ë¡œì íŠ¸ íŒŒì¼ import êµ¬ì¡° í™•ì¸\")\n",
    "print(\"4. ì•ˆì „í•œ import ìˆœì„œë¡œ ì¬ì‹¤í–‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32341928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ì¬ì„¤ì¹˜ (ìˆœí™˜ import ì˜¤ë¥˜ í•´ê²°)\n",
    "\n",
    "print(\"ğŸ”„ PyTorch ì™„ì „ ì¬ì„¤ì¹˜ ì‹œì‘...\")\n",
    "\n",
    "# 1. ê¸°ì¡´ PyTorch ì™„ì „ ì œê±°\n",
    "print(\"1ï¸âƒ£ ê¸°ì¡´ PyTorch ì œê±° ì¤‘...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# 2. ì‹œìŠ¤í…œ ìºì‹œ ì •ë¦¬\n",
    "print(\"\\n2ï¸âƒ£ ì‹œìŠ¤í…œ ìºì‹œ ì •ë¦¬ ì¤‘...\")\n",
    "!pip cache purge\n",
    "\n",
    "# 3. PyTorch ì¬ì„¤ì¹˜ (CUDA 11.8 ë²„ì „)\n",
    "print(\"\\n3ï¸âƒ£ PyTorch ì¬ì„¤ì¹˜ ì¤‘...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# 4. ì„¤ì¹˜ í™•ì¸\n",
    "print(\"\\n4ï¸âƒ£ ì„¤ì¹˜ í™•ì¸ ì¤‘...\")\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"âœ… PyTorch ë²„ì „: {torch.__version__}\")\n",
    "    print(f\"âœ… TorchVision ë²„ì „: {torchvision.__version__}\")\n",
    "    print(f\"âœ… CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # torch._dynamo í™•ì¸\n",
    "    try:\n",
    "        import torch._dynamo\n",
    "        if hasattr(torch._dynamo, 'config'):\n",
    "            print(\"âœ… torch._dynamo.config ì •ìƒ ì ‘ê·¼ ê°€ëŠ¥\")\n",
    "        else:\n",
    "            print(\"âš ï¸  torch._dynamo.config ì†ì„± ì—†ìŒ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  torch._dynamo ì ‘ê·¼ ë¬¸ì œ: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ PyTorch ì„¤ì¹˜ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘ (ê¶Œì¥)\")\n",
    "print(\"2. í”„ë¡œì íŠ¸ ë‹¤ì‹œ í´ë¡ \")\n",
    "print(\"3. í•™ìŠµ ì½”ë“œ ì¬ì‹¤í–‰\")\n",
    "print(\"4. ì—¬ì „íˆ ë¬¸ì œ ì‹œ ìˆœí™˜ import í™•ì¸ í•„ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•ˆì „í•œ Import ìˆœì„œë¡œ í•™ìŠµ ì‹¤í–‰ (ìˆœí™˜ import ë°©ì§€)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ›¡ï¸  ì•ˆì „í•œ Import ìˆœì„œë¡œ í•™ìŠµ ì‹¤í–‰\")\n",
    "\n",
    "# 1. í™˜ê²½ ì´ˆê¸°í™”\n",
    "print(\"1ï¸âƒ£ í™˜ê²½ ì´ˆê¸°í™” ì¤‘...\")\n",
    "os.environ['PYTHONDONTWRITEBYTECODE'] = '1'\n",
    "gc.collect()\n",
    "\n",
    "# 2. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "project_dir = '/content/sprint-ai03-team04/worktree/wooseok'\n",
    "os.chdir(project_dir)\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "print(f\"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "\n",
    "# 3. ì•ˆì „í•œ ìˆœì„œë¡œ ëª¨ë“ˆ Import\n",
    "print(\"\\n2ï¸âƒ£ ì•ˆì „í•œ ìˆœì„œë¡œ ëª¨ë“ˆ Import ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    # ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¨¼ì €\n",
    "    print(\"  ğŸ”¹ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import...\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤ ìˆœì„œëŒ€ë¡œ\n",
    "    print(\"  ğŸ”¹ config ëª¨ë“ˆ import...\")\n",
    "    from config import cfg\n",
    "    \n",
    "    print(\"  ğŸ”¹ dataset ëª¨ë“ˆ import...\")\n",
    "    from dataset import PillDetectionDataset, create_colab_dataset\n",
    "    \n",
    "    print(\"  ğŸ”¹ model ëª¨ë“ˆ import...\")\n",
    "    from models.yolo_model import create_model\n",
    "    \n",
    "    print(\"  ğŸ”¹ trainer ëª¨ë“ˆ import...\")\n",
    "    from trainer import Trainer\n",
    "    \n",
    "    print(\"  ğŸ”¹ train ëª¨ë“ˆ import...\")\n",
    "    from train import train_with_dataset\n",
    "    \n",
    "    print(\"âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ”„ ê¶Œì¥ ì¡°ì¹˜:\")\n",
    "    print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘\")\n",
    "    print(\"2. PyTorch ì¬ì„¤ì¹˜\")\n",
    "    print(\"3. í”„ë¡œì íŠ¸ íŒŒì¼ í™•ì¸\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. ì•ˆì „í•œ ì„¤ì •\n",
    "print(\"\\n3ï¸âƒ£ ì•ˆì „í•œ ì„¤ì • ì ìš© ì¤‘...\")\n",
    "try:\n",
    "    # ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ë³€ê²½\n",
    "    cfg.BATCH_SIZE = 2\n",
    "    cfg.NUM_WORKERS = 0\n",
    "    cfg.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"âœ… ë°°ì¹˜ í¬ê¸°: {cfg.BATCH_SIZE}\")\n",
    "    print(f\"âœ… Worker ìˆ˜: {cfg.NUM_WORKERS}\")\n",
    "    print(f\"âœ… ë””ë°”ì´ìŠ¤: {cfg.DEVICE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 5. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "print(\"\\n4ï¸âƒ£ ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •...\")\n",
    "DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "\n",
    "print(f\"ğŸ“Š JSON íŒŒì¼: {DATASET_JSON}\")\n",
    "print(f\"ğŸ–¼ï¸  ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {DATASET_IMAGES}\")\n",
    "\n",
    "# 6. í•™ìŠµ ì‹¤í–‰ (ì•ˆì „ ëª¨ë“œ)\n",
    "print(\"\\n5ï¸âƒ£ í•™ìŠµ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. ìœ„ì˜ ê²½ë¡œë¥¼ ì‹¤ì œ ë°ì´í„°ì…‹ ê²½ë¡œë¡œ ë³€ê²½\")\n",
    "print(\"2. ì•„ë˜ ì½”ë“œ ì£¼ì„ í•´ì œí•˜ì—¬ í•™ìŠµ ì‹¤í–‰\")\n",
    "\n",
    "# ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ë©´ í•™ìŠµ ì‹¤í–‰\n",
    "\"\"\"\n",
    "try:\n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {len(dataset)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        # í•™ìŠµ ì‹¤í–‰\n",
    "        print(\"ğŸš€ ì•ˆì „ ëª¨ë“œë¡œ í•™ìŠµ ì‹œì‘...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í•™ìŠµ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ë””ë²„ê¹… ë° ìˆ˜ì • ì½”ë“œ\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def fix_cuda_memory_error():\n",
    "    \"\"\"CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì¢…í•© ì†”ë£¨ì…˜\"\"\"\n",
    "    print(\"ğŸ”§ CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²° ì¤‘...\")\n",
    "    \n",
    "    # 1. GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"âœ… GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    \n",
    "    # 2. ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    print(\"âœ… CUDA ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”\")\n",
    "    \n",
    "    # 3. GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        cached_memory = torch.cuda.memory_reserved(device)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "        print(f\"ì´ ë©”ëª¨ë¦¬: {total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"ìºì‹œëœ ë©”ëª¨ë¦¬: {cached_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {(total_memory - allocated_memory) / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # 4. ê¶Œì¥ ì„¤ì • ì¶œë ¥\n",
    "    print(f\"\\nâš™ï¸  ê¶Œì¥ ì„¤ì •:\")\n",
    "    print(f\"ë°°ì¹˜ í¬ê¸°: 4 ì´í•˜ (í˜„ì¬ A100-40GB ê¸°ì¤€)\")\n",
    "    print(f\"Worker ìˆ˜: 0-2 (Colab í™˜ê²½)\")\n",
    "    print(f\"Mixed precision: ì‚¬ìš© ê¶Œì¥\")\n",
    "\n",
    "def create_safe_config():\n",
    "    \"\"\"ì•ˆì „í•œ í•™ìŠµ ì„¤ì • ìƒì„±\"\"\"\n",
    "    print(\"ğŸ›¡ï¸  ì•ˆì „í•œ í•™ìŠµ ì„¤ì • ìƒì„±...\")\n",
    "    \n",
    "    # config.py ìˆ˜ì •ì„ ìœ„í•œ ì•ˆì „í•œ ê°’ë“¤\n",
    "    safe_config = {\n",
    "        'BATCH_SIZE': 2,          # ë°°ì¹˜ í¬ê¸° ëŒ€í­ ê°ì†Œ\n",
    "        'NUM_WORKERS': 0,         # Worker ìˆ˜ 0ìœ¼ë¡œ ì„¤ì •\n",
    "        'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'MIXED_PRECISION': True,  # Mixed precision í™œì„±í™”\n",
    "        'GRADIENT_CLIP': 1.0,     # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ ì•ˆì „í•œ ì„¤ì •ê°’:\")\n",
    "    for key, value in safe_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return safe_config\n",
    "\n",
    "def test_simple_tensor_operations():\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°ìœ¼ë¡œ CUDA ìƒíƒœ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ§ª CUDA ìƒíƒœ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸\n",
    "            device = torch.device('cuda')\n",
    "            x = torch.randn(10, 10).to(device)\n",
    "            y = torch.randn(10, 10).to(device)\n",
    "            z = torch.mm(x, y)\n",
    "            print(\"âœ… ê¸°ë³¸ CUDA í…ì„œ ì—°ì‚° ì •ìƒ\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ í•´ì œ\n",
    "            del x, y, z\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ CUDA ì‚¬ìš© ë¶ˆê°€\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CUDA í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ğŸš¨ CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²° ì‹œì‘\")\n",
    "fix_cuda_memory_error()\n",
    "safe_config = create_safe_config()\n",
    "cuda_test_passed = test_simple_tensor_operations()\n",
    "\n",
    "if cuda_test_passed:\n",
    "    print(\"\\nâœ… CUDA ìƒíƒœ ì •ìƒ - ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ì¬ì‹œë„ ê°€ëŠ¥\")\n",
    "else:\n",
    "    print(\"\\nâŒ CUDA ìƒíƒœ ë¶ˆì•ˆì • - ëŸ°íƒ€ì„ ì¬ì‹œì‘ ê¶Œì¥\")\n",
    "    \n",
    "print(\"\\nğŸ”„ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. ëŸ°íƒ€ì„ ì¬ì‹œì‘ (í•„ìš”ì‹œ)\")\n",
    "print(\"2. ë°°ì¹˜ í¬ê¸°ë¥¼ 2ë¡œ ì„¤ì •\")\n",
    "print(\"3. NUM_WORKERSë¥¼ 0ìœ¼ë¡œ ì„¤ì •\")\n",
    "print(\"4. í•™ìŠµ ì¬ì‹œë„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84783eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA ì˜¤ë¥˜ í•´ê²° í›„ ì•ˆì „í•œ í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # ë””ë²„ê¹… ëª¨ë“œ\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# 2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# 3. ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ config ì„ì‹œ ìˆ˜ì •\n",
    "try:\n",
    "    from config import cfg\n",
    "    \n",
    "    # ì›ë³¸ ì„¤ì • ë°±ì—…\n",
    "    original_batch_size = cfg.BATCH_SIZE\n",
    "    original_num_workers = cfg.NUM_WORKERS\n",
    "    \n",
    "    # ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ì„ì‹œ ë³€ê²½\n",
    "    cfg.BATCH_SIZE = 2          # ë°°ì¹˜ í¬ê¸° ëŒ€í­ ê°ì†Œ\n",
    "    cfg.NUM_WORKERS = 0         # Worker ìˆ˜ 0ìœ¼ë¡œ ì„¤ì •\n",
    "    cfg.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(\"âš™ï¸  ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ë³€ê²½:\")\n",
    "    print(f\"ë°°ì¹˜ í¬ê¸°: {original_batch_size} â†’ {cfg.BATCH_SIZE}\")\n",
    "    print(f\"Worker ìˆ˜: {original_num_workers} â†’ {cfg.NUM_WORKERS}\")\n",
    "    print(f\"ë””ë°”ì´ìŠ¤: {cfg.DEVICE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 4. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½)\n",
    "DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "\n",
    "# 5. ì•ˆì „í•œ í•™ìŠµ ì‹¤í–‰\n",
    "try:\n",
    "    print(\"ğŸš€ ì•ˆì „í•œ í•™ìŠµ ì‹œì‘...\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    from dataset import create_colab_dataset\n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {len(dataset)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        # ì•ˆì „í•œ í•™ìŠµ ì‹¤í–‰\n",
    "        from train import train_with_dataset\n",
    "        \n",
    "        # ì¶”ê°€ ì•ˆì „ ì¥ì¹˜\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        print(\"ğŸ›¡ï¸  ì•ˆì „ ëª¨ë“œë¡œ í•™ìŠµ ì‹œì‘...\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n",
    "        print(\"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "        print(f\"JSON: {DATASET_JSON}\")\n",
    "        print(f\"Images: {DATASET_IMAGES}\")\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e):\n",
    "        print(f\"âŒ CUDA ì˜¤ë¥˜ ì¬ë°œìƒ: {e}\")\n",
    "        print(\"\\nğŸ”„ ê¶Œì¥ ì¡°ì¹˜:\")\n",
    "        print(\"1. ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ì¬ì‹œì‘\")\n",
    "        print(\"2. ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ë” ì¤„ì´ê¸°\")\n",
    "        print(\"3. CPU ëª¨ë“œë¡œ ì „í™˜ (cfg.DEVICE = 'cpu')\")\n",
    "    else:\n",
    "        print(f\"âŒ ê¸°íƒ€ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"ğŸ§¹ í•™ìŠµ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd625c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš¨ ê¸´ê¸‰ ëŒ€ì•ˆ: CPU ì „ìš© í•™ìŠµ (CUDA ì˜¤ë¥˜ ì§€ì† ì‹œ)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ”„ CPU ì „ìš© ëª¨ë“œë¡œ ì „í™˜...\")\n",
    "\n",
    "# 1. CUDA ì™„ì „ ë¹„í™œì„±í™”\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "torch.cuda.is_available = lambda: False\n",
    "\n",
    "# 2. í”„ë¡œì íŠ¸ ì„¤ì •\n",
    "os.chdir('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "# 3. CPU ì „ìš© ì„¤ì •\n",
    "try:\n",
    "    from config import cfg\n",
    "    \n",
    "    cfg.DEVICE = 'cpu'\n",
    "    cfg.BATCH_SIZE = 4          # CPUì—ì„œëŠ” ë°°ì¹˜ í¬ê¸° ì¡°ê¸ˆ ëŠ˜ë ¤ë„ ë¨\n",
    "    cfg.NUM_WORKERS = 2         # CPUì—ì„œëŠ” worker ì‚¬ìš© ê°€ëŠ¥\n",
    "    cfg.EPOCHS = 2              # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì—í¬í¬ ìˆ˜ ì¤„ì´ê¸°\n",
    "    \n",
    "    print(\"ğŸ’» CPU ì „ìš© ì„¤ì •:\")\n",
    "    print(f\"ë””ë°”ì´ìŠ¤: {cfg.DEVICE}\")\n",
    "    print(f\"ë°°ì¹˜ í¬ê¸°: {cfg.BATCH_SIZE}\")\n",
    "    print(f\"Worker ìˆ˜: {cfg.NUM_WORKERS}\")\n",
    "    print(f\"ì—í¬í¬: {cfg.EPOCHS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 4. CPU ì „ìš© í•™ìŠµ ì‹¤í–‰\n",
    "print(\"\\nâš ï¸  ì£¼ì˜: CPU ëª¨ë“œëŠ” ë§¤ìš° ëŠë¦½ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ ëª©ì )\")\n",
    "print(\"ì‹¤ì œ í•™ìŠµì€ CUDA ì˜¤ë¥˜ í•´ê²° í›„ GPUì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "\n",
    "# ì‚¬ìš©ì í™•ì¸\n",
    "print(\"\\nğŸ¤” CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\")\n",
    "print(\"ê³„ì†í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "\n",
    "# ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ë©´ CPU ëª¨ë“œë¡œ ì‹¤í–‰\n",
    "\"\"\"\n",
    "try:\n",
    "    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "    DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"\n",
    "    DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"\n",
    "    \n",
    "    from dataset import create_colab_dataset\n",
    "    dataset = create_colab_dataset(\n",
    "        json_path=DATASET_JSON,\n",
    "        img_dir=DATASET_IMAGES\n",
    "    )\n",
    "    \n",
    "    if dataset is not None:\n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {len(dataset)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        from train import train_with_dataset\n",
    "        print(\"ğŸ’» CPU ëª¨ë“œë¡œ í•™ìŠµ ì‹œì‘... (ë§¤ìš° ëŠë¦¼)\")\n",
    "        train_with_dataset(dataset)\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ CPU í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ¯ ê¶Œì¥ í•´ê²°ì±…:\")\n",
    "print(\"1. ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ì¬ì‹œì‘\")\n",
    "print(\"2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„\")\n",
    "print(\"3. ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ì„¤ì •\")\n",
    "print(\"4. ì´ì „ ì…€ì˜ 'ì•ˆì „í•œ í•™ìŠµ ì‹¤í–‰' ì½”ë“œ ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ada6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ìš©í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def check_system_resources():\n",
    "    \"\"\"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸\"\"\"\n",
    "    print(\"=== ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í˜„í™© ===\")\n",
    "    \n",
    "    # CPU ë° ë©”ëª¨ë¦¬\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    \n",
    "    print(f\"ğŸ–¥ï¸  CPU ì‚¬ìš©ëŸ‰: {cpu_percent}%\")\n",
    "    print(f\"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory.percent}%\")\n",
    "    print(f\"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {memory.available / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # GPU ì •ë³´\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        gpu_allocated = torch.cuda.memory_allocated(0)\n",
    "        gpu_cached = torch.cuda.memory_reserved(0)\n",
    "        \n",
    "        print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ğŸ“ˆ GPU ë©”ëª¨ë¦¬ ì´ëŸ‰: {gpu_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"ğŸ”¢ GPU í• ë‹¹ëœ ë©”ëª¨ë¦¬: {gpu_allocated / (1024**3):.2f} GB\")\n",
    "        print(f\"ğŸ’½ GPU ìºì‹œëœ ë©”ëª¨ë¦¬: {gpu_cached / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "def save_model_to_drive(model_path, drive_path):\n",
    "    \"\"\"ëª¨ë¸ì„ Google Driveì— ì €ì¥\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(drive_path):\n",
    "            os.makedirs(drive_path)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_path = os.path.join(drive_path, f\"model_{timestamp}.pth\")\n",
    "        \n",
    "        shutil.copy2(model_path, save_path)\n",
    "        print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_gpu_cache():\n",
    "    \"\"\"GPU ìºì‹œ ì •ë¦¬\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "def quick_dataset_check(json_path, img_dir):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ë¹ ë¥¸ í™•ì¸\"\"\"\n",
    "    print(\"=== ë°ì´í„°ì…‹ ë¹ ë¥¸ í™•ì¸ ===\")\n",
    "    \n",
    "    # JSON íŒŒì¼ í™•ì¸\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"âœ… JSON íŒŒì¼ ì¡´ì¬: {json_path}\")\n",
    "        \n",
    "        import json\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"ğŸ“Š ì´ë¯¸ì§€ ê°œìˆ˜: {len(data['images'])}\")\n",
    "            print(f\"ğŸ·ï¸  ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜: {len(data['annotations'])}\")\n",
    "            print(f\"ğŸ¯ ì¹´í…Œê³ ë¦¬ ê°œìˆ˜: {len(data['categories'])}\")\n",
    "    else:\n",
    "        print(f\"âŒ JSON íŒŒì¼ ì—†ìŒ: {json_path}\")\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "    if os.path.exists(img_dir):\n",
    "        img_count = len([f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        print(f\"âœ… ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì¡´ì¬: {img_dir}\")\n",
    "        print(f\"ğŸ–¼ï¸  ì´ë¯¸ì§€ íŒŒì¼ ê°œìˆ˜: {img_count}\")\n",
    "    else:\n",
    "        print(f\"âŒ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì—†ìŒ: {img_dir}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "print(\"ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤:\")\n",
    "print(\"- check_system_resources(): ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸\")\n",
    "print(\"- save_model_to_drive(model_path, drive_path): ëª¨ë¸ Drive ì €ì¥\")\n",
    "print(\"- clean_gpu_cache(): GPU ìºì‹œ ì •ë¦¬\")\n",
    "print(\"- quick_dataset_check(json_path, img_dir): ë°ì´í„°ì…‹ ë¹ ë¥¸ í™•ì¸\")\n",
    "\n",
    "# í˜„ì¬ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2525c4",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° í‰ê°€\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œëœ í›„ ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "### ğŸ“Š í•™ìŠµ ê²°ê³¼ í™•ì¸\n",
    "1. **í•™ìŠµ ê³¡ì„  ë¶„ì„**:\n",
    "   - ì†ì‹¤ í•¨ìˆ˜ ë³€í™” ì¶”ì´\n",
    "   - ê²€ì¦ ì„±ëŠ¥ ë³€í™”\n",
    "   - Overfitting ì—¬ë¶€ í™•ì¸\n",
    "\n",
    "2. **ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê·¸ í™•ì¸**:\n",
    "   - ì €ì¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸\n",
    "   - í•™ìŠµ ë¡œê·¸ ë¶„ì„\n",
    "   - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "\n",
    "3. **ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°**:\n",
    "   - mAP (mean Average Precision)\n",
    "   - í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "   - Confusion Matrix\n",
    "\n",
    "### ğŸ¯ ëª¨ë¸ í‰ê°€ ë„êµ¬\n",
    "- **ì •ëŸ‰ì  í‰ê°€**: mAP, Precision, Recall\n",
    "- **ì •ì„±ì  í‰ê°€**: ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "- **ì˜¤ë¥˜ ë¶„ì„**: ì˜ëª» ì˜ˆì¸¡ëœ ì¼€ì´ìŠ¤ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê²°ê³¼ ë° ì²´í¬í¬ì¸íŠ¸ í™•ì¸\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def check_training_results():\n",
    "    \"\"\"í•™ìŠµ ê²°ê³¼ í™•ì¸\"\"\"\n",
    "    print(\"=== í•™ìŠµ ê²°ê³¼ í™•ì¸ ===\")\n",
    "    \n",
    "    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "    current_dir = '/content/sprint-ai03-team04/worktree/wooseok'\n",
    "    os.chdir(current_dir)\n",
    "    print(f\"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "    \n",
    "    # outputs ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "    outputs_dir = 'outputs'\n",
    "    if os.path.exists(outputs_dir):\n",
    "        print(f\"\\nğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬ ({outputs_dir}) ë‚´ìš©:\")\n",
    "        files = os.listdir(outputs_dir)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(outputs_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path)\n",
    "                modified = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                print(f\"  ğŸ“„ {file} ({size} bytes, {modified.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "    else:\n",
    "        print(f\"âŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {outputs_dir}\")\n",
    "    \n",
    "    # í•™ìŠµ ë¡œê·¸ í™•ì¸\n",
    "    log_file = os.path.join(outputs_dir, 'training.log')\n",
    "    if os.path.exists(log_file):\n",
    "        print(f\"\\nğŸ“œ í•™ìŠµ ë¡œê·¸ ë§ˆì§€ë§‰ 10ì¤„:\")\n",
    "        with open(log_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[-10:]:\n",
    "                print(f\"  {line.strip()}\")\n",
    "    else:\n",
    "        print(\"âŒ í•™ìŠµ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "    \n",
    "    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸\n",
    "    checkpoint_files = [f for f in os.listdir(outputs_dir) if f.endswith('.pth')] if os.path.exists(outputs_dir) else []\n",
    "    if checkpoint_files:\n",
    "        print(f\"\\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤:\")\n",
    "        for checkpoint in checkpoint_files:\n",
    "            file_path = os.path.join(outputs_dir, checkpoint)\n",
    "            size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            print(f\"  ğŸ“¦ {checkpoint} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "    \n",
    "    return outputs_dir, checkpoint_files\n",
    "\n",
    "# ì‹¤í–‰\n",
    "outputs_dir, checkpoint_files = check_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def plot_training_curves():\n",
    "    \"\"\"í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ í‘œì‹œ\"\"\"\n",
    "    print(\"ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì‹œê°í™”\")\n",
    "    \n",
    "    # í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸\n",
    "    curve_image_path = os.path.join(outputs_dir, 'learning_curves.png')\n",
    "    \n",
    "    if os.path.exists(curve_image_path):\n",
    "        print(f\"âœ… í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ ë°œê²¬: {curve_image_path}\")\n",
    "        \n",
    "        # ì´ë¯¸ì§€ í‘œì‹œ\n",
    "        img = Image.open(curve_image_path)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title('Training and Validation Curves', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        print(\"í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê·¸ë˜í”„ ìƒì„±ì— ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "\n",
    "def load_checkpoint_history(checkpoint_path):\n",
    "    \"\"\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì´ë ¥ ë¡œë“œ\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        if 'history' in checkpoint:\n",
    "            history = checkpoint['history']\n",
    "            print(f\"âœ… í•™ìŠµ ì´ë ¥ ë¡œë“œ ì„±ê³µ: {checkpoint_path}\")\n",
    "            return history\n",
    "        else:\n",
    "            print(f\"âŒ ì²´í¬í¬ì¸íŠ¸ì— í•™ìŠµ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_history_from_checkpoint():\n",
    "    \"\"\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì´ë ¥ì„ ì½ì–´ì„œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\"\"\"\n",
    "    if not checkpoint_files:\n",
    "        print(\"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” best_model.pth ìš°ì„  ì„ íƒ\n",
    "    best_checkpoint = None\n",
    "    for checkpoint in checkpoint_files:\n",
    "        if 'best_model.pth' in checkpoint:\n",
    "            best_checkpoint = checkpoint\n",
    "            break\n",
    "    \n",
    "    if not best_checkpoint:\n",
    "        best_checkpoint = checkpoint_files[-1]  # ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸\n",
    "    \n",
    "    checkpoint_path = os.path.join(outputs_dir, best_checkpoint)\n",
    "    print(f\"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì´ë ¥ ì¶”ì¶œ: {best_checkpoint}\")\n",
    "    \n",
    "    history = load_checkpoint_history(checkpoint_path)\n",
    "    \n",
    "    if history:\n",
    "        # í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # ì†ì‹¤ ê·¸ë˜í”„\n",
    "        if 'train_loss' in history and 'val_loss' in history:\n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "            ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training and Validation Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # mAP ê·¸ë˜í”„\n",
    "        if 'val_map' in history:\n",
    "            epochs = range(1, len(history['val_map']) + 1)\n",
    "            ax2.plot(epochs, history['val_map'], 'g-', label='Validation mAP', linewidth=2)\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('mAP')\n",
    "            ax2.set_title('Validation mAP')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ìˆ˜ì¹˜ ì •ë³´ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "        if 'train_loss' in history:\n",
    "            print(f\"  ğŸ”¹ ìµœì¢… Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        if 'val_loss' in history:\n",
    "            print(f\"  ğŸ”¹ ìµœì¢… Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        if 'val_map' in history:\n",
    "            print(f\"  ğŸ”¹ ìµœì¢… Validation mAP: {history['val_map'][-1]:.4f}\")\n",
    "            if len(history['val_map']) > 1:\n",
    "                best_map = max(history['val_map'])\n",
    "                best_epoch = history['val_map'].index(best_map) + 1\n",
    "                print(f\"  ğŸ† ìµœê³  mAP: {best_map:.4f} (Epoch {best_epoch})\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"=\" * 50)\n",
    "plot_training_curves()\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "plot_history_from_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mAP ì¸¡ì • ë° ëª¨ë¸ í‰ê°€\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
    "sys.path.append('/content/sprint-ai03-team04/worktree/wooseok')\n",
    "\n",
    "def evaluate_model_map(model, val_loader, device):\n",
    "    \"\"\"ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ mAP ì¸¡ì •\"\"\"\n",
    "    print(\"ğŸ¯ ëª¨ë¸ mAP í‰ê°€ ì‹œì‘...\")\n",
    "    \n",
    "    model.eval()\n",
    "    pred_boxes, pred_labels, pred_scores = [], [], []\n",
    "    gt_boxes, gt_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc='mAP ê³„ì‚° ì¤‘'):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if outputs is not None:\n",
    "                # ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘\n",
    "                for out, target in zip(outputs, targets):\n",
    "                    if isinstance(out, dict) and 'boxes' in out:\n",
    "                        pred_boxes.append(out['boxes'].cpu())\n",
    "                        pred_labels.append(out['labels'].cpu())\n",
    "                        pred_scores.append(out['scores'].cpu())\n",
    "                        gt_boxes.append(target['boxes'].cpu())\n",
    "                        gt_labels.append(target['labels'].cpu())\n",
    "    \n",
    "    # mAP ê³„ì‚°\n",
    "    if pred_boxes and gt_boxes:\n",
    "        from utils.metrics import calculate_mAP\n",
    "        map_score = calculate_mAP(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels)\n",
    "        print(f\"âœ… mAP ê³„ì‚° ì™„ë£Œ: {map_score:.4f}\")\n",
    "        return map_score\n",
    "    else:\n",
    "        print(\"âŒ ìœ íš¨í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return 0.0\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    try:\n",
    "        from config import cfg\n",
    "        from models.yolo_model import create_model\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        model = create_model(cfg)\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
    "        best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "        \n",
    "        if os.path.exists(best_model_path):\n",
    "            print(f\"ğŸ“¦ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ: {best_model_path}\")\n",
    "            checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥\n",
    "            if 'epoch' in checkpoint:\n",
    "                print(f\"  ğŸ”¹ ì—í¬í¬: {checkpoint['epoch']}\")\n",
    "            if 'val_loss' in checkpoint:\n",
    "                print(f\"  ğŸ”¹ ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.4f}\")\n",
    "            \n",
    "            return model\n",
    "        else:\n",
    "            print(\"âŒ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_validation_loader():\n",
    "    \"\"\"ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„±\"\"\"\n",
    "    try:\n",
    "        from config import cfg\n",
    "        from dataset import PillDetectionDataset\n",
    "        from torch.utils.data import DataLoader, random_split\n",
    "        \n",
    "        def collate_fn(batch):\n",
    "            return tuple(zip(*batch))\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)\n",
    "        # Google Driveë‚˜ ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ì„¤ì •í•˜ì„¸ìš”\n",
    "        DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "        DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"  # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½\n",
    "        \n",
    "        print(f\"ğŸ“Š ë°ì´í„°ì…‹ ê²½ë¡œ:\")\n",
    "        print(f\"  JSON: {DATASET_JSON}\")\n",
    "        print(f\"  Images: {DATASET_IMAGES}\")\n",
    "        \n",
    "        if not os.path.exists(DATASET_JSON) or not os.path.exists(DATASET_IMAGES):\n",
    "            print(\"âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”\")\n",
    "            return None\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        dataset = PillDetectionDataset(\n",
    "            json_file=DATASET_JSON,\n",
    "            img_dir=DATASET_IMAGES\n",
    "        )\n",
    "        \n",
    "        # í•™ìŠµ/ê²€ì¦ ë¶„í•  (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)\n",
    "        val_size = int(len(dataset) * cfg.VAL_RATIO)\n",
    "        train_size = len(dataset) - val_size\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)  # ë™ì¼í•œ ì‹œë“œ ì‚¬ìš©\n",
    "        )\n",
    "        \n",
    "        # ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„±\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Colabì—ì„œëŠ” 0ìœ¼ë¡œ ì„¤ì •\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ: {len(val_dataset)} ìƒ˜í”Œ\")\n",
    "        return val_loader\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_model_evaluation():\n",
    "    \"\"\"ì „ì²´ ëª¨ë¸ í‰ê°€ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸ”¬ ëª¨ë¸ í‰ê°€ ì‹œì‘\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = load_best_model()\n",
    "    if model is None:\n",
    "        print(\"âŒ ëª¨ë¸ í‰ê°€ ì¤‘ë‹¨: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\")\n",
    "        return\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° ë¡œë” ìƒì„±\n",
    "    val_loader = create_validation_loader()\n",
    "    if val_loader is None:\n",
    "        print(\"âŒ ëª¨ë¸ í‰ê°€ ì¤‘ë‹¨: ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨\")\n",
    "        return\n",
    "    \n",
    "    # mAP í‰ê°€ ì‹¤í–‰\n",
    "    map_score = evaluate_model_map(model, val_loader, device)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼\")\n",
    "    print(f\"ğŸ“Š mAP (mean Average Precision): {map_score:.4f}\")\n",
    "    \n",
    "    # ì„±ëŠ¥ í•´ì„\n",
    "    if map_score > 0.5:\n",
    "        print(\"ğŸ‰ í›Œë¥­í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤!\")\n",
    "    elif map_score > 0.3:\n",
    "        print(\"ğŸ‘ ì–‘í˜¸í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤.\")\n",
    "    elif map_score > 0.1:\n",
    "        print(\"ğŸ“ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"ğŸ”§ ëª¨ë¸ êµ¬ì¡°ë‚˜ í•™ìŠµ ë°©ë²•ì„ ì¬ê²€í† í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    return map_score\n",
    "\n",
    "# ì‚¬ìš©ë²• ì•ˆë‚´\n",
    "print(\"ğŸ¯ mAP í‰ê°€ ë„êµ¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"\\nğŸ“‹ í‰ê°€ ì‹¤í–‰ ë°©ë²•:\")\n",
    "print(\"1. ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •\")\n",
    "print(\"2. run_model_evaluation() í•¨ìˆ˜ ì‹¤í–‰\")\n",
    "print(\"\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n",
    "print(\"- í•™ìŠµì´ ì™„ë£Œëœ í›„ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "print(\"- ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”\")\n",
    "print(\"- ìµœê³  ì„±ëŠ¥ ëª¨ë¸(best_model.pth)ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1347a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ì‹¤ì œ mAP í‰ê°€ ì‹¤í–‰\n",
    "\n",
    "# âš ï¸ ì¤‘ìš”: ì‹¤í–‰ ì „ì— ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
    "\n",
    "# 1. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)\n",
    "print(\"ğŸ“Š ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\")\n",
    "print(\"âš ï¸  ì•„ë˜ ê²½ë¡œë¥¼ ì‹¤ì œ ë°ì´í„°ì…‹ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”:\")\n",
    "\n",
    "# Google Driveì— ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ìš°\n",
    "DATASET_JSON = \"/content/drive/MyDrive/your_dataset/train.json\"\n",
    "DATASET_IMAGES = \"/content/drive/MyDrive/your_dataset/images/train\"\n",
    "\n",
    "# ë˜ëŠ” ì§ì ‘ ì—…ë¡œë“œí•œ ê²½ìš°\n",
    "# DATASET_JSON = \"/content/dataset/train.json\"\n",
    "# DATASET_IMAGES = \"/content/dataset/images/train\"\n",
    "\n",
    "print(f\"JSON íŒŒì¼: {DATASET_JSON}\")\n",
    "print(f\"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {DATASET_IMAGES}\")\n",
    "\n",
    "# 2. ê²½ë¡œ í™•ì¸\n",
    "json_exists = os.path.exists(DATASET_JSON)\n",
    "img_exists = os.path.exists(DATASET_IMAGES)\n",
    "\n",
    "print(f\"\\nâœ… ê²½ë¡œ í™•ì¸:\")\n",
    "print(f\"JSON íŒŒì¼ ì¡´ì¬: {json_exists}\")\n",
    "print(f\"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì¡´ì¬: {img_exists}\")\n",
    "\n",
    "if json_exists and img_exists:\n",
    "    print(\"âœ… ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸ ì™„ë£Œ!\")\n",
    "    print(\"\\nğŸš€ mAP í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # config.pyì—ì„œ ë°ì´í„°ì…‹ ê²½ë¡œ ì„ì‹œ ìˆ˜ì •\n",
    "    try:\n",
    "        from config import cfg\n",
    "        # ê²½ë¡œ ì„ì‹œ ë³€ê²½\n",
    "        original_json = cfg.TRAIN_JSON\n",
    "        original_img = cfg.TRAIN_IMG_DIR\n",
    "        \n",
    "        cfg.TRAIN_JSON = DATASET_JSON\n",
    "        cfg.TRAIN_IMG_DIR = DATASET_IMAGES\n",
    "        \n",
    "        print(f\"ğŸ“ ì„¤ì • ì„ì‹œ ë³€ê²½:\")\n",
    "        print(f\"  JSON: {original_json} â†’ {cfg.TRAIN_JSON}\")\n",
    "        print(f\"  Images: {original_img} â†’ {cfg.TRAIN_IMG_DIR}\")\n",
    "        \n",
    "        # mAP í‰ê°€ ì‹¤í–‰\n",
    "        final_map = run_model_evaluation()\n",
    "        \n",
    "        # ì„¤ì • ë³µì›\n",
    "        cfg.TRAIN_JSON = original_json\n",
    "        cfg.TRAIN_IMG_DIR = original_img\n",
    "        \n",
    "        print(f\"\\nğŸŠ í‰ê°€ ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ† ìµœì¢… mAP: {final_map:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
    "    print(\"\\nğŸ“‹ í•´ê²° ë°©ë²•:\")\n",
    "    print(\"1. Google Driveë¥¼ ë§ˆìš´íŠ¸í–ˆëŠ”ì§€ í™•ì¸\")\n",
    "    print(\"2. ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\")\n",
    "    print(\"3. ìœ„ì˜ DATASET_JSON, DATASET_IMAGES ë³€ìˆ˜ë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •\")\n",
    "    print(\"4. ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë° ê°œì„  ì œì•ˆ\n",
    "\n",
    "def analyze_training_performance():\n",
    "    \"\"\"í•™ìŠµ ì„±ëŠ¥ ì¢…í•© ë¶„ì„\"\"\"\n",
    "    print(\"ğŸ” í•™ìŠµ ì„±ëŠ¥ ì¢…í•© ë¶„ì„\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ\n",
    "    best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        \n",
    "        print(\"ğŸ“ˆ í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"  ğŸ”¹ ìµœê³  ì„±ëŠ¥ ì—í¬í¬: {checkpoint['epoch']}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"  ğŸ”¹ ìµœê³  ì„±ëŠ¥ ì‹œ ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.6f}\")\n",
    "        \n",
    "        if 'history' in checkpoint:\n",
    "            history = checkpoint['history']\n",
    "            \n",
    "            # ì†ì‹¤ ë¶„ì„\n",
    "            if 'train_loss' in history and 'val_loss' in history:\n",
    "                train_losses = history['train_loss']\n",
    "                val_losses = history['val_loss']\n",
    "                \n",
    "                print(f\"\\nğŸ“‰ ì†ì‹¤ í•¨ìˆ˜ ë¶„ì„:\")\n",
    "                print(f\"  ğŸ”¹ ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.6f}\")\n",
    "                print(f\"  ğŸ”¹ ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.6f}\")\n",
    "                print(f\"  ğŸ”¹ ì†ì‹¤ ë¹„ìœ¨ (val/train): {val_losses[-1]/train_losses[-1]:.2f}\")\n",
    "                \n",
    "                # Overfitting ë¶„ì„\n",
    "                if len(train_losses) > 3:\n",
    "                    recent_train = np.mean(train_losses[-3:])\n",
    "                    recent_val = np.mean(val_losses[-3:])\n",
    "                    \n",
    "                    if recent_val > recent_train * 1.5:\n",
    "                        print(\"  âš ï¸  ê³¼ì í•©(Overfitting) ê°€ëŠ¥ì„± ë†’ìŒ\")\n",
    "                    elif recent_val > recent_train * 1.2:\n",
    "                        print(\"  ğŸ“‹ ì•½ê°„ì˜ ê³¼ì í•© ê²½í–¥\")\n",
    "                    else:\n",
    "                        print(\"  âœ… ì ì ˆí•œ ì¼ë°˜í™” ì„±ëŠ¥\")\n",
    "            \n",
    "            # mAP ë¶„ì„\n",
    "            if 'val_map' in history:\n",
    "                map_scores = history['val_map']\n",
    "                print(f\"\\nğŸ¯ mAP ì„±ëŠ¥ ë¶„ì„:\")\n",
    "                print(f\"  ğŸ”¹ ìµœì¢… mAP: {map_scores[-1]:.4f}\")\n",
    "                \n",
    "                if len(map_scores) > 1:\n",
    "                    best_map = max(map_scores)\n",
    "                    best_map_epoch = map_scores.index(best_map)\n",
    "                    print(f\"  ğŸ† ìµœê³  mAP: {best_map:.4f} (ì—í¬í¬ {best_map_epoch})\")\n",
    "                    \n",
    "                    # ì„±ëŠ¥ ê°œì„  ì¶”ì´\n",
    "                    if len(map_scores) > 3:\n",
    "                        early_avg = np.mean(map_scores[:3])\n",
    "                        late_avg = np.mean(map_scores[-3:])\n",
    "                        improvement = late_avg - early_avg\n",
    "                        \n",
    "                        if improvement > 0.05:\n",
    "                            print(\"  ğŸ“ˆ ì„±ëŠ¥ í¬ê²Œ ê°œì„ ë¨\")\n",
    "                        elif improvement > 0.01:\n",
    "                            print(\"  ğŸ“Š ì„±ëŠ¥ ì•½ê°„ ê°œì„ ë¨\")\n",
    "                        else:\n",
    "                            print(\"  ğŸ“‰ ì„±ëŠ¥ ê°œì„  ì œí•œì \")\n",
    "                            \n",
    "    else:\n",
    "        print(\"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "def suggest_improvements():\n",
    "    \"\"\"ì„±ëŠ¥ ê°œì„  ì œì•ˆ\"\"\"\n",
    "    print(\"\\nğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    suggestions = [\n",
    "        {\n",
    "            \"category\": \"ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°\",\n",
    "            \"items\": [\n",
    "                \"ë” ê¹Šì€ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© (ResNet50, EfficientNet ë“±)\",\n",
    "                \"Multi-scale íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•œ FPN (Feature Pyramid Network) ì¶”ê°€\",\n",
    "                \"Attention ë©”ì»¤ë‹ˆì¦˜ ë„ì…\",\n",
    "                \"ë” ë§ì€ ì•µì»¤ ë°•ìŠ¤ í¬ê¸° ë° ë¹„ìœ¨ ì‹¤í—˜\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ğŸ“Š ë°ì´í„° ë° ì „ì²˜ë¦¬\",\n",
    "            \"items\": [\n",
    "                \"ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš© (íšŒì „, í¬ê¸° ë³€ê²½, ìƒ‰ìƒ ì¡°ì • ë“±)\",\n",
    "                \"ë” ë§ì€ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘\",\n",
    "                \"í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë‹ (Hard Negative Mining)\",\n",
    "                \"í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜ ì¡°ì •, ì˜¤ë²„ìƒ˜í”Œë§ ë“±)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ğŸ›ï¸ í•™ìŠµ ì„¤ì •\",\n",
    "            \"items\": [\n",
    "                \"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (CosineAnnealingLR, ReduceLROnPlateau ë“±)\",\n",
    "                \"ë” ê¸´ í•™ìŠµ ì‹œê°„ (ë” ë§ì€ ì—í¬í¬)\",\n",
    "                \"ë°°ì¹˜ í¬ê¸° ì¡°ì • ì‹¤í—˜\",\n",
    "                \"ì˜µí‹°ë§ˆì´ì € ë³€ê²½ (Adam, AdamW ë“±)\",\n",
    "                \"ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°’ ì¡°ì •\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ğŸ”§ ì†ì‹¤ í•¨ìˆ˜\",\n",
    "            \"items\": [\n",
    "                \"Focal Loss ë„ì… (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)\",\n",
    "                \"IoU Loss, GIoU Loss ë“± ë°•ìŠ¤ íšŒê·€ ì†ì‹¤ ê°œì„ \",\n",
    "                \"Multi-task ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¡°ì •\",\n",
    "                \"Label Smoothing ì ìš©\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ğŸ“ˆ í›„ì²˜ë¦¬\",\n",
    "            \"items\": [\n",
    "                \"NMS (Non-Maximum Suppression) ì„ê³„ê°’ íŠœë‹\",\n",
    "                \"ì‹ ë¢°ë„ ì„ê³„ê°’ ìµœì í™”\",\n",
    "                \"Test Time Augmentation (TTA) ì ìš©\",\n",
    "                \"ì•™ìƒë¸” ë°©ë²• ë„ì…\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for suggestion in suggestions:\n",
    "        print(f\"\\n{suggestion['category']}\")\n",
    "        for i, item in enumerate(suggestion['items'], 1):\n",
    "            print(f\"  {i}. {item}\")\n",
    "\n",
    "def performance_checklist():\n",
    "    \"\"\"ì„±ëŠ¥ ì²´í¬ë¦¬ìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\nâœ… ì„±ëŠ¥ ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    checklist = [\n",
    "        \"[ ] í•™ìŠµ ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± í™•ì¸\",\n",
    "        \"[ ] ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°• ê¸°ë²• ì ìš©\",\n",
    "        \"[ ] ì ì ˆí•œ ëª¨ë¸ ë³µì¡ë„ ì„ íƒ (ê³¼ì í•© vs ê³¼ì†Œì í•©)\",\n",
    "        \"[ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ë“±)\",\n",
    "        \"[ ] ì†ì‹¤ í•¨ìˆ˜ ê°œì„  (Focal Loss, IoU Loss ë“±)\",\n",
    "        \"[ ] ì •ê·œí™” ê¸°ë²• ì ìš© (Dropout, BatchNorm ë“±)\",\n",
    "        \"[ ] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì ìš©\",\n",
    "        \"[ ] Early Stopping ë° ì²´í¬í¬ì¸íŠ¸ ì „ëµ\",\n",
    "        \"[ ] ê²€ì¦ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\",\n",
    "        \"[ ] í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìµœì¢… í‰ê°€\"\n",
    "    ]\n",
    "    \n",
    "    for item in checklist:\n",
    "        print(f\"  {item}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ ì‚¬í•­:\")\n",
    "    print(f\"1. ìœ„ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë“¤ì„ ìˆœì„œëŒ€ë¡œ ê²€í† \")\n",
    "    print(f\"2. ê°€ì¥ ì˜í–¥ì´ í´ ê²ƒ ê°™ì€ í•­ëª©ë¶€í„° ìš°ì„  ì ìš©\")\n",
    "    print(f\"3. ê° ë³€ê²½ì‚¬í•­ ì ìš© í›„ ì„±ëŠ¥ ì¸¡ì • ë° ë¹„êµ\")\n",
    "    print(f\"4. ì²´ê³„ì ì¸ ì‹¤í—˜ ê¸°ë¡ ë° ê´€ë¦¬\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "analyze_training_performance()\n",
    "suggest_improvements()\n",
    "performance_checklist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2535864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "def save_training_results():\n",
    "    \"\"\"í•™ìŠµ ê²°ê³¼ë¥¼ Google Driveì— ì €ì¥\"\"\"\n",
    "    print(\"ğŸ’¾ í•™ìŠµ ê²°ê³¼ ì €ì¥\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Google Drive ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "    drive_save_path = \"/content/drive/MyDrive/sprint-ai03-team04-results\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_dir = f\"{drive_save_path}/training_session_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        os.makedirs(session_dir, exist_ok=True)\n",
    "        print(f\"ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±: {session_dir}\")\n",
    "        \n",
    "        # outputs ë””ë ‰í† ë¦¬ ì „ì²´ ë³µì‚¬\n",
    "        if os.path.exists(outputs_dir):\n",
    "            target_outputs = os.path.join(session_dir, \"outputs\")\n",
    "            shutil.copytree(outputs_dir, target_outputs, dirs_exist_ok=True)\n",
    "            print(f\"âœ… í•™ìŠµ ê²°ê³¼ ë³µì‚¬ ì™„ë£Œ: {target_outputs}\")\n",
    "            \n",
    "            # ë³µì‚¬ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥\n",
    "            copied_files = os.listdir(target_outputs)\n",
    "            print(f\"ğŸ“‹ ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "            for file in copied_files:\n",
    "                file_path = os.path.join(target_outputs, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                    print(f\"  ğŸ“„ {file} ({size:.2f} MB)\")\n",
    "        else:\n",
    "            print(\"âŒ outputs ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            \n",
    "        return session_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_best_model():\n",
    "    \"\"\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\"\"\"\n",
    "    print(\"\\nğŸ“¥ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "            file_size = os.path.getsize(best_model_path) / (1024 * 1024)  # MB\n",
    "            print(f\"ğŸ“¦ ëª¨ë¸ íŒŒì¼: best_model.pth ({file_size:.2f} MB)\")\n",
    "            \n",
    "            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰\n",
    "            print(\"ğŸ”½ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
    "            files.download(best_model_path)\n",
    "            print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ best_model.pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "def create_training_summary():\n",
    "    \"\"\"í•™ìŠµ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
    "    print(\"\\nğŸ“„ í•™ìŠµ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    summary_content = []\n",
    "    summary_content.append(\"# í•™ìŠµ ìš”ì•½ ë³´ê³ ì„œ\")\n",
    "    summary_content.append(f\"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    summary_content.append(\"\")\n",
    "    \n",
    "    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´\n",
    "    best_model_path = os.path.join(outputs_dir, 'best_model.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        \n",
    "        summary_content.append(\"## í•™ìŠµ ê²°ê³¼\")\n",
    "        if 'epoch' in checkpoint:\n",
    "            summary_content.append(f\"- ìµœê³  ì„±ëŠ¥ ì—í¬í¬: {checkpoint['epoch']}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            summary_content.append(f\"- ìµœê³  ì„±ëŠ¥ ì‹œ ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.6f}\")\n",
    "        \n",
    "        if 'history' in checkpoint:\n",
    "            history = checkpoint['history']\n",
    "            if 'val_map' in history and history['val_map']:\n",
    "                best_map = max(history['val_map'])\n",
    "                summary_content.append(f\"- ìµœê³  mAP: {best_map:.4f}\")\n",
    "    \n",
    "    summary_content.append(\"\")\n",
    "    summary_content.append(\"## íŒŒì¼ ëª©ë¡\")\n",
    "    \n",
    "    if os.path.exists(outputs_dir):\n",
    "        for file in os.listdir(outputs_dir):\n",
    "            file_path = os.path.join(outputs_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                summary_content.append(f\"- {file} ({size:.2f} MB)\")\n",
    "    \n",
    "    # ìš”ì•½ íŒŒì¼ ì €ì¥\n",
    "    summary_path = os.path.join(outputs_dir, 'training_summary.md')\n",
    "    try:\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\\\n'.join(summary_content))\n",
    "        print(f\"âœ… ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {summary_path}\")\n",
    "        \n",
    "        # ë‚´ìš© ì¶œë ¥\n",
    "        print(\"\\\\nğŸ“‹ ìš”ì•½ ë‚´ìš©:\")\n",
    "        for line in summary_content:\n",
    "            print(f\"  {line}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "def backup_and_download():\n",
    "    \"\"\"ì „ì²´ ë°±ì—… ë° ë‹¤ìš´ë¡œë“œ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸš€ í•™ìŠµ ê²°ê³¼ ë°±ì—… ë° ë‹¤ìš´ë¡œë“œ ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. ìš”ì•½ ë³´ê³ ì„œ ìƒì„±\n",
    "    create_training_summary()\n",
    "    \n",
    "    # 2. Google Driveì— ì €ì¥\n",
    "    saved_dir = save_training_results()\n",
    "    if saved_dir:\n",
    "        print(f\"\\\\nâœ… Google Drive ì €ì¥ ì™„ë£Œ: {saved_dir}\")\n",
    "    \n",
    "    # 3. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "    download_best_model()\n",
    "    \n",
    "    print(\"\\\\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "    print(\"\\\\nğŸ“‹ ì €ì¥ëœ ë‚´ìš©:\")\n",
    "    print(\"1. Google Driveì— ì „ì²´ í•™ìŠµ ê²°ê³¼ ë°±ì—…\")\n",
    "    print(\"2. ë¡œì»¬ì— best_model.pth ë‹¤ìš´ë¡œë“œ\")\n",
    "    print(\"3. training_summary.md ìš”ì•½ ë³´ê³ ì„œ\")\n",
    "\n",
    "# ì‚¬ìš©ë²• ì•ˆë‚´\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ ë„êµ¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"\\\\nğŸ“‹ ì‚¬ìš© ë°©ë²•:\")\n",
    "print(\"1. backup_and_download() - ì „ì²´ ë°±ì—… ë° ë‹¤ìš´ë¡œë“œ\")\n",
    "print(\"2. download_best_model() - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ\")\n",
    "print(\"3. save_training_results() - Google Driveì—ë§Œ ì €ì¥\")\n",
    "print(\"\\\\nâš ï¸  ì£¼ì˜ì‚¬í•­:\")\n",
    "print(\"- Google Driveê°€ ë§ˆìš´íŠ¸ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\")\n",
    "print(\"- ì¶©ë¶„í•œ Drive ì €ì¥ ê³µê°„ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
