{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1sLn_VII4y3xEFFms-WdHEl1ifpxPLQAt","authorship_tag":"ABX9TyNER757tQ0Tqajh10Bg/B+g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 사전 설정"],"metadata":{"id":"ulJjWDDYRa3k"}},{"cell_type":"markdown","source":["## 라이브러리 설치"],"metadata":{"id":"wxiCaQ9BRjA1"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"zG6i5AItQ7tm","executionInfo":{"status":"ok","timestamp":1752801856153,"user_tz":-540,"elapsed":13761,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}}},"outputs":[],"source":["import os\n","import sys\n","import math\n","import glob\n","import random\n","import shutil\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageEnhance, ImageFilter\n","\n","from tqdm import tqdm\n","from sklearn.metrics import average_precision_score, classification_report\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.utils.data.sampler import WeightedRandomSampler\n","\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms.functional as F2\n","import torchvision.transforms.v2 as v2\n","from torchvision import transforms\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn"]},{"cell_type":"markdown","source":["## 구글 드라이브 연결"],"metadata":{"id":"OYiLDhMyRpWa"}},{"cell_type":"code","source":["from google.colab import drive\n","# 드라이브 마운트\n","drive.mount('/content/drive')\n","g_path  = \"/content/drive/MyDrive/data\"\n","os.makedirs(g_path, exist_ok=True)"],"metadata":{"id":"CJAv0uopUWcH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752801878699,"user_tz":-540,"elapsed":11762,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"91b95096-8b1c-4de9-ecea-8acd11ba257a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## GPU 설정"],"metadata":{"id":"nOsJdv6hktpK"}},{"cell_type":"code","source":["# device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"t4YQwlTNko65","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752801881047,"user_tz":-540,"elapsed":5,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"c592d0ca-2545-4f03-9145-edd0d29b55a1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["# 데이터 불러오기 및 탐색"],"metadata":{"id":"fA5ZdWsXUiJx"}},{"cell_type":"code","metadata":{"id":"8cc3f4f9"},"source":["# Install the Kaggle API\n","# !pip install kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2db7f4f"},"source":["# Download the competition data\n","# You will need to upload your kaggle.json API token for this step to work.\n","# See https://github.com/Kaggle/kaggle-api for details.\n","# !kaggle competitions download -c ai03-level1-project"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 관리자 권한 디렉토리 만들기\n","# !sudo mkdir -p /root/.config/kaggle"],"metadata":{"id":"y5ToYAPb9lRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# json 파일 만든 디렉토리에 복사\n","# !cp \"/content/drive/MyDrive/Sprint Project/kaggle.json\" /root/.config/kaggle/"],"metadata":{"id":"TyQm49JA6M1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모드 변경 (Read/Write)\n","# !chmod 600 /root/.config/kaggle/kaggle.json"],"metadata":{"id":"K8CKgQQD9ViU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 경로 설정\n","img_path = \"/content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_images\"\n","json_path = \"/content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_annotations\"\n","\n","print(f\"Image path: {img_path}\")\n","print(f\"Annotation path: {json_path}\")"],"metadata":{"id":"9vf3pJNIi3eE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752801885897,"user_tz":-540,"elapsed":44,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"530ed727-e0b8-4b3b-e15f-42d7d7b41f51"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Image path: /content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_images\n","Annotation path: /content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_annotations\n"]}]},{"cell_type":"code","metadata":{"id":"d52bad3a"},"source":["# Get the list of image files\n","#image_files = [os.path.join(root, f) for root, _, files in os.walk(img_path) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","\n","#if not image_files:\n","#    print(\"No image files found in the directory.\")\n","#else:\n","    # Load the first image to get the reference size\n","#    first_img_path = image_files[0]\n","#    first_img = cv2.imread(first_img_path)\n","\n","#    if first_img is None:\n","#        print(f\"Could not read the first image: {first_img_path}\")\n","#    else:\n","#        first_h, first_w, _ = first_img.shape\n","#        first_size = (first_w, first_h)\n","#        print(f\"Size of the first image ({os.path.basename(first_img_path)}): {first_size} (Width, Height)\")\n","\n","#        all_same_size = True\n","        # Check the size of remaining images\n","#        for img_file in image_files[1:]:\n","#            img = cv2.imread(img_file)\n","#           if img is not None:\n","#                h, w, _ = img.shape\n","#                current_size = (w, h)\n","#                if current_size != first_size:\n","#                    print(f\"Image {os.path.basename(img_file)} has a different size: {current_size}\")\n","#                    all_same_size = False\n","#            else:\n","#                print(f\"Could not read image: {os.path.basename(img_file)}\")\n","\n","#        if all_same_size:\n","#            print(\"\\nAll images checked have the same size as the first image.\")\n","\n","#print(\"\\nSimple size check complete.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8dc7b50"},"source":["# 데이터 수량 확인\n","#def count_files_in_directory(directory):\n","#   if not os.path.exists(directory):\n","#       return 0\n","#   return len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n","\n","#num_image_files = count_files_in_directory(img_path)\n","#num_annotation_files = count_files_in_directory(json_path)\n","\n","#print(f\"Number of image files in '{img_path}': {num_image_files}\")\n","#print(f\"Number of annotation files in '{json_path}': {num_annotation_files}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 폴더 확인 후 train_anotation 안에 하위 폴더들이 있고 그 안에 파일들 있는 것 확인."],"metadata":{"id":"kAyzwg5rm2MQ"}},{"cell_type":"code","metadata":{"id":"ae70e148"},"source":["#def count_files_recursive(directory):\n","#   if not os.path.exists(directory):\n","#       return 0\n","#       count = 0\n","#       for root, _, files in os.walk(directory):\n","#        count += len(files)\n","#       return count\n","\n","#num_annotation_files_recursive = count_files_recursive(json_path)\n","\n","#print(f\"Total number of annotation files (including subdirectories) in '{json_path}': {num_annotation_files_recursive}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dff514c3","executionInfo":{"status":"ok","timestamp":1752802145607,"user_tz":-540,"elapsed":253708,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"3a775263-f048-4abe-f195-28391157e388"},"source":["# Count image files (non-recursive)\n","img_path = \"/content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_images\"\n","num_image_files = sum([len(files) for r, d, files in os.walk(img_path)])\n","print(f\"Number of image files in '{img_path}': {num_image_files}\")\n","\n","# Count annotation files (recursive)\n","json_path = \"/content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_annotations\"\n","num_annotation_files_recursive = sum([len(files) for r, d, files in os.walk(json_path)])\n","print(f\"Total number of annotation files (including subdirectories) in '{json_path}': {num_annotation_files_recursive}\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of image files in '/content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_images': 1489\n","Total number of annotation files (including subdirectories) in '/content/drive/MyDrive/data/ai03-level1-project.zip (Unzipped Files)/train_annotations': 4526\n"]}]},{"cell_type":"code","metadata":{"id":"4718c4a4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752802473086,"user_tz":-540,"elapsed":1660,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"ba76ef60-7765-4d03-98aa-6c6a601c3a97"},"source":["# Pair image files and annotation file\n","def get_file_paths(img_dir, json_dir, ids_to_include=None):\n","    \"\"\"\n","    Pairs image files with their corresponding annotation files.\n","\n","    Args:\n","        img_dir (str): Directory containing image files.\n","        json_dir (str): Directory containing annotation files (can be recursive).\n","        ids_to_include (list, optional): A list of filenames (without extensions)\n","                                          to include in the pairing. If None, all\n","                                          matching files are paired.\n","\n","    Returns:\n","        list: A list of tuples, where each tuple is (image_path, annotation_path).\n","    \"\"\"\n","    image_files = [os.path.join(root, f) for root, _, files in os.walk(img_dir) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","    annotation_files = [os.path.join(root, f) for root, _, files in os.walk(json_dir) for f in files if f.lower().endswith('.json')]\n","\n","    # Create a dictionary mapping image filenames (without extension) to their full paths\n","    image_dict = {os.path.splitext(os.path.basename(img))[0]: img for img in image_files}\n","\n","    # Create a list of pairs of image and annotation file paths, matching by filename\n","    paired_files = []\n","    for annotation_path in annotation_files:\n","        annotation_filename_without_ext = os.path.splitext(os.path.basename(annotation_path))[0]\n","\n","        # Check if the filename is in the image dictionary and if it should be included based on ids_to_include\n","        if annotation_filename_without_ext in image_dict and \\\n","           (ids_to_include is None or annotation_filename_without_ext in ids_to_include):\n","            image_path = image_dict[annotation_filename_without_ext]\n","            paired_files.append((image_path, annotation_path))\n","\n","    return paired_files\n","\n","# Get the paired image and annotation files (using all files by default)\n","paired_files = get_file_paths(img_path, json_path)\n","\n","print(f\"Found {len(paired_files)} paired image and annotation files.\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 4526 paired image and annotation files.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1o2egxZuUU6M83GMOK1xeMezgKBYPDOPQ"},"id":"a090aec1","executionInfo":{"status":"ok","timestamp":1752802485347,"user_tz":-540,"elapsed":9861,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"f3fc139d-afb0-435c-e1b3-ed4aeacfd58b","collapsed":true},"source":["import cv2\n","import json\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Assuming 'paired_files' is already defined and contains (image_path, annotation_path) tuples\n","# If not, you'll need to run the cell that defines 'paired_files' first.\n","if 'paired_files' not in locals() or not paired_files:\n","    print(\"Error: 'paired_files' not found or is empty. Please run the cell that loads and pairs image and annotation files.\")\n","else:\n","    # Select a sample image and annotation file (e.g., the first one in the list)\n","    # Iterate through the first 5 paired files or fewer if less than 5 exist\n","    num_samples_to_show = min(5, len(paired_files))\n","\n","    for i in range(num_samples_to_show):\n","        sample_image_path, sample_annotation_path = paired_files[i]\n","\n","        # Load the image\n","        image = cv2.imread(sample_image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB for matplotlib\n","\n","        # Load the annotation file\n","        with open(sample_annotation_path, 'r') as f:\n","            annotation_data = json.load(f)\n","\n","        # Extract bounding boxes from the annotation data\n","        # Assuming the annotation format is similar to the sample shown previously\n","        # where 'annotations' is a list of dicts and each dict has a 'bbox' key [x, y, w, h]\n","        bounding_boxes = []\n","        if 'annotations' in annotation_data:\n","            for annotation in annotation_data['annotations']:\n","                if 'bbox' in annotation:\n","                    x, y, w, h = annotation['bbox']\n","                    bounding_boxes.append((x, y, w, h))\n","\n","        # Draw bounding boxes on the image\n","        image_with_boxes = image.copy()\n","        for (x, y, w, h) in bounding_boxes:\n","            # Draw a rectangle (bounding box) on the image\n","            # cv2.rectangle(image, (x, y), (x+w, y+h), color, thickness)\n","            # Color is in BGR format for OpenCV, so use (255, 0, 0) for blue.\n","            # If using matplotlib to display, keep it in RGB. We are using matplotlib, so color can be (0, 0, 255) for blue\n","            cv2.rectangle(image_with_boxes, (x, y), (x+w, y+h), (0, 0, 255), 2) # Blue color, thickness 2\n","\n","        # Display the image with bounding boxes\n","        plt.figure(figsize=(10, 10))\n","        plt.imshow(image_with_boxes)\n","        plt.title(f\"Sample {i+1}: Image with Bounding Boxes: {os.path.basename(sample_image_path)}\")\n","        plt.axis('off') # Hide axes\n","        plt.show()\n","\n","        print(f\"\\nSample {i+1}: Loaded annotations for {os.path.basename(sample_image_path)}:\")\n","        display(annotation_data)"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"584ae5e2"},"source":["Here's a custom dataset class for loading your image and annotation data. This class will:\n","\n","- Inherit from `torch.utils.data.Dataset`.\n","- Implement `__len__` to return the total number of data samples.\n","- Implement `__getitem__` to load an image and its corresponding annotations for a given index.\n","- Include basic image loading and annotation parsing based on the structure of your JSON files.\n","- Allow for optional data transformations."]},{"cell_type":"code","metadata":{"id":"96bbd508","executionInfo":{"status":"ok","timestamp":1752806837770,"user_tz":-540,"elapsed":50,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}}},"source":["from torch.utils.data import Dataset\n","import os\n","import json\n","import cv2\n","import torch\n","import numpy as np\n","import torchvision.transforms.functional as F2 # Import F2 for tensor conversion\n","from collections import defaultdict # Import defaultdict for COCO format\n","from tqdm import tqdm # Import tqdm for progress bar\n","\n","\n","class PillDetectionDataset(Dataset):\n","    def __init__(self, paired_files, transforms=None):\n","        \"\"\"\n","        Args:\n","            paired_files (list): List of tuples, where each tuple is (image_path, annotation_path).\n","            transforms (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.paired_files = paired_files\n","        self.transforms = transforms\n","\n","        # Create a mapping from original category IDs to COCO category IDs (1-indexed)\n","        # We need to iterate through all annotation files to get the unique original category IDs\n","        unique_original_category_ids = set()\n","        print(\"Collecting unique original category IDs for mapping...\")\n","        # Use tqdm to show progress for collecting unique IDs\n","        for _, annotation_path in tqdm(paired_files, desc=\"Collecting Unique Category IDs\"):\n","            try:\n","                with open(annotation_path, 'r') as f:\n","                    annotation_data = json.load(f)\n","                if 'annotations' in annotation_data:\n","                    for annotation in annotation_data['annotations']:\n","                        if 'category_id' in annotation:\n","                            unique_original_category_ids.add(annotation['category_id'])\n","            except Exception as e:\n","                print(f\"Error processing annotation file {annotation_path}: {e}\")\n","                continue # Skip problematic annotation files\n","\n","\n","        # Sort the unique original category IDs to create a consistent mapping\n","        sorted_original_category_ids = sorted(list(unique_original_category_ids))\n","        # Create a mapping: original_id -> coco_id (1-indexed)\n","        self.original_to_coco_category_id = {\n","            original_id: i + 1 for i, original_id in enumerate(sorted_original_category_ids)\n","        }\n","        # Determine the actual number of classes based on unique original IDs\n","        self.num_actual_classes = len(sorted_original_category_ids)\n","        self.expected_max_label = self.num_actual_classes # Expected max 1-indexed label\n","\n","        print(f\"Created mapping for {len(self.original_to_coco_category_id)} original categories. Actual number of classes: {self.num_actual_classes}\")\n","        # Print the mapping for inspection (optional)\n","        # print(\"Original to COCO category ID mapping:\")\n","        # display(self.original_to_coco_category_id)\n","\n","\n","    def __len__(self):\n","        return len(self.paired_files)\n","\n","    def __getitem__(self, idx):\n","        img_path, annotation_path = self.paired_files[idx]\n","\n","        # Load image\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            print(f\"Warning: Could not load image {img_path}. Skipping.\")\n","            # Return None or raise an error, depending on desired behavior\n","            # For now, returning None and handling it in the DataLoader loop (if collate_fn supports it)\n","            return None, None\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n","        img_height, img_width, _ = img.shape\n","\n","        # Load annotations\n","        try:\n","            with open(annotation_path, 'r') as f:\n","                annotation_data = json.load(f)\n","        except Exception as e:\n","            print(f\"Error loading annotation file {annotation_path}: {e}. Skipping image.\")\n","            return None, None\n","\n","\n","        # Extract bounding boxes and labels\n","        boxes = []\n","        labels = []\n","        # Assuming 'annotations' is a list of dicts and each dict has 'bbox' and 'category_id'\n","        if 'annotations' in annotation_data:\n","            for annotation in annotation_data['annotations']:\n","                if 'bbox' in annotation and 'category_id' in annotation:\n","                    x, y, w, h = annotation['bbox']\n","\n","                    # Validate bounding box coordinates\n","                    # Ensure coordinates are within image bounds and width/height are positive\n","                    x_min, y_min, x_max, y_max = x, y, x + w, y + h\n","\n","                    # Clip coordinates to image bounds\n","                    x_min = max(0, x_min)\n","                    y_min = max(0, y_min)\n","                    x_max = min(img_width, x_max)\n","                    y_max = min(img_height, y_max) # Corrected typo: img_height\n","\n","                    # Check for invalid box dimensions after clipping\n","                    if x_max > x_min and y_max > y_min:\n","                        # Get the COCO format label using the mapping\n","                        original_label = annotation['category_id']\n","\n","                        if original_label in self.original_to_coco_category_id:\n","                            coco_label = self.original_to_coco_category_id[original_label]\n","                            # Validate mapped COCO label range (should be 1 to num_actual_classes)\n","                            # This check is redundant if original_label is in the mapping, but kept for clarity\n","                            if 1 <= coco_label <= self.expected_max_label:\n","                                boxes.append([x_min, y_min, x_max, y_max]) # Convert [x, y, w, h] to [x_min, y_min, x_max, y_max]\n","                                labels.append(coco_label)\n","                            else:\n","                                # This case should ideally not happen if mapping is correct and original IDs are within unique_original_category_ids\n","                                print(f\"Error: Mapped COCO label {coco_label} for original ID {original_label} is out of expected range [1, {self.expected_max_label}] in {annotation_path}. Skipping annotation.\")\n","                        else:\n","                             # If original category ID is not in the mapping, skip this annotation\n","                             print(f\"Warning: Original category ID {original_label} found in {annotation_path} but not in initial mapping. Skipping annotation.\")\n","\n","\n","                    else:\n","                        print(f\"Warning: Invalid bounding box dimensions [{x}, {y}, {w}, {h}] for image {os.path.basename(img_path)}. Skipping annotation.\")\n","\n","\n","        # Convert boxes and labels to tensors\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","        # Create target dictionary\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        # You might want to add image_id, area, iscrowd etc. depending on your needs\n","        target[\"image_id\"] = torch.tensor([idx]) # Simple image_id for now\n","\n","        # Convert image to tensor\n","        # Using F2.to_image and F2.to_dtype to handle different image types and convert to float\n","        try:\n","            img = F2.to_image(img)\n","            img = F2.to_dtype(img, torch.float32, scale=True) # Scale to [0, 1]\n","        except Exception as e:\n","            print(f\"Error converting image {img_path} to tensor: {e}. Skipping.\")\n","            return None, None\n","\n","\n","        if self.transforms:\n","             # Apply transforms to both image and target\n","             # Note: Applying transforms to bounding boxes requires special handling.\n","             # The transforms you use should be compatible with bounding box transformations.\n","             # For simplicity, this example only applies transforms to the image.\n","             # You will need to modify this part if you need data augmentation that affects bounding boxes.\n","            img = self.transforms(img)\n","\n","        # Ensure the number of boxes and labels match after validation and processing\n","        if boxes.shape[0] != labels.shape[0]:\n","             print(f\"Warning: Mismatch between number of boxes ({boxes.shape[0]}) and labels ({labels.shape[0]}) in image {os.path.basename(img_path)}. Skipping image.\")\n","             return None, None\n","\n","        return img, target\n","\n","    def get_annotations(self):\n","        \"\"\"\n","        Returns annotations in COCO format.\n","        \"\"\"\n","        coco_format_data = {\n","            \"images\": [],\n","            \"annotations\": [],\n","            \"categories\": []\n","        }\n","\n","        annotation_id_counter = 0\n","        # Create COCO categories based on the mapping\n","        # Ensure categories are included for all original IDs in the mapping\n","        coco_format_data[\"categories\"] = [\n","            {\"id\": coco_id, \"name\": f\"category_{original_id}\", \"supercategory\": \"none\"}\n","            for original_id, coco_id in sorted(self.original_to_coco_category_id.items(), key=lambda item: item[1]) # Sort by coco_id\n","        ]\n","        # Add background category (ID 0)\n","        if not any(cat['id'] == 0 for cat in coco_format_data[\"categories\"]):\n","             coco_format_data[\"categories\"].insert(0, {\"id\": 0, \"name\": \"background\", \"supercategory\": \"none\"}) # Insert at the beginning\n","\n","\n","        for idx, (img_path, annotation_path) in enumerate(tqdm(self.paired_files, desc=\"Formatting to COCO\")):\n","            # Add image info\n","            img = cv2.imread(img_path)\n","            if img is None:\n","                continue # Skip if image could not be loaded\n","            img_height, img_width, _ = img.shape\n","            coco_format_data[\"images\"].append({\n","                \"id\": idx, # Use index as image_id for simplicity\n","                \"width\": img_width,\n","                \"height\": img_height,\n","                \"file_name\": os.path.basename(img_path)\n","            })\n","\n","            # Load annotations\n","            try:\n","                with open(annotation_path, 'r') as f:\n","                    annotation_data = json.load(f)\n","            except Exception as e:\n","                print(f\"Error loading annotation file {annotation_path}: {e}. Skipping annotations for this image.\")\n","                continue\n","\n","\n","            if 'annotations' in annotation_data:\n","                for annotation in annotation_data['annotations']:\n","                    if 'bbox' in annotation and 'category_id' in annotation:\n","                        x, y, w, h = annotation['bbox']\n","                        original_label = annotation['category_id']\n","\n","                        if original_label in self.original_to_coco_category_id:\n","                            coco_label = self.original_to_coco_category_id[original_label]\n","\n","                            # Basic validation for bbox (optional, but good practice)\n","                            # Ensure bbox is within image bounds and has positive dimensions\n","                            x_min, y_min, x_max, y_max = x, y, x + w, y + h\n","                            x_min = max(0, x_min)\n","                            y_min = max(0, y_min)\n","                            x_max = min(img_width, x_max)\n","                            y_max = min(img_height, y_max)\n","                            valid_bbox = x_max > x_min and y_max > y_min\n","\n","                            if valid_bbox:\n","                                coco_format_data[\"annotations\"].append({\n","                                    \"id\": annotation_id_counter,\n","                                    \"image_id\": idx, # Use index as image_id\n","                                    \"category_id\": coco_label,\n","                                    \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min], # Store in [x, y, w, h] format for COCO\n","                                    \"area\": (x_max - x_min) * (y_max - y_min),\n","                                    \"iscrowd\": annotation.get(\"iscrowd\", 0) # Assume 0 if not present\n","                                })\n","                                annotation_id_counter += 1\n","                            else:\n","                                print(f\"Warning: Invalid or out-of-bounds bounding box after clipping in {os.path.basename(img_path)}. Skipping annotation.\")\n","\n","                        # No else needed here, as original_label not in mapping is handled during __getitem__ init\n","\n","\n","        # Create a dummy COCO object from the formatted data\n","        # This is a workaround as CocoEvaluator expects a COCO object\n","        class DummyCOCO:\n","            def __init__(self, dataset):\n","                self.dataset = dataset\n","                self.imgToAnns, self.catToImgs = self.createIndex()\n","                self.imgs = {img['id']: img for img in dataset['images']} # Add imgs attribute\n","                self.cats = {cat['id']: cat for cat in dataset['categories']} # Add cats attribute\n","                self.anns = {ann['id']: ann for ann in dataset['annotations']} # Add anns attribute\n","\n","\n","            def createIndex(self):\n","                # Simplified indexing for evaluation\n","                imgToAnns = defaultdict(list)\n","                catToImgs = defaultdict(list)\n","                for ann in self.dataset['annotations']:\n","                    imgToAnns[ann['image_id']].append(ann)\n","                    catToImgs[ann['category_id']].append(ann['image_id'])\n","                return imgToAnns, catToImgs\n","\n","            def getImgIds(self, imgIds=None, catIds=None):\n","                 imgIds = imgIds or list(self.imgs.keys())\n","                 catIds = catIds or list(self.cats.keys())\n","\n","                 img_ids_with_cat = set()\n","                 for ann in self.dataset['annotations']:\n","                     if ann['category_id'] in catIds:\n","                         img_ids_with_cat.add(ann['image_id'])\n","\n","                 if imgIds:\n","                     return list(set(imgIds) & img_ids_with_cat)\n","                 else:\n","                     return list(img_ids_with_cat)\n","\n","\n","            def loadImgs(self, ids=None):\n","                if ids is None:\n","                    return self.dataset['images']\n","                else:\n","                    return [self.imgs[id] for id in ids if id in self.imgs]\n","\n","\n","            def loadAnns(self, ids=None):\n","                 if ids is None:\n","                     return self.dataset['annotations']\n","                 else:\n","                     return [self.anns[id] for id in ids if id in self.anns]\n","\n","\n","            def getCatIds(self, catNms=None, supNms=None, catIds=None):\n","                 cats = self.dataset['categories']\n","                 cats = cats if catNms is None else [cat for cat in cats if cat['name'] in catNms]\n","                 cats = cats if supNms is None else [cat for cat in cats if cat['supercategory'] in supNms]\n","                 cat_ids = [cat['id'] for cat in cats]\n","\n","                 if catIds:\n","                     return list(set(cat_ids) & set(catIds))\n","                 else:\n","                     return cat_ids\n","\n","            def loadCats(self, ids):\n","                return [self.cats[id] for id in ids if id in self.cats]\n","\n","\n","        return DummyCOCO(coco_format_data)"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7f4e5ab1"},"source":["This dataset class takes the list of `paired_files` generated earlier and an optional `transforms` object. The `__getitem__` method loads the image and its corresponding JSON annotation, extracts the bounding boxes and category IDs, converts them to tensors, and returns the image and target dictionary.\n","\n","**Important Considerations:**\n","\n","- **Bounding Box Transformations:** If you apply data augmentations that change the image geometry (e.g., resizing, cropping, flipping), you will need to implement corresponding transformations for the bounding boxes. Libraries like `torchvision.transforms.v2` or `albumentations` can help with this. The current code only applies image transformations.\n","- **Image ID:** A simple sequential image ID is used here. You might want to use a more robust identifier if needed.\n","- **Other Annotation Data:** The current implementation only extracts bounding boxes and category IDs. You can extend it to include other information from the annotation file (e.g., segmentation masks, keypoints) if your task requires it."]},{"cell_type":"markdown","metadata":{"id":"20ada728"},"source":["Now that you have the `PillDetectionDataset` class, let's create DataLoaders for training and validation. We'll first split the `paired_files` into training and validation sets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"634ae21b","executionInfo":{"status":"ok","timestamp":1752806574396,"user_tz":-540,"elapsed":24,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"5df7618f-1e03-460f-a3f7-d48478cda44d"},"source":["from torch.utils.data import DataLoader, random_split\n","\n","# Define the split ratio (e.g., 80% for training, 20% for validation)\n","train_ratio = 0.8\n","val_ratio = 0.2\n","\n","# Ensure the sum of ratios is 1\n","assert train_ratio + val_ratio == 1.0\n","\n","# Calculate the number of samples for each set\n","total_samples = len(paired_files)\n","train_size = int(train_ratio * total_samples)\n","val_size = total_samples - train_size # Ensure all samples are used\n","\n","# Split the dataset\n","# We split the paired_files list directly as the dataset takes this list as input\n","train_paired_files, val_paired_files = random_split(paired_files, [train_size, val_size])\n","\n","print(f\"Total samples: {total_samples}\")\n","print(f\"Training samples: {len(train_paired_files)}\")\n","print(f\"Validation samples: {len(val_paired_files)}\")"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Total samples: 4526\n","Training samples: 3620\n","Validation samples: 906\n"]}]},{"cell_type":"markdown","metadata":{"id":"9a22f7fa"},"source":["Now, we'll create the `PillDetectionDataset` instances for the training and validation sets and then create the DataLoaders.\n","\n","For object detection, a custom `collate_fn` is usually needed for the DataLoader to handle targets (bounding boxes, labels, etc.) of varying sizes in a batch."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"647864c0","executionInfo":{"status":"ok","timestamp":1752806578740,"user_tz":-540,"elapsed":5,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"0a0326ae-4614-4517-a7e8-9116efc4da6b"},"source":["from torchvision.transforms import functional as F\n","\n","# Define a collation function for the DataLoader\n","# This function takes a batch of samples (image, target) and stacks them into tensors\n","# while handling the target dictionary.\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","# Create dataset instances for training and validation\n","# You can add transforms here if needed, e.g., data augmentation for training\n","train_dataset = PillDetectionDataset(train_paired_files)\n","val_dataset = PillDetectionDataset(val_paired_files)\n","\n","# Define batch size\n","batch_size = 2 # You can adjust this based on your GPU memory\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","print(\"\\nDataLoaders created successfully.\")\n","print(f\"Training DataLoader has {len(train_loader)} batches of size {batch_size}\") # Use batch_size from the loader\n","print(f\"Validation DataLoader has {len(val_loader)} batches of size {batch_size}\") # Use batch_size from the loader"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataLoaders created successfully.\n","Training DataLoader has 1810 batches of size 2\n","Validation DataLoader has 453 batches of size 2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"aZQubuTIoqPN","executionInfo":{"status":"error","timestamp":1752802966868,"user_tz":-540,"elapsed":4969,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"287a5017-2b3c-4a61-e768-b4096059444a"},"source":["# Determine the number of unique category IDs\n","unique_category_ids = set()\n","for _, annotation_path in tqdm(paired_files, desc=\"Collecting unique category IDs\"):\n","    with open(annotation_path, 'r') as f:\n","        annotation_data = json.load(f)\n","    if 'annotations' in annotation_data:\n","        for annotation in annotation_data['annotations']:\n","            if 'category_id' in annotation:\n","                unique_category_ids.add(annotation['category_id'])\n","\n","# The number of classes for the model is the number of unique categories + 1 (for background)\n","num_classes = len(unique_category_ids)\n","print(f\"\\nFound {num_classes} unique category IDs in the dataset.\")"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Collecting unique category IDs:   0%|          | 13/4526 [00:04<28:34,  2.63it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-24-3662219752.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaired_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Collecting unique category IDs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mannotation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'annotations'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mannotation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edca4b0f","executionInfo":{"status":"ok","timestamp":1752806707712,"user_tz":-540,"elapsed":668,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"49dc552f-e9fb-4861-8e70-50174ba15dc0"},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load a model pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","    # get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","    # replace the pre-trained head with a new one that has num_classes\n","    # Note: The number of classes in the classifier head should be num_classes + 1\n","    # because the background is considered as class 0.\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n","\n","    return model\n","\n","# Based on previous execution, the actual number of unique categories is 73.\n","num_classes = 73\n","\n","# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","print(\"Faster R-CNN model defined and moved to device.\")"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Faster R-CNN model defined and moved to device.\n"]}]},{"cell_type":"code","metadata":{"id":"222c0423","executionInfo":{"status":"ok","timestamp":1752806713107,"user_tz":-540,"elapsed":36,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}}},"source":["import torch\n","from tqdm import tqdm\n","import numpy as np\n","from collections import defaultdict\n","\n","# Helper function to calculate Intersection over Union (IoU)\n","def calculate_iou(box1, box2):\n","    \"\"\"\n","    Calculates the Intersection over Union (IoU) of two bounding boxes.\n","    Boxes are expected in [x_min, y_min, x_max, y_max] format.\n","    \"\"\"\n","    # Determine the coordinates of the intersection rectangle\n","    x_min_inter = max(box1[0], box2[0])\n","    y_min_inter = max(box1[1], box2[1])\n","    x_max_inter = min(box1[2], box2[2])\n","    y_max_inter = min(box1[3], box2[3])\n","\n","    # Compute the area of intersection rectangle\n","    inter_area = max(0, x_max_inter - x_min_inter) * max(0, y_max_inter - y_min_inter)\n","\n","    # Compute the area of both the prediction and ground-truth rectangles\n","    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","\n","    # Compute the intersection over union by dividing the intersection area by the sum of prediction and ground-truth areas - the intersection area\n","    iou = inter_area / (box1_area + box2_area - inter_area + 1e-6) # Add a small epsilon to avoid division by zero\n","\n","    return iou\n","\n","# Helper function to calculate Average Precision (AP) for a single class\n","def calculate_ap(recall, precision):\n","    \"\"\"\n","    Calculates Average Precision (AP) from precision and recall values.\n","    Uses the 11-point interpolation method (common in original PASCAL VOC).\n","    For a more accurate method (used in COCO), you would interpolate over all unique recall values.\n","    \"\"\"\n","    # 11-point interpolation\n","    ap = 0.0\n","    for t in np.arange(0.0, 1.1, 0.1):\n","        if np.sum(recall >= t) == 0:\n","            p = 0\n","        else:\n","            p = np.max(precision[recall >= t])\n","        ap += p / 11.0\n","    return ap\n","\n","# Evaluation function to calculate mAP\n","def evaluate(model, data_loader, device, iou_threshold=0.5):\n","    \"\"\"\n","    Evaluates the model on the provided data loader and calculates mAP.\n","\n","    Args:\n","        model: The object detection model to evaluate.\n","        data_loader: The DataLoader for the evaluation dataset.\n","        device: The device to run evaluation on (e.g., 'cuda' or 'cpu').\n","        iou_threshold (float): The IoU threshold for considering a detection as a true positive.\n","\n","    Returns:\n","        float: The calculated mean Average Precision (mAP).\n","    \"\"\"\n","    model.eval() # Set the model to evaluation mode\n","\n","    # Store predictions and ground truth for each image\n","    all_predictions = []\n","    all_ground_truth = []\n","    image_ids = []\n","\n","    with torch.no_grad(): # Disable gradient calculation during evaluation\n","        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n","            images = list(image.to(device) for image in images)\n","            # targets are the ground truth annotations\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","            # Get predictions from the model\n","            predictions = model(images)\n","\n","            # Collect predictions and ground truth for each image in the batch\n","            for i, p in enumerate(predictions):\n","                 # Move tensors to CPU and convert to numpy arrays or lists for easier processing\n","                 boxes = p['boxes'].cpu().numpy()\n","                 scores = p['scores'].cpu().numpy()\n","                 labels = p['labels'].cpu().numpy() # These are the 1-indexed labels from the model\n","\n","                 # Get ground truth for the current image\n","                 gt_boxes = targets[i]['boxes'].cpu().numpy()\n","                 gt_labels = targets[i]['labels'].cpu().numpy() # These are the 1-indexed labels from the dataset\n","\n","                 # Store predictions and ground truth along with image ID\n","                 # Assuming image_id is available in targets\n","                 img_id = targets[i]['image_id'].item() if targets and 'image_id' in targets[i] else len(all_predictions) # Use index if image_id not available\n","\n","                 all_predictions.append({\n","                     \"image_id\": img_id,\n","                     \"boxes\": boxes,\n","                     \"scores\": scores,\n","                     \"labels\": labels\n","                 })\n","                 all_ground_truth.append({\n","                     \"image_id\": img_id,\n","                     \"boxes\": gt_boxes,\n","                     \"labels\": gt_labels,\n","                     \"detected\": np.zeros(len(gt_boxes), dtype=bool) # Keep track of detected ground truth boxes\n","                 })\n","                 image_ids.append(img_id)\n","\n","    # --- Calculate mAP ---\n","    # This is a simplified mAP calculation. For a more rigorous evaluation (like COCO),\n","    # you would need to consider different IoU thresholds and more sophisticated matching.\n","\n","    # Get all unique class labels (excluding background, assuming background is 0)\n","    # Collect all labels from ground truth and predictions to find unique labels\n","    all_labels = set()\n","    for gt in all_ground_truth:\n","        all_labels.update(gt['labels'])\n","    for pred in all_predictions:\n","        all_labels.update(pred['labels'])\n","\n","    # Remove background label if present (assuming background is 0)\n","    if 0 in all_labels:\n","        all_labels.remove(0)\n","\n","    unique_classes = sorted(list(all_labels))\n","\n","    average_precisions = []\n","\n","    for class_id in tqdm(unique_classes, desc=\"Calculating AP per class\"):\n","        # Get predictions and ground truth for the current class across all images\n","        class_predictions = []\n","        class_ground_truth = []\n","        for img_pred, img_gt in zip(all_predictions, all_ground_truth):\n","            # Filter predictions for the current class\n","            class_preds_for_img = [(img_pred['boxes'][i], img_pred['scores'][i])\n","                                   for i in range(len(img_pred['labels']))\n","                                   if img_pred['labels'][i] == class_id]\n","            class_predictions.extend(class_preds_for_img)\n","\n","            # Filter ground truth for the current class\n","            class_gts_for_img = [img_gt['boxes'][i]\n","                                 for i in range(len(img_gt['labels']))\n","                                 if img_gt['labels'][i] == class_id]\n","            class_ground_truth.extend(class_gts_for_img) # Append boxes\n","\n","            # Reset 'detected' flag for ground truth for this class across all images\n","            # This is not entirely correct for per-class AP calculation over all images.\n","            # A more rigorous approach tracks detections per image and per class.\n","            # For simplicity in this basic implementation, we'll reset per class.\n","            # In a true implementation, you would need to manage detection status per ground truth box.\n","            # For this basic version, we'll just count total ground truths for the class.\n","            # The matching logic below is also simplified.\n","\n","        # Sort predictions by confidence score in descending order\n","        class_predictions.sort(key=lambda x: x[1], reverse=True)\n","\n","        # Simplified True Positives and False Positives calculation\n","        TP = np.zeros(len(class_predictions))\n","        FP = np.zeros(len(class_predictions))\n","        num_ground_truths = len(class_ground_truth)\n","\n","        # Keep track of which ground truth boxes have been matched for this class across all images\n","        # This requires a more complex structure than a simple boolean array if ground truths\n","        # from different images are in a single list.\n","        # For a correct implementation, you need to match predictions to ground truths within each image first.\n","        # Let's refine the process to match within images.\n","\n","        # Re-structure: Match predictions to ground truths within each image\n","        img_wise_matches = defaultdict(list) # Store (pred_idx, gt_idx, iou) for matches in each image\n","        img_wise_detected_gt = defaultdict(set) # Store indices of detected ground truths per image\n","\n","        # Iterate through predictions and match with ground truth within the same image\n","        # This requires knowing which prediction corresponds to which image.\n","        # The current structure of class_predictions flattens across images.\n","        # We need to iterate through images again.\n","\n","        # Let's restart the matching logic for per-image matching\n","\n","        true_positives = [] # List of booleans, whether each prediction is a TP\n","        false_positives = [] # List of booleans, whether each prediction is a FP\n","        scores = [] # List of scores for all predictions of this class\n","\n","        # Iterate through each image's predictions and ground truths\n","        for img_pred, img_gt in zip(all_predictions, all_ground_truth):\n","            img_preds_for_class = [(img_pred['boxes'][i], img_pred['scores'][i])\n","                                   for i in range(len(img_pred['labels']))\n","                                   if img_pred['labels'][i] == class_id]\n","\n","            img_gts_for_class = [(img_gt['boxes'][i], i) # Store box and original index\n","                                 for i in range(len(img_gt['labels']))\n","                                 if img_gt['labels'][i] == class_id]\n","\n","            # Keep track of detected ground truth indices for this image and class\n","            detected_gt_indices = set()\n","\n","            # Sort predictions for this image by score\n","            img_preds_for_class.sort(key=lambda x: x[1], reverse=True)\n","\n","            for pred_box, score in img_preds_for_class:\n","                scores.append(score)\n","                is_tp = False\n","                best_iou = 0.0\n","                best_gt_idx = -1\n","\n","                # Find the best matching ground truth box for this prediction within the image\n","                for gt_box, gt_idx in img_gts_for_class:\n","                    iou = calculate_iou(pred_box, gt_box)\n","                    if iou > best_iou:\n","                        best_iou = iou\n","                        best_gt_idx = gt_idx\n","\n","                # Determine if it's a True Positive or False Positive\n","                if best_iou >= iou_threshold:\n","                    # Check if the best matching ground truth has already been detected by a higher scoring prediction\n","                    if best_gt_idx not in detected_gt_indices:\n","                        is_tp = True\n","                        detected_gt_indices.add(best_gt_idx) # Mark this ground truth as detected\n","\n","                if is_tp:\n","                    true_positives.append(True)\n","                    false_positives.append(False)\n","                else:\n","                    false_positives.append(True)\n","                    true_positives.append(False) # Not a true positive\n","\n","        # Sort TP, FP, and scores by score in descending order\n","        # Combine scores with TP and FP flags for sorting\n","        sorted_indices = np.argsort(scores)[::-1]\n","        sorted_tp = np.array(true_positives)[sorted_indices]\n","        sorted_fp = np.array(false_positives)[sorted_indices]\n","\n","        # Calculate cumulative TP and FP\n","        cumulative_tp = np.cumsum(sorted_tp)\n","        cumulative_fp = np.cumsum(sorted_fp)\n","\n","        # Calculate Precision and Recall\n","        # Precision = TP / (TP + FP)\n","        # Recall = TP / Number of Ground Truths for this class\n","        num_ground_truths_for_class = sum(len([lbl for lbl in gt['labels'] if lbl == class_id]) for gt in all_ground_truth)\n","\n","        if num_ground_truths_for_class == 0:\n","            # If there are no ground truths for this class, AP is 0 unless there are also no predictions.\n","            # If there are predictions but no ground truths, precision is 0, recall is 0. AP is 0.\n","            # If there are no predictions and no ground truths, AP is typically considered 1 (vacuously true)\n","            # or sometimes excluded from mAP calculation. For simplicity, we'll treat as 0 AP if no ground truths.\n","            average_precisions.append(0.0)\n","            continue\n","\n","        precision = cumulative_tp / (cumulative_tp + cumulative_fp + 1e-6) # Add epsilon for stability\n","        recall = cumulative_tp / num_ground_truths_for_class\n","\n","        # Calculate AP using the 11-point interpolation method\n","        ap = calculate_ap(recall, precision)\n","        average_precisions.append(ap)\n","\n","    # Calculate mAP by averaging the APs for all classes\n","    if len(average_precisions) > 0:\n","        mAP = np.mean(average_precisions)\n","    else:\n","        mAP = 0.0 # No classes found or evaluated\n","\n","    print(f\"\\nCalculated mAP @ IoU={iou_threshold}: {mAP:.4f}\")\n","\n","    # Return the mAP value\n","    return {\"mAP\": mAP}"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"ea8ef083","executionInfo":{"status":"error","timestamp":1752806879678,"user_tz":-540,"elapsed":19471,"user":{"displayName":"JONG SEO LEE","userId":"07367611241658641309"}},"outputId":"64064236-46ff-4198-caea-911f3eae1d11"},"source":["import torch\n","import os\n","\n","# Assuming model, optimizer, scheduler, train_loader, val_loader, device are already defined\n","# Assuming train_one_epoch and evaluate functions are already defined (specifically the one from cell 222c0423)\n","\n","# Define the number of epochs\n","num_epochs = 10 # You can adjust this\n","\n","# Define the directory to save the model checkpoints\n","model_save_dir = \"/content/drive/MyDrive/model_checkpoints\"\n","os.makedirs(model_save_dir, exist_ok=True)\n","\n","# Training loop\n","print(\"Starting training...\")\n","best_mAP = 0.0 # To keep track of the best mAP for saving the best model\n","\n","for epoch in range(num_epochs):\n","    # Train for one epoch\n","    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n","\n","    # Update the learning rate\n","    # Assuming you are using a scheduler defined as 'scheduler'\n","    if 'scheduler' in globals() and scheduler is not None:\n","         scheduler.step()\n","    else:\n","         print(\"Warning: Learning rate scheduler 'scheduler' not found or is None. Skipping scheduler step.\")\n","\n","\n","    # Evaluate on the validation set\n","    print(\"\\nEvaluating on validation set...\")\n","    # Call the evaluate function which now calculates mAP directly\n","    eval_results = evaluate(model, val_loader, device)\n","\n","    # Check if evaluation returned results and if mAP is available\n","    if eval_results is not None and \"mAP\" in eval_results:\n","        current_mAP = eval_results[\"mAP\"]\n","        print(f\"Validation mAP: {current_mAP:.4f}\")\n","\n","        # Save the model if the current mAP is better than the best mAP\n","        if current_mAP > best_mAP:\n","            best_mAP = current_mAP\n","            # Include epoch number in the filename\n","            model_save_path = os.path.join(model_save_dir, f\"fasterrcnn_resnet50_fpn_best_mAP_epoch_{epoch:03d}.pth\")\n","            torch.save(model.state_dict(), model_save_path)\n","            print(f\"Saved best model to {model_save_path} with mAP: {best_mAP:.4f}\")\n","        else:\n","            print(f\"Validation mAP ({current_mAP:.4f}) did not improve from best mAP ({best_mAP:.4f})\")\n","    else:\n","        print(\"Evaluation results or mAP not available. Skipping model saving based on mAP for this epoch.\")\n","\n","\n","print(\"\\nTraining finished.\")\n","\n","# You can load the best model later using:\n","# model = get_model_instance_segmentation(num_classes) # Initialize model architecture\n","# model.load_state_dict(torch.load(os.path.join(model_save_dir, \"fasterrcnn_resnet50_fpn_best_mAP_epoch_XXX.pth\"))) # Replace XXX with the epoch number\n","# model.to(device)"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n"]},{"output_type":"error","ename":"IndexError","evalue":"Target 27733 is out of bounds.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-51-1580992663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-35-2499192256.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mregression_targets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regression_targets cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m             \u001b[0mloss_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_box_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfastrcnn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss_classifier\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss_box_reg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_box_reg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mfastrcnn_loss\u001b[0;34m(class_logits, box_regression, labels, regression_targets)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mclassification_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# get indices that correspond to the regression targets for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Target 27733 is out of bounds."]}]}]}